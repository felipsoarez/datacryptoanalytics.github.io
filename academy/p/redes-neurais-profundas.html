<!DOCTYPE html>
<html lang="en">
   <head>
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-160618603-1"></script>
      <script>
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
         gtag('js', new Date());
         
         gtag('config', 'UA-160618603-1');
      </script>
      <meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</title>
      <meta name="HandheldFriendly" content="True">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link rel="shortcut icon" href="https://datacryptoanalytics.com/logo-web-datacrypto-analytics.png">
      <link rel="stylesheet" href="page_files/all.css">
      <link rel="stylesheet" type="text/css" href="page_files/libraries.css">
      <link rel="stylesheet" type="text/css" href="page_files/style.css">
      <meta name="description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
      <link rel="shortcut icon" href="https://datacryptoml.000webhostapp.com/img/icon_DataCrypto.png" type="https://datacryptoml.000webhostapp.com/img/datacrypto-analytics.png">
      <link rel="canonical" href="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
      <meta name="referrer" content="no-referrer-when-downgrade">
      <meta property="og:site_name" content="DataCrypto Analytics Blog">
      <meta property="og:type" content="article">
      <meta property="og:title" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
      <meta property="og:description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
      <meta property="og:url" content="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
      <meta property="og:image" content="https://www.datacryptoanalytics.ml/img/fundo.jpg">
      <meta property="article:published_time" content="2020-03-20T16:51:04.000Z">
      <meta property="article:modified_time" content="2020-03-20T16:51:04.000Z">
      <meta property="article:tag" content="Articles">
      <meta property="article:publisher" content="https://fb.me/datacryptopy">
      <meta name="twitter:card" content="summary_large_image">
      <meta name="twitter:title" content="DataCrypto Analytics - Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
      <meta name="twitter:description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
      <meta name="twitter:url" content="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
      <meta name="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/3/3c/Neuralnetwork.png">
      <meta name="twitter:label1" content="Written by">
      <meta name="twitter:data1" content="DataCrypto Analytics Dev Team">
      <meta name="twitter:label2" content="Filed under">
      <meta name="twitter:data2" content="Articles">
      <meta name="twitter:site" content="@DataCryptoML">
      <meta property="og:image:width" content="1800">
      <meta property="og:image:height" content="1200">
      <script type="text/javascript" async="" src="page_files/analytics.js"></script><script type="application/ld+json">
         {
             "@context": "https://schema.org",
             "@type": "Article",
             "publisher": {
                 "@type": "Organization",
                 "name": "DataCrypto Analytics ",
                 "logo": "https://datacryptoanalytics.github.io/logo-web-datacrypto-analytics.png"
             },
             "author": {
                 "@type": "Person",
                 "name": "DataCrypto Analytics Dev Team",
                 "url": "https://datacryptoanalytics.github.io/logo-web-datacrypto-analytics.png",
                 "sameAs": []
             },
             "headline": "DataCrypto Analytics Dev",
             "url": "https://datacryptoanalytics.github.io/",
             "datePublished": "2020-03-20T16:51:04.000Z",
             "dateModified": "2020-03-20T16:51:04.000Z",
             "image": {
                 "@type": "ImageObject",
                 "url": "https://datacryptoanalytics.github.io/logo-web-datacrypto-analytics.png",
                 "width": 1800,
                 "height": 1200
             },
             "keywords": "Articles",
             "description": "Comprar Bitcoin 10% de desconto | Buy Bitcoin 10% discount",
             "mainEntityOfPage": {
                 "@type": "WebPage",
                 "@id": "https://datacryptoanalytics.github.io/"
             }
         }
      </script>
      <script defer="defer" src="page_files/a.js"></script>
      <meta name="generator" content="Ghost 3.9">
      <link rel="alternate" type="application/rss+xml" title="Cindicator Blog" href="https://www.datacryptoanalytics.ml/blog/">
      <script async="" src="page_files/js.js"></script>
      <link rel="stylesheet" href="assets/css/styles.css">
      <link rel="stylesheet" href="assets/css/font-awesome.css">
   </head>
   <body class="post-template tag-articles nav-closed scroll">
      <div class="main-container">
         <header>
            <div class="container">
               <div class="container">
                  <div class="row">
                     <div class="col-3 ml-auto">
                        <div class="navbar-brand">
                           <a href="" title="DataCrypto Analytics Blog">
                           <img src="images/logo-white-datacrypto.png" class="imsombra" width='122' height="53"></img>
                           </a>
                        </div>
                     </div>
                     <div class="col-9">
                        <div class="inner">
                           <div class="navigation">
                              <a href="#" class="navigation-trigger" data-toggle="modal" data-target="#menu">
                                 <svg width="19" height="16" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
                                    <use xlink:href="#path0_fill" transform="translate(.344 .195)"></use>
                                    <defs>
                                       <path id="path0_fill" d="M1.044 2.128h15.912c.577 0 1.044-.476 1.044-1.064C18 .477 17.533 0 16.956 0H1.044C.467 0 0 .477 0 1.064c0 .588.467 1.064 1.044 1.064zm15.912 4.387H1.044C.467 6.515 0 6.993 0 7.58c0 .589.467 1.064 1.044 1.064h15.912c.577 0 1.044-.475 1.044-1.064 0-.586-.467-1.064-1.044-1.064zm0 6.515H1.044C.467 13.03 0 13.508 0 14.094c0 .589.467 1.064 1.044 1.064h15.912c.577 0 1.044-.476 1.044-1.064 0-.586-.467-1.064-1.044-1.064z"></path>
                                    </defs>
                                 </svg>
                              </a>
                              <nav class="nav">
                                 <ul class="">
                                    <li class="nav-home">
                                       <a href="https://datacryptoanalytics.com/membership" class="nav-link" target="_blank">
                                          <ion-icon name="people-circle-outline"></ion-icon>
                                          Membros
                                       </a>
                                    </li>
                                    <li class="nav-home">
                                       <a href="https://dc-analytics.gitbook.io/docs/" class="nav-link" target="_blank">
                                          <ion-icon name="open-outline"></ion-icon>
                                          Docs
                                       </a>
                                    </li>
                                    <li class="nav-home">
                                       <a href="https://datacryptoanalytics.com/indicadores" class="nav-link" target="_blank">
                                          <ion-icon name="open-outline"></ion-icon>
                                          Indicadores
                                       </a>
                                    </li>
                                    <li class="nav-home">
                                       <a href="https://datacrypto-analytics.evermart.com.br/" class="nav-link" target="_blank">
                                          <ion-icon name="log-in-outline"></ion-icon>
                                          Login
                                       </a>
                                    </li>
                                    <li class="nav-home">
                                       <a href="https://br.tradingview.com/u/DataCryptoAnalytics/" class="nav-link imsombra"  target="_blank">
                                          <svg width="36" height="28" viewBox="0 0 36 28" xmlns="http://www.w3.org/2000/svg"  style="color: #ffffff;>
                                          <path d="M14 22H7V11H0V4h14v18zM28 22h-8l7.5-18h8L28 22z" fill="currentColor"></path>
                                          <circle cx="20" cy="8" r="4" fill="currentColor"></circle>
                                          </svg>
                                       </a>
                                    </li>
                                 </ul>
                              </nav>
                           </div>
                        </div>
                     </div>
                  </div>
               </div>
               <div class="info-bar">
                  <div class="progress" data-read="read" style="width: 0%;" data-original-title="1% read" title=""></div>
               </div>
         </header>
         <!-- Menu Modal -->
         <div class="modal fade" id="menu" tabindex="-1" role="dialog" aria-hidden="true">
         <div class="modal-dialog" role="document">
         <div class="modal-content">
         <div class="modal-body">
         <a href="#" data-dismiss="modal" aria-label="Close" class="close btn">
         <i class="close-button"></i>
         </a>
         <div class="container">
         <div class="row">
         <div class="col-md-12">
         <h3 class="modal-title">Menu</h3>
         <div class="modal-nav"></div>
         </div>
         </div>
         <div class="row additional">
         <div class="col-sm-12">
         <h3 class="modal-title">Social</h3>
         <ul class="share">
         <li><a href="https://fb.me/datacryptopy" class="facebook" target="_blank" data-toggle="tooltip" data-placement="top" title="Facebook" data-original-title="crowdindicator/"><ion-icon name="logo-facebook"></ion-icon></a></li>
         <li><a href="https://twitter.com/DataCryptoML" class="twitter" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="@DataCryptoML"><ion-icon name="logo-twitter"></ion-icon></a></li>
         <li><a href="https://www.youtube.com/" class="youtube" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Youtube"><ion-icon name="logo-youtube"></ion-icon></a></li>
         <li><a href="https://www.instagram.com/" class="instagram" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Instagram"><ion-icon name="logo-instagram"></ion-icon></a></li>
         <li><a href="" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="GitHub"><ion-icon name="logo-github"></ion-icon></a></li>
         </ul>
         </div>
         </div>
         </div>
         </div>
         </div>
         </div>
         </div>
         <main id="content" role="main" data-id="0">
         <article class="post tag-articles">
         <div class="content-inner">
         <div class="container">
         <div class="row">
         <div class="col-xl-12 ml-auto mr-auto">
         <div class="row">
         <div class="col-lg-8 col-md-12">
         <div class="post-meta">
         <div>    <!-- GTranslate: https://gtranslate.io/ -->
         <a href="#" onclick="doGTranslate('pt|zh-CN');return false;" title="Chinese (Simplified)" class="gflag nturl" style="background-position:-300px -0px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Chinese (Simplified)" /></a><a href="#" onclick="doGTranslate('pt|en');return false;" title="English" class="gflag nturl" style="background-position:-0px -0px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="English" /></a><a href="#" onclick="doGTranslate('pt|fr');return false;" title="French" class="gflag nturl" style="background-position:-200px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="French" /></a><a href="#" onclick="doGTranslate('pt|de');return false;" title="German" class="gflag nturl" style="background-position:-300px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="German" /></a><a href="#" onclick="doGTranslate('pt|it');return false;" title="Italian" class="gflag nturl" style="background-position:-600px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Italian" /></a><a href="#" onclick="doGTranslate('pt|pt');return false;" title="Portuguese" class="gflag nturl" style="background-position:-300px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Portuguese" /></a><a href="#" onclick="doGTranslate('pt|ru');return false;" title="Russian" class="gflag nturl" style="background-position:-500px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Russian" /></a><a href="#" onclick="doGTranslate('pt|es');return false;" title="Spanish" class="gflag nturl" style="background-position:-600px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Spanish" /></a><a href="#" onclick="doGTranslate('pt|vi');return false;" title="Vietnamese" class="gflag nturl" style="background-position:-200px -400px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Vietnamese" /></a>
         <style type="text/css">
         <!--
            a.gflag {vertical-align:middle;font-size:16px;padding:1px 0;background-repeat:no-repeat;background-image:url(//gtranslate.net/flags/16.png);}
            a.gflag img {border:0;}
            a.gflag:hover {background-image:url(//gtranslate.net/flags/16a.png);}
            #goog-gt-tt {display:none !important;}
            .goog-te-banner-frame {display:none !important;}
            .goog-te-menu-value:hover {text-decoration:none !important;}
            body {top:0 !important;}
            #google_translate_element2 {display:none!important;}
            -->
         </style>
         <br /><select onchange="doGTranslate(this);"><option value="">Select Language</option><option value="pt|af">Afrikaans</option><option value="pt|sq">Albanian</option><option value="pt|ar">Arabic</option><option value="pt|hy">Armenian</option><option value="pt|az">Azerbaijani</option><option value="pt|eu">Basque</option><option value="pt|be">Belarusian</option><option value="pt|bg">Bulgarian</option><option value="pt|ca">Catalan</option><option value="pt|zh-CN">Chinese (Simplified)</option><option value="pt|zh-TW">Chinese (Traditional)</option><option value="pt|hr">Croatian</option><option value="pt|cs">Czech</option><option value="pt|da">Danish</option><option value="pt|nl">Dutch</option><option value="pt|en">English</option><option value="pt|et">Estonian</option><option value="pt|tl">Filipino</option><option value="pt|fi">Finnish</option><option value="pt|fr">French</option><option value="pt|gl">Galician</option><option value="pt|ka">Georgian</option><option value="pt|de">German</option><option value="pt|el">Greek</option><option value="pt|ht">Haitian Creole</option><option value="pt|iw">Hebrew</option><option value="pt|hi">Hindi</option><option value="pt|hu">Hungarian</option><option value="pt|is">Icelandic</option><option value="pt|id">Indonesian</option><option value="pt|ga">Irish</option><option value="pt|it">Italian</option><option value="pt|ja">Japanese</option><option value="pt|ko">Korean</option><option value="pt|lv">Latvian</option><option value="pt|lt">Lithuanian</option><option value="pt|mk">Macedonian</option><option value="pt|ms">Malay</option><option value="pt|mt">Maltese</option><option value="pt|no">Norwegian</option><option value="pt|fa">Persian</option><option value="pt|pl">Polish</option><option value="pt|pt">Portuguese</option><option value="pt|ro">Romanian</option><option value="pt|ru">Russian</option><option value="pt|sr">Serbian</option><option value="pt|sk">Slovak</option><option value="pt|sl">Slovenian</option><option value="pt|es">Spanish</option><option value="pt|sw">Swahili</option><option value="pt|sv">Swedish</option><option value="pt|th">Thai</option><option value="pt|tr">Turkish</option><option value="pt|uk">Ukrainian</option><option value="pt|ur">Urdu</option><option value="pt|vi">Vietnamese</option><option value="pt|cy">Welsh</option><option value="pt|yi">Yiddish</option></select><div id="google_translate_element2"></div>
         <script type="text/javascript">
            function googleTranslateElementInit2() {new google.translate.TranslateElement({pageLanguage: 'pt',autoDisplay: false}, 'google_translate_element2');}
         </script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit2"></script>
         <script type="text/javascript">
            /* <![CDATA[ */
            eval(function(p,a,c,k,e,r){e=function(c){return(c<a?'':e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)r[e(c)]=k[c]||e(c);k=[function(e){return r[e]}];e=function(){return'\\w+'};c=1};while(c--)if(k[c])p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c]);return p}('6 7(a,b){n{4(2.9){3 c=2.9("o");c.p(b,f,f);a.q(c)}g{3 c=2.r();a.s(\'t\'+b,c)}}u(e){}}6 h(a){4(a.8)a=a.8;4(a==\'\')v;3 b=a.w(\'|\')[1];3 c;3 d=2.x(\'y\');z(3 i=0;i<d.5;i++)4(d[i].A==\'B-C-D\')c=d[i];4(2.j(\'k\')==E||2.j(\'k\').l.5==0||c.5==0||c.l.5==0){F(6(){h(a)},G)}g{c.8=b;7(c,\'m\');7(c,\'m\')}}',43,43,'||document|var|if|length|function|GTranslateFireEvent|value|createEvent||||||true|else|doGTranslate||getElementById|google_translate_element2|innerHTML|change|try|HTMLEvents|initEvent|dispatchEvent|createEventObject|fireEvent|on|catch|return|split|getElementsByTagName|select|for|className|goog|te|combo|null|setTimeout|500'.split('|'),0,{}))
            /* ]]> */
         </script></div>
         <time datetime="2020-03-20">20 March 2020</time>
         <div class="tags"><a href="https://www.datacryptoanalytics.ml/">Articles</a></div>
         </div>
         <h1 class="post-title">Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</h1>
         </div>
         </div>
         <div class="row">
         <div class="col-lg-8 col-md-12">
         <div class="editor-content">
         <h2>Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</h2>
         <p class="scribe">
         <a href="https://www.mql5.com/pt/users/vlad1949" class="author">Vladimir Perervenko</a>  | 20 outubro, 2017
         </p>
         <br><div id="content"><div id="content">
         <h3>Conteúdo</h3>
         <ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#intro">Introdução</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#darch">1. Uma breve descrição dos recursos do pacote </a></li>
         <ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#initialization">1.1. Funções de inicialização dos neurônios</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#activation">1.2. Funções de ativação dos neurônios</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#training">1.3. Métodos de treinamento</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#regularization">1.4. Métodos de regulação e estabilização</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#rbm">1.5. Métodos e parâmetros de treinamento de um RBM</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#dnn">1.6. Métodos e parâmetros de treinamento da DNN</a></li>
         </ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#validation">2. Testando a qualidade do trabalho de uma DNN dependendo dos parâmetros usados</a></li>
         <ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#experiments">2.1. Experimentos</a></li>
         <ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#preparation">2.1.1. Dados de entrada (preparação)</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#base">2.1.2. Modelo básico de comparação</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#structure">2.1.3. Estrutura de uma DNN</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#methods">2.1.4. Variantes de treinamento</a></li>
         <ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#pretraining">Com pré-treinamento</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#nopretraining">Sem pré-treinamento</a></li>
         </ul>
         </ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#analythics">2.2. Análise de resultados</a></li>
         </ul>
         <li><a href="https://www.mql5.com/pt/articles/3473#final">Conclusão</a></li>
         <li><a href="https://www.mql5.com/pt/articles/3473#programs">Aplicação</a></li>
         </ul>
         <h3 id="intro">Introdução</h3>
         <h4>Principais direções de estudo e aplicação</h4>
         <p>Atualmente, existem dois fluxos principais no estudo e aplicação de
         redes neurais profundas. Eles diferem na abordagem da inicialização dos
         pesos dos neurônios em camadas ocultas.</p>
         <p><b><i>Abordagem 1. </i></b>As redes neurais são muito sensíveis ao
         método de inicialização de neurônios em camadas ocultas, especialmente
         se o número de camadas ocultas aumentar (maior que 3). O professor
         G.Hynton foi o primeiro a tentar e resolver esse problema. A ideia por
         trás de sua abordagem foi iniciar os pesos dos neurônios em camadas
         ocultas com os pesos obtidos durante o treinamento não supervisionado
         das redes neurais auto-associativas constituídas por RBM (máquina de
         Boltzmann restrita) ou AE (autoencoder). Esses RBMs empilhados (SRBM) e
         AE empilhados (SAE) são treinados com uma grande variedade de dados
         não-rotulados. O objetivo desse treinamento é destacar estruturas
         ocultas (representações, imagens) e relacionamentos nos dados. A
         inicialização de neurônios com pesos MLP, obtidos durante o
         pré-treinamento, coloca o MLP ao espaço de soluções muito próximo ao
         ótimo. Isso permite diminuir o número de dados rotulados e as épocas
         durante o seguinte ajuste fino (treinamento) do MLP. Estas são vantagens
         extremamente importantes para muitas esferas de aplicação prática,
         especialmente ao processar muitos dados.</p>
         <p><i><b>Abordagem 2: </b></i>Outro grupo de cientistas liderados por
         Yoshua Benjio criou métodos específicos de inicialização de neurônios
         ocultos, funções específicas de ativação, métodos de estabilização e
         treinamento. O sucesso desta direção está relacionada com um extenso
         desenvolvimento de redes neurais convolutivas profundas e redes
         neuronais recorrentes (DCNN, RNN). Tais redes neurais apresentaram alta
         eficiência no reconhecimento de imagens, análise e classificação de
         textos, juntamente com a tradução do discurso falado de um idioma para
         outro. A ideias e métodos desenvolvidos para essas redes neurais também
         começaram a ser usadas ​​para a MLP.</p>
         <p>Atualmente, ambas abordagens são usadas ativamente. Deve-se notar que
         com quase os mesmos resultados, as redes neurais com pré-treinamento
         exigem menos recursos computacionais e menos amostras para treinamento.
         Esta é uma vantagem importante. Eu pessoalmente sou a favor das redes
         neurais profundas com pré-treinamento. Eu acredito que o futuro pertence
         à aprendizagem não supervisionada.</p>
         <h4>Pacotes em R que permitem desenvolver e usar DNN</h4>
         <p>R possui uma série de pacotes para criar e usar uma DNN com um nível diferente de complexidade e conjunto de recursos.</p>
         <p><i>Pacotes que permitem criar, treinar e testar um DNN com pré-treinamento:</i></p>
         <ul>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/deepnet/" rel="nofollow" title="https://rdrr.io/cran/deepnet/" target="_blank">deepnet</a></b>
         é um pacote simples que não possui muitas configurações e parâmetros.
         Permite criar ambas as redes neurais SAE com pré-treinamento e SRBM. No <a href="https://www.mql5.com/pt/articles/3526" target="_blank">artigo anterior</a>
         nós consideramos uma implementação prática de Experts usando este
         pacote. O uso de RBM para pré-treinamento produz resultados menos
         estáveis. Este pacote é adequado para o primeiro encontro com este tema e
         sobre as peculiaridades de tais redes. Com a abordagem certa, ela pode
         ser usada em um Expert. <a href="https://www.mql5.com/go?link=https://rdrr.io/cran/RcppDL/" rel="nofollow" title="https://rdrr.io/cran/RcppDL/" target="_blank">RcppDL</a> é uma versão deste pacote ligeiramente menor em С++.</li>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/darch/" rel="nofollow" title="https://rdrr.io/cran/darch/" target="_blank">darch v.0.12</a></b>
         é um pacote complexo e flexível que tem muitos parâmetros e
         configurações. As configurações recomendadas são definidas como padrão.
         Este pacote permite criar e configurar uma rede neural de qualquer
         complexidade e configuração. Ela utiliza a SRBM para pré-treinamento.
         Este pacote é para usuários avançados. Nós vamos discutir os seus
         recursos em detalhes mais tarde. <br>
         </li>
         </ul>
         <p><i>Abaixo estão os pacotes que permitem criar, treinar e testar uma DNN sem pré-treinamento:</i></p>
         <ul>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/h2o/" rel="nofollow" title="https://rdrr.io/cran/h2o/" target="_blank">H2O</a> é</b>
         um pacote para processamento de dados grandes (&gt;1M de linhas e
         &gt;1K de colunas). A rede neural profunda utilizada nela possui um
         sistema de regularização desenvolvido. Suas capacidades são excessivas
         para o nosso campo, mas isso não deve nos parar de usá-lo. </li>
         <li><a href="https://www.mql5.com/go?link=http://mxnet.incubator.apache.org/get_started/install.html" rel="nofollow" title="http://mxnet.io/get_started/install.html" target="_blank"><b>mxnet</b> </a>permite
         criar não só uma MLP, mas também redes recorrentes complexas, por
         convolução e LSTM. Este pacote tem uma API para vários idiomas,
         incluindo R e Python. A filosofia do pacote é diferente das listadas
         acima. Isso ocorre porque os desenvolvedores escreveram pacotes
         principalmente para Python. O pacote mxnet para R é mais leve e
         possuindo menos recursos do que o pacote para Python. Isso não faz este
         pacote pior. <br>
         </li>
         </ul>
         <p>O tema das redes profundas e recorrentes está bem desenvolvido no
         ambiente Python. Existem muitos pacotes interessantes para a construção
         de redes neurais desse tipo que a R não possui. Estes são pacotes R que
         permitem executar programas/módulos escritos em Python:</p>
         <ul>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/PythonInR/" rel="nofollow" title="https://rdrr.io/cran/PythonInR/" target="_blank">PythonInR</a> e<a href="https://www.mql5.com/go?link=https://cran.r-project.org/web/packages/reticulate/index.html" rel="nofollow" title="https://cran.r-project.org/web/packages/reticulate/index.html" target="_blank"> reticulate</a></b>
         são dois pacotes que permitem execução de qualquer código Python em R.
         Para isso, você precisa ter o Python 2/3 instalado no seu computador.</li>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/kerasR/" rel="nofollow" title="https://rdrr.io/cran/kerasR/" target="_blank">kerasr</a></b> é uma interface R para uma biblioteca popular de aprendizagem profunda - <b>keras</b>.</li>
         <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/tensorflow/" rel="nofollow" title="https://rdrr.io/cran/tensorflow/" target="_blank">tensorflow</a></b> é um pacote que fornece acesso à API TensorFow completa no ambiente R.</li>
         </ul>
         <p>Recentemente, a Microsoft publicou a biblioteca <a href="https://www.mql5.com/go?link=https://github.com/Microsoft/CNTK" rel="nofollow" title="https://github.com/Microsoft/CNTK" target="_blank">cntk v.2.1 (Computational Network Toolkit)</a> no GitHub. Ela pode ser usada como backend em comparação com Keras. Recomenda-se testá-lo em nossos problemas.</p>
         <p>Yandex está se mantendo - a sua própria biblioteca O CatBoost está
         disponível em código aberto. Esta biblioteca pode ser usada para
         treinamento de modelos com dados de diferentes tipos. Isso inclui dados
         difíceis de apresentar como números, por exemplo, tipos de nuvens e
         tipos de mercadorias. <a href="https://www.mql5.com/go?link=https://github.com/catboost/catboost" rel="nofollow" title="https://github.com/catboost/catboost" target="_blank">O código-fonte</a>, <a href="https://www.mql5.com/go?link=https://tech.yandex.ru/CatBoost/" rel="nofollow" title="https://tech.yandex.ru/CatBoost/" target="_blank">documentação</a>, benchmarks e ferramentas necessárias já foram <a href="https://www.mql5.com/go?link=https://github.com/catboost/catboost" rel="nofollow" target="_blank" title="https://github.com/catboost/catboost">publicadas no GitHub</a><span style="color:rgb(34, 34, 34);">
         com a licença Apache 2.0. Apesar de não serem redes neurais, mas
         árvores melhoradas, é aconselhável testar o algoritmo, especialmente
         porque contém a API da R.</span></p>
         <br>
         <h3 id="darch">1. Uma breve descrição dos recursos do pacote</h3>
         <p>O <b>pacote darch ver. 0.12.0</b> fornece uma ampla gama de
         funcionalidades, permitindo que você não apenas crie e treine um modelo,
         mas faça o que quiser para suas necessidades e preferências. Mudanças
         significativas foram introduzidas na versão do pacote (0.10.0),
         considerado no <a href="https://www.mql5.com/pt/articles/1628" target="_blank">artigo anterior</a>.
         Foram adicionadas novas funções de ativação, inicialização e
         estabilização. A novidade mais notável é que tudo foi trazido para uma
         única função <i>darch()</i>, que é um construtor ao mesmo tempo. As
         placas gráficas são suportadas. Após o treinamento, a função retorna um
         objeto da classe <a href="https://www.mql5.com/go?link=https://cran.r-project.org/web/packages/darch/darch.pdf" rel="nofollow" title="https://cran.r-project.org/web/packages/darch/darch.pdf" target="_blank">DArch</a>. A estrutura do objeto é apresentada na Fig. 1. As funções <i>predict()</i> e <i>darchTest()</i> retornam uma previsão sobre os novos dados ou métricas de classificação.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/strDarch.png" title="StrDarch" alt="StrDarch" style="vertical-align:middle;"></p>
         <p style="text-align:center;"><span class="small">Fig.1. Estrutura do objeto DArch&nbsp;</span></p>
         <p>Todos os parâmetros têm valores padrão. Esses valores geralmente não
         são ótimos. Todos esses parâmetros podem ser divididos em três grupos -
         geral, para RBM e para NN. Nós vamos considerar alguns deles em detalhes
         mais tarde.</p>
         <table class="standart" width="100%" cellspacing="0" cellpadding="2">
         <thead>
         <tr>
         <th>Funções</th>
         <th>Tipos</th>
         </tr>
         </thead>
         <tbody>
         <tr>
         <td>Funções de inicialização</td>
         <td><ul>
         <li><b><i>generateWeightsFunction</i></b> = с(<a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v9/glorot10a.html" rel="nofollow" title="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank">generateWeightsGlorotUniform</a>, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v9/glorot10a.html" rel="nofollow" title="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank">generateWeightsGlorotNormal</a>,&nbsp;</li>
         </ul>
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generateWeightsUniform,
         generateWeightsNormal,&nbsp;<br>
         <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp; &nbsp; <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1502.01852" rel="nofollow" title="https://arxiv.org/abs/1502.01852" target="_blank">&nbsp;generateWeightsHeUniform</a>, <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1502.01852" rel="nofollow" title="https://arxiv.org/abs/1502.01852" target="_blank">generateWeightsHeNormal</a>)</p></td>
         </tr>
         <tr>
         <td>Funções de ativação</td>
         <td><ul>
         <li><b><i>darch.unitFunction</i></b> = c(sigmoidUnit, linearUnit, tanhUnit, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" rel="nofollow" title="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" target="_blank">rectifiedLinearUnit</a>, &nbsp; &nbsp;</li>
         </ul>
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1511.07289" rel="nofollow" title="https://arxiv.org/abs/1511.07289" target="_blank">exponentialLinearUnit</a>,&nbsp;<a href="https://www.mql5.com/go?link=http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf" rel="nofollow" title="http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf" target="_blank">softplusUnit</a>, <a href="https://www.mql5.com/go?link=http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html" rel="nofollow" title="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html" target="_blank">softmaxUnit</a>, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v28/goodfellow13.pdf" rel="nofollow" title="http://proceedings.mlr.press/v28/goodfellow13.pdf" target="_blank">maxoutUnit</a>)</td>
         </tr>
         <tr>
         <td>Funções de treinamento</td>
         <td><ul>
         <li><b><i>darch.fineTuneFunction</i></b> = c(<a href="https://www.mql5.com/go?link=https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" rel="nofollow" title="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" target="_blank">backpropagation</a>, <a href="https://en.wikipedia.org/wiki/Rprop" rel="nofollow" title="http://sci2s.ugr.es/keel/pdf/algorithm/articulo/2003-Neuro-Igel-IRprop+.pdf" target="_blank">rpropagation</a>(<a href="https://en.wikipedia.org/wiki/Rprop" rel="nofollow" title="https://en.wikipedia.org/wiki/Rprop" target="_blank">Rprop+</a>, Rprop-, iRprop+, iRprop-),</li>
         </ul>
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
         &nbsp;<a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" title="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" target="_blank"> minimizeAutoencoder</a>, <a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" title="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" target="_blank">minimizaClassifier</a>)</td>
         </tr>
         <tr>
         <td>Nível de treinamento</td>
         <td><ul>
         <li><b><i>bp.learnRate</i></b> = 1 é o nível de treinamento
         para backpropagation. Isso pode ser um vetor se diferentes níveis de
         treinamento forem usados ​​em cada camada de NN</li>
         <li><b><i>bp.learnRateScale</i></b> = 1. O nível de treinamento é multiplicado por esse valor após cada época</li>
         </ul></td>
         </tr>
         <tr>
         <td>Funções de estabilização</td>
         <td><ul>
         <li><b><i>darch.dropout</i></b> = 0 é um número (0,1) ou um vetor com o nível de eliminação para cada camada de NN <br>
         </li>
         <li><b><i>darch.dropout.dropConnect</i></b> = F indica se&nbsp;DropConnect deve ser usado em vez de ser abandonado</li>
         <li>
         <p><b><i>darch.dropout.momentMatching</i></b> = 0</p>
         </li>
         <li><b><i>darch.dropout.oneMaskPerEpoch</i></b> = F indica se deve ser gerado uma nova máscara para um novo lote (FALSE, padrão) ou para cada época (TRUE)</li>
         <li><b><i>darch.dither</i></b> = F mostra se <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1508.04826" rel="nofollow" title="https://arxiv.org/abs/1508.04826" target="_blank">dither</a> deve ser aplicado a todos os dados de entrada do conjunto de treinamento</li>
         <li><b><i>darch.nesterovMomentum</i></b> = T</li>
         <li><i><b>darch.weightDecay</b></i> = 0</li>
         <li><b><i>normalizeWeights</i></b> = F</li>
         <li> <b><i>normalizeWeightsBound</i></b> é o limite superior para a norma L2 do vetor de entrada de pesos. Isso é usado apenas se <b><i>normalizeWeights</i></b> = TRUE</li>
         </ul></td>
         </tr>
         <tr>
         <td>Momentum</td>
         <td><ul>
         <li><b><i>darch.initialMomentum</i></b> = 0.5</li>
         <li><b><i>darch.finalMomentum</i></b> = 0.9</li>
         <li><b><i>darch.momentumRampLength</i></b> = 1</li>
         </ul></td>
         </tr>
         <tr>
         <td>&nbsp;Condições de parada</td>
         <td><ul>
         <li><b><i>darch.stopClassErr</i></b> = 100</li>
         <li><b><i>darch.stopErr </i></b>= -Inf<br>
         </li>
         <li><b><i>darch.stopValidClassErr</i></b> = 100</li>
         <li><b><i>darch.stopValidErr</i></b> = -Inf    </li>
         </ul></td>
         </tr>
         </tbody>
         </table>
         <p>Uma rede neural profunda é composta por n RBM (n = camadas -1)
         conectadas em uma rede auto-associativa (SRBM) e as redes neurais reais
         MLP com várias camadas. O treinamento em camada da RBM é um treinamento
         não supervisionado em dados não-rotulados. O ajuste fino da rede neural
         requer supervisão e é realizado em dados rotulados. </p>
         <p>A divisão desses estágios de treinamento com parâmetros nos dá a
         oportunidade de usar dados de diferentes volumes (não uma estrutura
         diferente!!) e obter vários modelos ajustados com base em um
         pré-treinamento. Se os dados para pré-treinamento e ajuste fino forem
         iguais, o treinamento pode ser realizado de uma só vez sem dividi-lo em
         duas etapas. O pré-treinamento pode ser ignorado (<i>rbm.numEpochs = 0;&nbsp;</i><i>darch.numEpochs = 10</i>)). Nesse caso, você pode usar apenas uma rede neural de várias camadas ou treinar apenas a RBM (<i>rbm.numEpochs = 10; darch.numEpochs = 0</i>). Você ainda terá acesso a todos os parâmetros internos. </p>
         <p>A rede neural treinada pode ser treinada em novos dados quantas vezes
         for necessário. Isso só é possível com um número limitado de modelos. O
         diagrama estrutural de uma rede neural profunda inicializada por
         máquinas de Boltzmann complexas e restritas (DNRBM) é exibido na Fig.2.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNSRBM_2.png" title="DNSRBM" alt="DNSRBM" style="vertical-align:middle;" width="690" height="434"></p>
         <p style="text-align:center;"><span class="small">Fig.2. Diagrama estrutural de DNSRBM</span></p>
         <br>
         <p id="initialization"><b>1.1. Funções de inicialização dos neurônios</b></p>
         <p>Existem duas funções principais de inicialização de neurônios no pacote.&nbsp; </p>
         <ul>
         <li><i>generateWeightsUniform()</i> usa a função <i>runif(n, min, max)</i>, sendo implementada da seguinte forma:</li>
         </ul>
         <pre class="code">&gt; generateWeightsUniform
         function (numUnits1, numUnits2, weights.min = getParameter(<span class="string">".weights.min"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;-<span class="number">0.1</span>, ...), weights.max = getParameter(<span class="string">".weights.max"</span>, <span class="number">0.1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;...), ...)
         {
         &nbsp;&nbsp;&nbsp;&nbsp;matrix(runif(numUnits1 * numUnits2, weights.min, weights.max),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nrow = numUnits1, ncol = numUnits2)
         }
         &lt;environment: namespace:darch&gt;
         </pre>
         <p><i>numUnits1</i> é o número de neurônios na camada anterior e <i>numUnits2</i> é o número de neurônios na camada atual.&nbsp;</p>
         <ul>
         <li><i>generateWeightsNormal()</i>usa a função <i>rnorm(n, mean, sd)</i> sendo implementada no pacote da seguinte forma:</li>
         </ul>
         <pre class="code">&gt; generateWeightsNormal
         function (numUnits1, numUnits2, weights.mean = getParameter(<span class="string">".weights.mean"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;<span class="number">0</span>, ...), weights.sd = getParameter(<span class="string">".weights.sd"</span>, <span class="number">0.01</span>, ...),
         &nbsp;&nbsp;&nbsp;&nbsp;...)
         {
         &nbsp;&nbsp;&nbsp;&nbsp;matrix(rnorm(numUnits1 * numUnits2, weights.mean, weights.sd),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nrow = numUnits1, ncol = numUnits2)
         }
         &lt;environment: namespace:darch&gt;
         </pre>
         <p>Outras quatro funções estão usando essas duas funções, mas definem <i>min, max, média </i>e<i> sd</i> com funções específicas. Você pode estudá-las se você inserir o nome da função sem colchetes no terminal.</p>
         <br>
         <p id="activation"><b>1.2. Funções de ativação dos neurônios</b></p>
         <p>Além das funções de ativação padrão, o pacote sugere uma ampla gama de novas funções. Aqui estão algumas delas:<br>
         </p>
         <pre class="code">x &lt;- seq(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)
         par(mfrow = c(<span class="number">2</span>,<span class="number">3</span>))
         plot(x, y = <span class="number">1</span>/(<span class="number">1</span> + <span class="functions">exp</span>(-x)), t = <span class="string">"l"</span>, main = <span class="string">"sigmoid"</span>)
         abline(v = <span class="number">0</span>, col = <span class="number">2</span>)
         plot(x, y = tanh(x), t = <span class="string">"l"</span>, main = <span class="string">"tanh"</span>)
         abline(v = <span class="number">0</span>, h = <span class="number">0</span>, col = <span class="number">2</span>)
         plot(x, y = <span class="functions">log</span>(<span class="number">1</span> + <span class="functions">exp</span>(x)), t = <span class="string">"l"</span>, main = <span class="string">"softplus"</span>);
         abline(v = <span class="number">0</span>, col = <span class="number">2</span>)
         plot(x, y = ifelse(x &gt; <span class="number">0</span>, x ,<span class="functions">exp</span>(x) - <span class="number">1</span>), t = <span class="string">"l"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp; main = <span class="string">"ELU"</span>)
         abline(h = <span class="number">0</span>, v = <span class="number">0</span>, col = <span class="number">2</span>)
         plot(x, y = ifelse(x &gt; <span class="number">0</span>, x , <span class="number">0</span>), t = <span class="string">"l"</span>, main = <span class="string">"ReLU"</span>)
         abline(h = <span class="number">0</span>, v = <span class="number">0</span>, col = <span class="number">2</span>)
         par(mfrow = c(<span class="number">1</span>,<span class="number">1</span>))
         </pre>
         <p>&nbsp;</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/actFun_3.png" title="activFun" alt="activFun" style="vertical-align:middle;" width="750" height="442"></p>
         <p style="text-align:center;"><span class="small">Fig.3. Funções de ativação dos neurônios</span></p>
         <p>Vamos considerar a <i>função de ativação maxout separadamente. </i>Esta
         função vem de redes por convolução. A camada oculta da rede neural é
         dividida por módulos do tamanho da agregação (poolSize). O número de
         neurônios na camada oculta deve ser divisível pelo tamanho da agregação
         (pool). Para o treinamento, um neurônio com uma ativação máxima é
         selecionado da pool e enviado para a entrada. A função de ativação dos
         neurônios na pool é definida separadamente. Em palavras simples, esta é
         uma camada dupla (convolução + maxpooling) com capacidades limitadas no
         passo de filtração. De acordo com várias publicações, ela produz bons
         resultados em conjunto com <i>dropout</i>. Fig. 4. É exibido esquematicamente uma camada oculta com 8 neurônios e dois tamanhos da pool</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/maxout_4__1.png" title="Maxout" alt="Maxout" style="vertical-align:middle;" width="610" height="305"></p>
         <p style="text-align:center;"><span class="small">Fig.4. A função de ativação <i>maxout</i></span></p>
         <p id="training"><b>1.3. Métodos de treinamento</b></p>
         <br>
         <p>Infelizmente, existem apenas dois métodos de treinamento no pacote - <b><i>backpropagation</i> </b>e <b><i>rprop </i></b>da
         versão básica e melhorada com atualização do peso durante a
         backpropagation e sem ela. Existe também a possibilidade de alterar o
         nível de treinamento com a ajuda do multiplicador <b><i>bp.learnRateScale.</i></b>.</p>
         <br>
         <p id="regularization"><b>1.4. Métodos de regulação e estabilização</b></p>
         <ul>
         <li><b>dropout </b> é uma técnica de eliminação (método de
         regularização) de uma parte dos neurônios da camada oculta durante o
         treinamento. Os neurônios são zerados em uma ordem aleatória. O número
         relativo de neurônios a serem descartados é definido pelo parâmetro <i>darch.dropout</i><i>.</i>
         O nível de regularização em cada camada oculta pode ser diferente. A
         máscara de regularização pode ser gerada para cada lote ou para cada
         época.</li>
         <li><b>dropconnect</b> desliga as conexões entre uma parte dos
         neurônios da camada atual e os neurônios da camada anterior. As conexões
         são cortadas em uma ordem aleatória. O número relativo de conexões a
         serem cortadas é definido pelo mesmo parâmetro <i>darch.dropout (</i>geralmente
         não superior a 0.5). De acordo com algumas publicações, o dropconnect
         exibe melhores resultados do que o método de regularização (dropout).</li>
         <li><a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1508.04826" rel="nofollow" title="/go?link=https://arxiv.org/abs/1508.04826" target="_blank"><b>dither</b></a> é uma maneira de evitar um retreinamento através da redução (dithering) dos dados de entrada. <br>
         </li>
         <li><i><b>weightDecay</b></i> o peso de cada neurônio será multiplicado por (1 — <i><b>weightDecay)</b></i>antes da atualização.</li>
         <li><b><i>normalizeWeights</i></b> é uma maneira de normalizar um vetor de entrada de pesos de neurônios com uma possível limitação acima (norma L2)</li>
         </ul>
         <p>Os três primeiros métodos são usados ​​apenas de maneira separada.&nbsp;</p>
         <br>
         <p id="rbm"><b>1.5. Métodos e parâmetros de treinamento de um RBM</b></p>
         <p>Há duas maneiras de treinar uma SRBM. Qualquer RBM é treinada uma a
         uma durante a rbm.numEpochs ou cada RBM é treinada em uma única época ao
         mesmo tempo. A escolha de um desses métodos é feita pelo parâmetro <i>rbm.consecutive:</i> TRUE ou padrão é o primeiro método e FALSE é o segundo método. Fig.5 apresenta um esquema de treinamento em duas variantes. O <i>rbm.lastLayer</i>
         pode ser usado para especificar a camada de SRBM em que o
         pré-treinamento deve ser interrompido. Se 0, todas as camadas devem ser
         treinadas e se (-1) a camada superior é para deixar sem treino. Isso faz
         sentido, já que a camada superior precisa ser treinada separadamente e
         leva muito mais tempo. Outros parâmetros não precisam de explicações
         adicionais.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/SRBMtarin_5.png" title="SRBMtrain" alt="SRBMtrain" style="vertical-align:middle;" width="690" height="434"></p>
         <p style="text-align:center;"><span class="small">Fig.5. Dois métodos de treinamento de uma SRBM</span></p>
         <br>
         <p id="dnn"><b>1.6. Métodos e parâmetros de treinamento da DNN</b></p>
         <p>Uma DNN pode ser treinada de duas maneiras - com pré-treinamento e
         sem ele. Os parâmetros utilizados nestes métodos são totalmente
         diferentes. Por exemplo, não é necessário usar métodos específicos de
         inicialização e regularização no treinamento <b>com pré-treinamento</b>.
         De fato, usar esses métodos pode piorar o resultado. A razão por trás
         disso é que, após um pré-treinamento, os pesos dos neurônios nas camadas
         ocultas serão colocados na área próxima aos valores ótimos e eles
         precisarão apenas de uma pequena afinação fina. Para obter o mesmo
         resultado no treinamento <b>sem pré-treinamento</b>, todos os métodos
         disponíveis de inicialização e regularização terão de ser utilizados.
         Geralmente, o treinamento de uma rede neural dessa maneira demora mais.</p>
         <p>Então, vamos nos concentrar no <b>treinamento com pré-treinamento</b>. Normalmente, ele acontece em duas etapas.</p>
         <ol>
         <li>Treinando a SRBM em um grande conjunto de dados não gravados. Os
         parâmetros de pré-treinamento são definidos separadamente. Como
         resultado, nós temos uma rede neural iniciada por pesos da SRBM. Em
         seguida, a camada superior da rede neural é treinada com dados rotulados
         com seus próprios parâmetros de treinamento. Desta forma, nós temos uma
         rede neural com uma camada superior treinada e pesos iniciados nas
         camadas inferiores. Salve-o como um objeto independente para uso
         posterior.&nbsp; <br>
         </li>
         <li>Na segunda etapa, nós usaremos algumas amostras rotuladas, baixo
         nível de treinamento e um pequeno número de épocas de treinamento para
         todas as camadas da rede neural. Este é um ajuste fino da rede. A rede
         neural é treinada.</li>
         </ol>
         <p>A possibilidade de divisão de estágios de pré-treinamento, ajuste
         fino e treinamentos adicionais oferece uma flexibilidade incrível na
         criação de algoritmos de treinamento não só para uma DNN, mas para
         treinamento de comitês de DNN. Fig.6. representa várias variantes de
         treinamento de DNN e comitês de DNN.&nbsp;</p>
         <ul>
         <li>Variante <i>а</i>. Salve a DNN em todas as épocas durante o ajuste
         fino. Desta forma, nós teremos um número de DNN com um grau de
         treinamento diferente. Mais tarde, cada uma dessas redes neurais pode
         ser usada separadamente ou como parte de um comitê. A desvantagem deste
         cenário é que todos as DNN são treinadas nos mesmos dados, pois todos
         elas tinham os mesmos parâmetros de treinamento. <br>
         </li>
         <li>Variante <i>b</i>. Faça o ajuste fino da DNN iniciada <b>em paralelo</b>
         com diferentes conjuntos de dados (janela deslizante, janela em
         crescimento, etc.) e diferentes parâmetros. Como resultado, nós teremos
         uma DNN que produzirá previsões menos correlacionadas do que as
         variantes <i>a.</i></li>
         <li>Faça o ajuste fino da variante<i> c</i> na DNN iniciada <b>sequencialmente</b>
         com diferentes conjuntos de dados e diferentes parâmetros. Salve os
         modelos intermediários. Isto é o que anteriormente nós chamamos de
         treinamento adicional. Isso pode ser executado sempre que há dados novos
         suficientes.</li>
         </ul>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNNtrain_6.png" title="DNNtrain" alt="DNNtrain" style="vertical-align:middle;" width="749" height="296"></p>
         <p style="text-align:center;"><span class="small">Fig.6. Variantes de treinamento da DNN</span></p>
         <br>
         <h3 id="validation">2. Testando a qualidade do trabalho de uma DNN, dependendo dos parâmetros utilizados.</h3>
         <h4 id="experiments">2.1. Experimentos</h4>
         <br>
         <p id="preparation"><b>2.1.1. Dados de entrada (preparação)</b></p>
         <p>Nós usaremos os dados e funções da parte anterior do artigo (<a href="https://www.mql5.com/pt/articles/3486" target="_blank">1</a>, <a href="https://www.mql5.com/pt/articles/3507" target="_blank">2</a>, <a href="https://www.mql5.com/pt/articles/3526" target="_blank">3</a>).
         Lá foram discutidos em detalhe várias variantes da preparação
         preliminar de dados. Vou mencionar brevemente as etapas de preparação
         preliminar que nós vamos realizar. OHLCV é o dado inicial, o mesmo que
         antes. Os dados de entrada são filtros digitais e os dados de saída são o
         ZigZag. As funções e a imagem do espaço de trabalho Cotir.RData podem
         ser usadas.</p>
         <p>Os estágios de preparação de dados para realizar serão reunidos em funções separadas:</p>
         <ul>
         <li>PrepareData() — cria o DataSet inicial e remove os NA;</li>
         <li>SplitData() — divide o conjunto de dados iniciais nos subconjuntos de pré-treinamento, train, val, test;</li>
         <li>CappingData() — identifica e imputa os outliers em todos os subconjuntos.</li>
         </ul>
         <p>Para salvar o espaço no artigo, eu não vou trazer a lista dessas
         funções aqui. Elas podem ser baixadas do GitHub já que elas foram
         consideradas em detalhes nos artigos anteriores. Nós vamos analisar os
         resultados mais tarde. Nós não vamos discutir todos os métodos de
         transformação de dados durante o processamento preliminar. Muitos deles
         são bem conhecidos e amplamente utilizados. Nós usaremos um método menos
         conhecido de <i><b>discretização </b></i>(supervisionado e não supervisionado). No segundo artigo <a href="https://www.mql5.com/pt/articles/3507#discret" target="_blank">nós consideramos</a> dois pacotes de discretização supervisionada (<b>discretization </b>e <b>smbinning</b>). Eles contêm diferentes algoritmos de discretização.</p>
         <p>Nós examinaremos os diferentes métodos de dividir variáveis
         contínuas em bins e as formas de usar essas variáveis ​​discretizadas
         em modelos.</p>
         <p><i>O que é binning?</i></p>
         <p>Binning é um termo usado na modelagem de pontuação. Ela é conhecida
         como discretização na aprendizagem por máquinas. Este é um processo de
         transformar uma variável contínua em um número finito de intervalos
         (caixas). Isso ajuda a entender sua distribuição e relacionamento com a
         variável objetivo binária. Os bins criados neste processo podem se
         tornar características da característica preditiva para uso em modelos.</p>
         <p><i>Por que o binning?</i></p>
         <p>Apesar de algumas resalvas sobre o binning, ele possui vantagens significativas.&nbsp; </p>
         <ul>
         <li>Ele permite incluir os dados ausentes (NA) e outros cálculos específicos (divisão por zero, por exemplo) no modelo.</li>
         <li>Ele controla ou mitiga o impacto dos outliers no modelo.</li>
         <li>Ele resolve o problema de diferentes escalas em preditores,
         tornando os coeficientes ponderados comparáveis ​​no modelo final.&nbsp;</li>
         </ul>
         <p><i>Discretização não supervisionada</i></p>
         <p>A discretização não supervisionada divide uma função contínua em bins
         sem levar em conta outras informações. Esta divisão tem duas opções.
         Elas são bins de mesmo comprimento e bins de mesma frequência.</p>
         <table class="standart" width="100%" cellspacing="0" cellpadding="3">
         <thead>
         <tr>
         <th>Opção <br></th>
         <th>Alvo <br></th>
         <th>Exemplo <br></th>
         <th>Desvantagem <br></th>
         </tr>
         </thead>
         <tbody>
         <tr>
         <td>Bins de mesmo comprimento<br></td>
         <td>Comprensão da distribuição da variável<br></td>
         <td>Histograma clássico com bunkers de mesmo comprimento, que pode ser calculado usando regras diferentes (sturges, rice ets)<br></td>
         <td>O número de registros no bunker pode ser muito pequeno para um cálculo correto<br></td>
         </tr>
         <tr>
         <td>Bins de frequência qual<br></td>
         <td>Analisa o relacionamento com a variável objetivo binária usando índices como a taxa incorreta<br></td>
         <td>Quartilies ou Percentis</td>
         <td>Os pontos de corte selecionados não podem maximizar a diferença entre as caixas na verificação contra a variável objetivo<br></td>
         </tr>
         </tbody>
         </table>
         <p><i>Discretização supervisionada</i></p>
         <p>A discretização supervisionada divide a variável contínua em caixas
         projetadas na variável objetivo. A ideia-chave aqui é encontrar esses
         pontos de corte que maximizarão a diferença entre os grupos.</p>
         <p>Usando algoritmos como ChiMerge ou Particionamento Recursivo, os
         analistas podem rapidamente encontrar pontos ótimos em segundos e
         avaliar sua relação com a variável objetivo, usando índices como peso de
         evidência (WoE) e valor de informação (IV).</p>
         <p>WoE pode ser usado como um instrumento para transformar preditores no
         estágio de pré-processamento para algoritmos de aprendizagem
         supervisionada. Durante a discretização dos preditores, nós podemos
         substituí-los por suas novas variáveis ​​nominais ou pelos valores de
         seu WoE. A segunda variante é interessante porque permite afastar-se da
         transformação das variáveis ​​nominais (fatores) para as artificiais.
         Isso dá uma melhoria significativa na qualidade da classificação.&nbsp;</p>
         <p>WOE e IV desempenham duas funções diferentes na análise de dados:</p>
         <ul>
         <li><b>WOE descreve a relação da variável preditiva e a variável objetivo binária.</b></li>
         <li><b>IV mede a força dessas relações.</b></li>
         </ul>
         <p>Vamos descobrir o qual WOE e IV estão usando diagramas e fórmulas.
         Lembre-se do gráfico da variável v.fatl dividido em 10 áreas equitativas
         na <a href="https://www.mql5.com/pt/articles/3507" target="_blank">segunda parte do artigo</a>.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Discret6.png" title="vfatl_discr" alt="vfatl_discr" style="vertical-align:middle;" width="750" height="484"></p>
         <p style="text-align:center;"><span class="small">Fig.7. A variável v.fatl é dividida em 10 áreas equitativas</span></p>
         <p><b><i>Capacidade preditiva de dados (WOE)</i></b></p>
         <p>Como você pode ver, cada bin possui amostras que entram na classe "1"
         e na classe "-1". A capacidade preditiva dos bins WoEi são calculados
         com a fórmula&nbsp;</p>
         <p style="text-align:center;">WoEi = ln(Gi/Bi)*100</p>
         <p>где:</p>
         <p> Gi — frequência relativa das amostras "boas" (no nosso caso "bom" = "1") em cada bin da variável;</p>
         <p>Bi — frequência relativa das amostras "ruins" (no nosso caso "ruim" = "-1") em cada bin da variável.</p>
         <p>Se WoEi = 1, o que significa que o número de amostras "boas" e
         "ruins" neste compartimento é aproximadamente o mesmo, então a
         habilidade preditiva desse bin é 0. Se as amostras "boas" superarem em
         número as "ruins", WOE &gt;0 e vice-versa.</p>
         <p><b><i>Valor de informação (IV)&nbsp;</i></b></p>
         <p> Esta é a medida mais comum de identificar a significância de
         variáveis ​​e medir a diferença na distribuição de amostras "boas" e
         "ruins". O valor da informação pode ser calculado com a fórmula:</p>
         <p style="text-align:center;">IV = ∑ (Gi – Bi) ln (Gi/Bi)</p>
         <p>O valor da informação de uma variável é igual à soma de todas bins da
         variável. Os valores desse coeficiente podem ser interpretados da
         seguinte forma:</p>
         <ul>
         <li>abaixo de 0,02 — variável estatisticamente insignificante;</li>
         <li>0,02 - 0,1 — variável estatisticamente fraca;</li>
         <li>0,1 - 0,3 — variável estatisticamente significativa;</li>
         <li>0,3 e acima — variável estatisticamente forte.</li>
         </ul>
         <p>Em seguida, os bins são unidos/divididos usando vários algoritmos e
         critérios de otimização para tornar a diferença entre esses bins tão
         grande quanto possível. Por exemplo, o pacote <b>smbinning </b> usa o
         Particionamento Recursivo para categorizar valores numéricos e o valor
         de informação para resolver os pontos de corte ótimos. O pacote <b>discretization </b> resolve esse problema com o ChiMerge e MDL. Deve-se ter em mente que os<b> pontos de corte são obtidos no conjunto de treinamento</b> e costuma-se dividir os conjuntos de validação e teste.&nbsp;</p>
         <p>Existem vários pacotes que permitem fazer variáveis ​​numéricas discretas de uma maneira ou de outra. Sendo elas: <b>discretization</b><i>, </i><b>smbinning</b><i>, </i><b>Information</b><i>, </i><b>InformationValue </b><i>e </i><b>woebinning</b>.
         Nós precisamos tornar o conjunto de dados de teste discreto, dividir os
         conjuntos de validação e teste usando essas informações. Nós também
         queremos ter controle visual dos resultados. Por causa desses
         requisitos, eu escolhi o pacote <b>woebinning</b>.&nbsp;</p>
         <p>O pacote se divide automaticamente em <b><i>valores numéricos e fatores</i></b> e os liga à variável objetivo binária. Aqui são contempladas duas abordagens: </p>
         <ul>
         <li>a implementação de classificação fina e bruta uni sequencialmente classes e níveis granulados;</li>
         <li>um segmento de abordagem semelhante a uma árvore através de bins iniciais de iteração através da divisão binária. <br>
         </li>
         </ul>
         <p>Ambos os procedimentos combinam bins divididos com base nos valores
         semelhantes de WOE e a parada com base nos critérios IV. O pacote pode
         ser usado tanto com variáveis ​​autônomas quanto com todo o quadro de
         dados. Isso fornece ferramentas flexíveis para estudar várias soluções
         para binning e para expandir novos dados.</p>
         <p>Vamos fazer o cálculo. Nós já temos as cotações do terminal carregado
         em nosso ambiente de trabalho (ou imagem do ambiente de trabalho
         Cotir.RData do GitHub). Sequência de cálculos e resultados:</p>
         <div style="margin:0px; padding:0px;">
         <ol>
         <li>&nbsp;PrepareData() — cria o conjunto de dados inicial dt[7906,
         14], limpo de NA. O conjunto inclui o rótulo temporário Data, variáveis
         de entrada (12) e a variável objetivo Class (fator com dois níveis
         "-1" e "+1").</li>
         <li>&nbsp;SplitData() — divide o conjunto de dados inicial dt[] em
         subconjunto de pré-treinamento, train, val, test na proporção de
         2000/1000/500/500, unindo eles em um dataframe DT[4, 4000, 14].</li>
         <li>&nbsp;CappingData() — identifica e imputa os outliers em todos
         os subconjuntos, obtém o conjunto DTcap[4, 4000, 14]. Apesar do fato de
         que a discretização é toleranta aos outliers, nós os imputaremos. Você
         pode experimentar sem esta etapa. Como você pode lembrar, os parâmetros
         de outliers (pre.outl) são definidos no subconjunto de pré-treinamento.
         Processe os conjuntos de train/val/test usando esses parâmetros.</li>
         <li>&nbsp;NormData() — normaliza o conjunto, usando o método <i>spatialSing</i> do pacote <b>caret</b>.
         Semelhante ao imputamento dos outliers, os parâmetros de normalização
         (preproc) são definidos no subconjunto pretrain. As amostras
         train/val/test são processadas usando esses parâmetros. Nós temos
         DTcap.n[4, 4000, 14] como resultado.</li>
         <li>DiscretizeData() — define os parâmetros de discretização
         (preCut), a qualidade das variáveis ​​e suas bins à luz de WOE e
         IV.&nbsp;</li>
         </ol>
         </div>
         <div style="margin:0px; padding:0px;">
         <pre class="code">evalq({
         &nbsp;&nbsp;dt &lt;- PrepareData(Data, <span class="predefines">Open</span>, <span class="predefines">High</span>, <span class="predefines">Low</span>, <span class="predefines">Close</span>, <span class="predefines">Volume</span>)
         &nbsp;&nbsp;DT &lt;- SplitData(dt, <span class="number">2000</span>, <span class="number">1000</span>, <span class="number">500</span>,<span class="number">500</span>)
         &nbsp;&nbsp;pre.outl &lt;- PreOutlier(DT$pretrain)
         &nbsp;&nbsp;DTcap &lt;- CappingData(DT, impute = T, fill = T, dither = F,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pre.outl = pre.outl)
         &nbsp;&nbsp;preproc &lt;- PreNorm(DTcap, meth = meth)
         &nbsp;&nbsp;DTcap.n &lt;- NormData(DTcap, preproc = preproc)
         &nbsp;&nbsp;preCut &lt;- PreDiscret(DTcap.n)
         }, env)
         </pre>
         <p>Vamos colocar os dados de discretização em todas as variáveis ​​em uma tabela e olhar para elas:</p>
         <pre class="code">evalq(tabulate.binning &lt;- woe.binning.table(preCut), env)
         &gt; env$tabulate.binning
         $`WOE Table <span class="keyword">for</span> v.fatl`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.3904381926</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">130</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">2.4</span>%&nbsp;&nbsp;<span class="number">15.6</span>% <span class="number">171.3</span> <span class="number">0.185</span>
         <span class="number">2</span> &lt;= -<span class="number">0.03713814085</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">498</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">271</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">50.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.8</span>%&nbsp;&nbsp;<span class="number">35.2</span>%&nbsp;&nbsp;<span class="number">63.2</span> <span class="number">0.149</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1130198981</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">141</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">167</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.5</span>%&nbsp;&nbsp;<span class="number">54.2</span>% -<span class="number">14.5</span> <span class="number">0.003</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">219</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">550</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">22.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">54.3</span>%&nbsp;&nbsp;<span class="number">71.5</span>% -<span class="number">89.7</span> <span class="number">0.289</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.626</span>
         $`WOE Table <span class="keyword">for</span> ftlm`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.2344708291</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">333</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">129</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">12.7</span>%&nbsp;&nbsp;<span class="number">27.9</span>%&nbsp;&nbsp;<span class="number">97.2</span> <span class="number">0.204</span>
         <span class="number">2</span> &lt;= -<span class="number">0.01368798447</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">268</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">193</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">27.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.1</span>%&nbsp;&nbsp;<span class="number">41.9</span>%&nbsp;&nbsp;<span class="number">35.2</span> <span class="number">0.028</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1789073635</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">210</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">251</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24.8</span>%&nbsp;&nbsp;<span class="number">54.4</span>% -<span class="number">15.4</span> <span class="number">0.005</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">177</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">439</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">43.4</span>%&nbsp;&nbsp;<span class="number">71.3</span>% -<span class="number">88.4</span> <span class="number">0.225</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.463</span>
         $`WOE Table <span class="keyword">for</span> rbci`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.1718377948</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">421</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">195</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">42.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.3</span>%&nbsp;&nbsp;<span class="number">31.7</span>%&nbsp;&nbsp;<span class="number">79.4</span> <span class="number">0.185</span>
         <span class="number">2</span> &lt;= -<span class="number">0.09060410462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">153</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">86</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">67</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.6</span>%&nbsp;&nbsp;<span class="number">43.8</span>%&nbsp;&nbsp;<span class="number">27.4</span> <span class="number">0.006</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.3208178176</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">391</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">532</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">39.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">52.6</span>%&nbsp;&nbsp;<span class="number">57.6</span>% -<span class="number">28.4</span> <span class="number">0.037</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">90</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">218</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.5</span>%&nbsp;&nbsp;<span class="number">70.8</span>% -<span class="number">86.1</span> <span class="number">0.107</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.335</span>
         $`WOE Table <span class="keyword">for</span> v.rbci`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.1837437563</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">406</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">210</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">41.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.8</span>%&nbsp;&nbsp;<span class="number">34.1</span>%&nbsp;&nbsp;<span class="number">68.3</span> <span class="number">0.139</span>
         <span class="number">2</span> &lt;= <span class="number">0.03581374495</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">253</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">208</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">25.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.6</span>%&nbsp;&nbsp;<span class="number">45.1</span>%&nbsp;&nbsp;<span class="number">22.0</span> <span class="number">0.011</span>
         <span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.2503922644</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">194</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">267</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.4</span>%&nbsp;&nbsp;<span class="number">57.9</span>% -<span class="number">29.5</span> <span class="number">0.020</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">135</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">327</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32.3</span>%&nbsp;&nbsp;<span class="number">70.8</span>% -<span class="number">86.1</span> <span class="number">0.161</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.331</span>
         $`WOE Table <span class="keyword">for</span> v.satl`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp;&nbsp;&nbsp;WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.01840058612</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">585</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">338</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">59.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.4</span>%&nbsp;&nbsp;<span class="number">36.6</span>%&nbsp;&nbsp; <span class="number">57.3</span> <span class="number">0.148</span>
         <span class="number">2</span>&nbsp;&nbsp; &lt;= <span class="number">0.3247097195</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">316</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">453</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32.0</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44.8</span>%&nbsp;&nbsp;<span class="number">58.9</span>%&nbsp;&nbsp;-<span class="number">33.6</span> <span class="number">0.043</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.4003869443</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">122</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">3.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">12.1</span>%&nbsp;&nbsp;<span class="number">79.2</span>% -<span class="number">131.4</span> <span class="number">0.116</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">55</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">99</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.6</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.8</span>%&nbsp;&nbsp;<span class="number">64.3</span>%&nbsp;&nbsp;-<span class="number">56.4</span> <span class="number">0.024</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp; NA <span class="number">0.330</span>
         $`WOE Table <span class="keyword">for</span> v.stlm`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.4030051922</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">118</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">11.9</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">3.6</span>%&nbsp;&nbsp;<span class="number">23.4</span>% <span class="number">121.1</span> <span class="number">0.102</span>
         <span class="number">2</span> &lt;= -<span class="number">0.1867821117</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">282</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">180</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">28.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.8</span>%&nbsp;&nbsp;<span class="number">39.0</span>%&nbsp;&nbsp;<span class="number">47.3</span> <span class="number">0.051</span>
         <span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.1141896118</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">301</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">314</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">31.0</span>%&nbsp;&nbsp;<span class="number">51.1</span>%&nbsp;&nbsp;-<span class="number">1.8</span> <span class="number">0.000</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">287</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">482</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">29.0</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">47.6</span>%&nbsp;&nbsp;<span class="number">62.7</span>% -<span class="number">49.4</span> <span class="number">0.092</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.244</span>
         $`WOE Table <span class="keyword">for</span> pcci`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.1738420887</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">397</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">219</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">40.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.6</span>%&nbsp;&nbsp;<span class="number">35.6</span>%&nbsp;&nbsp;<span class="number">61.9</span> <span class="number">0.115</span>
         <span class="number">2</span> &lt;= -<span class="number">0.03163945242</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">307</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">165</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">142</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.0</span>%&nbsp;&nbsp;<span class="number">46.3</span>%&nbsp;&nbsp;<span class="number">17.4</span> <span class="number">0.005</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.2553612644</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">270</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">345</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">27.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">34.1</span>%&nbsp;&nbsp;<span class="number">56.1</span>% -<span class="number">22.1</span> <span class="number">0.015</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">156</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">306</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.8</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.2</span>%&nbsp;&nbsp;<span class="number">66.2</span>% -<span class="number">65.0</span> <span class="number">0.094</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.228</span>
         $`WOE Table <span class="keyword">for</span> v.ftlm`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.03697698898</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">555</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">368</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">56.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36.4</span>%&nbsp;&nbsp;<span class="number">39.9</span>%&nbsp;&nbsp;<span class="number">43.5</span> <span class="number">0.086</span>
         <span class="number">2</span>&nbsp;&nbsp; &lt;= <span class="number">0.2437475615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">279</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">336</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">28.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.2</span>%&nbsp;&nbsp;<span class="number">54.6</span>% -<span class="number">16.2</span> <span class="number">0.008</span>
         <span class="number">3</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.4</span>%&nbsp;&nbsp;<span class="number">66.7</span>% -<span class="number">66.9</span> <span class="number">0.099</span>
         <span class="number">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.194</span>
         $`WOE Table <span class="keyword">for</span> v.rftl`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.1578370554</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">372</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">244</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">37.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24.1</span>%&nbsp;&nbsp;<span class="number">39.6</span>%&nbsp;&nbsp;<span class="number">44.6</span> <span class="number">0.060</span>
         <span class="number">2</span>&nbsp;&nbsp;&lt;= <span class="number">0.1880959621</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">768</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">384</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">384</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">37.9</span>%&nbsp;&nbsp;<span class="number">50.0</span>%&nbsp;&nbsp; <span class="number">2.4</span> <span class="number">0.000</span>
         <span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.3289762494</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">129</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">179</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.7</span>%&nbsp;&nbsp;<span class="number">58.1</span>% -<span class="number">30.4</span> <span class="number">0.014</span>
         <span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">103</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">205</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.3</span>%&nbsp;&nbsp;<span class="number">66.6</span>% -<span class="number">66.4</span> <span class="number">0.065</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.140</span>
         $`WOE Table <span class="keyword">for</span> stlm`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span> &lt;= -<span class="number">0.4586732186</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">60</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">94</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.3</span>%&nbsp;&nbsp;<span class="number">61.0</span>% -<span class="number">42.5</span> <span class="number">0.014</span>
         <span class="number">2</span> &lt;= -<span class="number">0.1688696056</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">266</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">196</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.4</span>%&nbsp;&nbsp;<span class="number">42.4</span>%&nbsp;&nbsp;<span class="number">32.9</span> <span class="number">0.025</span>
         <span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.2631157075</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">922</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">440</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">482</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">47.6</span>%&nbsp;&nbsp;<span class="number">52.3</span>%&nbsp;&nbsp;-<span class="number">6.7</span> <span class="number">0.002</span>
         <span class="number">4</span>&nbsp;&nbsp;&lt;= <span class="number">0.3592235072</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">97</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">57</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.6</span>%&nbsp;&nbsp;<span class="number">37.0</span>%&nbsp;&nbsp;<span class="number">55.6</span> <span class="number">0.023</span>
         <span class="number">5</span>&nbsp;&nbsp;&lt;= <span class="number">0.4846279843</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">81</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">73</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.2</span>%&nbsp;&nbsp;<span class="number">47.4</span>%&nbsp;&nbsp;<span class="number">12.8</span> <span class="number">0.001</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">110</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">4.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.9</span>%&nbsp;&nbsp;<span class="number">71.4</span>% -<span class="number">89.2</span> <span class="number">0.057</span>
         <span class="number">8</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.122</span>
         $`WOE Table <span class="keyword">for</span> v.rstl`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.4541701981</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">94</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">60</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.9</span>%&nbsp;&nbsp;<span class="number">39.0</span>%&nbsp;&nbsp;<span class="number">47.3</span> <span class="number">0.017</span>
         <span class="number">2</span>&nbsp;&nbsp;&lt;= -<span class="number">0.3526306487</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">62</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">92</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.1</span>%&nbsp;&nbsp;<span class="number">59.7</span>% -<span class="number">37.1</span> <span class="number">0.010</span>
         <span class="number">3</span>&nbsp;&nbsp;&lt;= -<span class="number">0.2496412214</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">53</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">101</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.0</span>%&nbsp;&nbsp;<span class="number">65.6</span>% -<span class="number">62.1</span> <span class="number">0.029</span>
         <span class="number">4</span> &lt;= -<span class="number">0.08554320418</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">307</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">142</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">165</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.3</span>%&nbsp;&nbsp;<span class="number">53.7</span>% -<span class="number">12.6</span> <span class="number">0.002</span>
         <span class="number">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&lt;= <span class="number">0.360854678</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">491</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">432</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">49.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">42.7</span>%&nbsp;&nbsp;<span class="number">46.8</span>%&nbsp;&nbsp;<span class="number">15.2</span> <span class="number">0.011</span>
         <span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">146</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">162</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.8</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.0</span>%&nbsp;&nbsp;<span class="number">52.6</span>%&nbsp;&nbsp;-<span class="number">8.0</span> <span class="number">0.001</span>
         <span class="number">8</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.070</span>
         $`WOE Table <span class="keyword">for</span> v.pcci`
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
         <span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.4410911486</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">92</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">62</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.1</span>%&nbsp;&nbsp;<span class="number">40.3</span>%&nbsp;&nbsp;<span class="number">41.9</span> <span class="number">0.013</span>
         <span class="number">2</span> &lt;= -<span class="number">0.03637567714</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">400</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">369</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">40.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36.5</span>%&nbsp;&nbsp;<span class="number">48.0</span>%&nbsp;&nbsp;<span class="number">10.5</span> <span class="number">0.004</span>
         <span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1801156117</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">206</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">255</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">25.2</span>%&nbsp;&nbsp;<span class="number">55.3</span>% -<span class="number">18.9</span> <span class="number">0.008</span>
         <span class="number">4</span>&nbsp;&nbsp; &lt;= <span class="number">0.2480148615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">84</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">70</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.9</span>%&nbsp;&nbsp;<span class="number">45.5</span>%&nbsp;&nbsp;<span class="number">20.6</span> <span class="number">0.003</span>
         <span class="number">5</span>&nbsp;&nbsp; &lt;= <span class="number">0.3348752487</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">67</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">87</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.6</span>%&nbsp;&nbsp;<span class="number">56.5</span>% -<span class="number">23.7</span> <span class="number">0.004</span>
         <span class="number">6</span>&nbsp;&nbsp; &lt;= <span class="number">0.4397404288</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">76</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">78</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;-<span class="number">0.2</span> <span class="number">0.000</span>
         <span class="number">7</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">63</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">91</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.0</span>%&nbsp;&nbsp;<span class="number">59.1</span>% -<span class="number">34.4</span> <span class="number">0.009</span>
         <span class="number">9</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.042</span>
         </pre>
         <p>A tabela possui os seguintes valores para cada variável:&nbsp;</p>
         <ul>
         <li><i>Final.Bin</i> — limites de bin;</li>
         <li><i>Total.Count</i>&nbsp; — número total de amostras em bin;</li>
         <li><i>Total.Distr</i> — número relativo de amostras em bin;</li>
         <li><i>0.Count</i> — número de amostras pertencentes à classe&nbsp;"0";</li>
         <li><i>1.Count</i> — número de amostras pertencentes à classe&nbsp;"1";</li>
         <li><i>0.Distr —</i> — número relativo de amostras pertencentes à classe&nbsp;"0";</li>
         <li><i>1.Distr</i> — número relativo de amostras pertencentes à classe&nbsp;"1";</li>
         <li><i>1.Rate</i> — razão percentual das amostras da classe "1" para o número de amostras da classe "0";</li>
         <li>&nbsp;<i>WOE</i> — capacidade preditiva das bins;</li>
         <li><i>&nbsp;IV </i> — importância estatística das bins. <br>
         </li>
         </ul>
         </div>
         <p>A representação gráfica será mais ilustrativa. Desenhe os
         gráficos&nbsp;WOE de todas as variáveis ​​na ordem crescente da sua IV
         com base nesta tabela:</p>
         <pre class="code">&gt; evalq(woe.binning.plot(preCut), env)</pre>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_9.png" title="WOE 8" alt="WOE 8" style="vertical-align:middle;" width="734" height="592"></p>
         <p style="text-align:center;"><span class="small">Fig.8. WOE das 4 melhores variáveis</span></p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_10.png" title="WOE 10" alt="WOE 10" style="vertical-align:middle;" width="750" height="778"></p>
         <p style="text-align:center;"><span class="small">Fig.9. WOE das variáveis ​​5-8</span></p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_11.png" title="WOE 11" alt="WOE 11" style="vertical-align:middle;" width="750" height="750"></p>
         <p style="text-align:center;"><span class="small">Fig.10. WOE de variáveis ​​de entrada 9-12&nbsp;</span></p>
         <p>Gráfico da variável total das variáveis ​​por sua IV.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/IV_8.png" title="IV  " alt="IV  " style="vertical-align:middle;" width="741" height="602"></p>
         <p style="text-align:center;"><span class="small">Fig.11. Intervalo de variáveis ​​pela sua IV</span></p>
         <p>Nós não vamos usar duas variáveis ​​insignificantes v.rstl e v.pcci,
         que têm IV &lt; 0.1. Nós podemos ver das tabelas que de 10 variáveis
         significativas, apenas a v.satl e stlm têm uma relação não-linear com a
         variável objetivo. Outras variáveis ​​têm uma relação linear. <br>
         </p>
         <p>Para experiências adicionais, nós precisamos criar três conjuntos. Eles são:</p>
         <ul>
         <li><i>DTbin</i> é um conjunto de dados onde os preditores numéricos
         contínuos são transformados em fatores com o número de níveis iguais ao
         número de caixas nas quais eles são divididos;</li>
         <li><i>DTdum</i> é um conjunto de dados onde os preditores de fatores
         do conjunto de dados DTbin são transformados em variáveis ​​binárias
         artificiais;</li>
         <li><i>DTwoe</i> é um conjunto de dados onde os preditores de fatores
         são transformados em variáveis ​​numéricas, substituindo seus níveis
         pelos valores de WOE desses níveis.</li>
         </ul>
         <p>O primeiro conjunto de DTbin é necessário para treinamento e obtenção
         das métricas do modelo básico. O segundo e terceiro conjuntos serão
         utilizados para o treinamento da DNN e para comparar a eficiência desses
         dois métodos de transformação.</p>
         <p>A função <b>woe.binning.deploy()</b> do pacote <i>woebinning</i> nos permitirá resolver esse problema com bastante facilidade. Os seguintes dados devem ser passados ​​para a função:</p>
         <ul>
         <li>&nbsp;quadro de dados com preditores e a variável objetivo, onde a variável objetivo deve ter o valor de 0 ou 1;</li>
         <li>&nbsp;parâmetros de discretização, obtidos na fase anterior (preCut);</li>
         <li>&nbsp;nomes das variáveis ​​que precisam ser categorizadas. Se
         todas as variáveis ​​devem ser categorizadas, basta especificar o nome
         do quadro de dados;</li>
         <li>&nbsp;especificar o IV mínimo para que as variáveis ​​não sejam categorizadas;</li>
         <li>&nbsp;especificar quais variáveis ​​adicionais (exceto as
         categorizadas) que queremos obter. Existem duas variantes - "woe" e
         "dum".&nbsp;</li>
         </ul>
         <p>A função retorna um quadro de dados contendo variáveis ​​iniciais,
         variáveis ​​categorizadas e variáveis ​​adicionais (se elas foram
         especificadas). Os nomes das variáveis ​​recém-criadas são criados
         adicionando um prefixo ou sufixo correspondente ao nome da variável
         inicial. Dessa forma, os prefixos de todas as variáveis ​​adicionais são
         "dum" ou "woe" e as variáveis ​​categorizadas possuem o sufixo
         "binned". Vamos escrever uma função <span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"><i>DiscretizeData()</i></span>, o que transformará o conjunto de dados inicial usando a woe.binning.deploy().</p>
         <pre class="code">DiscretizeData &lt;- function(X, preCut, <span class="keyword">var</span>){
         &nbsp;&nbsp;require(<span class="keyword">foreach</span>)
         &nbsp;&nbsp;require(woeBinning)
         &nbsp;&nbsp;DTd &lt;- list()
         &nbsp;&nbsp;<span class="keyword">foreach</span>(i = <span class="number">1</span>:length(X)) %<span class="keyword">do</span>% {
         &nbsp;&nbsp;&nbsp;&nbsp;X[[i]] %&gt;% <span class="keyword">select</span>(-Data) %&gt;% targ.<span class="keyword">int</span>() %&gt;%
         &nbsp;&nbsp;&nbsp;&nbsp;woe.binning.deploy(preCut, min.iv.total = <span class="number">0.1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; add.woe.or.dum.<span class="keyword">var</span> = <span class="keyword">var</span>) -&gt; res
         &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>(res)
         &nbsp;&nbsp;} -&gt; DTd
         &nbsp;&nbsp;list(pretrain = DTd[[<span class="number">1</span>]] ,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train = DTd[[<span class="number">2</span>]] ,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val =&nbsp;&nbsp; DTd[[<span class="number">3</span>]] ,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test =&nbsp;&nbsp;DTd[[<span class="number">4</span>]] ) -&gt; DTd
         &nbsp;&nbsp;<span class="keyword">return</span>(DTd)
         }
         </pre>
         <br>
         <p>Os parâmetros de entrada da função são dados iniciais (lista X) com os slots <i>pretrain/train/val/test</i>, parâmetros de discretização<i> preCut</i> e o tipo da variável adicional<i> (string </i><i>var</i><i>)</i>. </p>
         <p>A função removerá a variável "Data" de cada slot e alterará a
         variável objetivo - fator "Class" para a variável de objetivo numérico
         "Cl". Com base nisso, ela enviará woe.binning.deploy() para a entrada da
         função. Além disso, especificamos nos parâmetros de entrada desta
         função o IV mínimo = 0.1 para incluir as variáveis ​​no conjunto de
         saída. Na saída, receberemos uma lista com os mesmos slots <i>pretrain/train/val/test</i>.
         Em cada slot, as variáveis ​​categorizadas e, se solicitadas, as
         variáveis ​​adicionais serão adicionadas às variáveis ​​iniciais. Vamos
         calcular todos os conjuntos necessários e adicionar os dados brutos do
         conjunto DTcap.n para eles.</p>
         <pre class="code">evalq({
         &nbsp;&nbsp;require(dplyr)
         &nbsp;&nbsp;require(<span class="keyword">foreach</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;DTbin = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">""</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;DTwoe = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">"woe"</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;DTdum = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">"dum"</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;X.woe &lt;- list()
         &nbsp;&nbsp;&nbsp;&nbsp;X.bin &lt;- list()
         &nbsp;&nbsp;&nbsp;&nbsp;X.dum &lt;- list()
         &nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">foreach</span>(i = <span class="number">1</span>:length(DTcap.n)) %<span class="keyword">do</span>% {
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTbin[[i]] %&gt;% <span class="keyword">select</span>(contains(<span class="string">"binned"</span>)) -&gt; X.bin[[i]]
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTdum[[i]] %&gt;% <span class="keyword">select</span>(starts_with(<span class="string">"dum"</span>)) -&gt; X.dum[[i]]
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTwoe[[i]] %&gt;% <span class="keyword">select</span>(starts_with(<span class="string">"woe"</span>)) %&gt;%
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;divide_by(<span class="number">100</span>) -&gt; X.woe[[i]]
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>(list(bin =&nbsp;&nbsp;X.bin[[i]], woe = X.woe[[i]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dum = X.dum[[i]], raw = DTcap.n[[i]]))
         &nbsp;&nbsp;&nbsp;&nbsp;} -&gt; DTcut
         &nbsp;&nbsp;&nbsp;&nbsp;list(pretrain = DTcut[[<span class="number">1</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train = DTcut[[<span class="number">2</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val =&nbsp;&nbsp; DTcut[[<span class="number">3</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test =&nbsp;&nbsp;DTcut[[<span class="number">4</span>]] ) -&gt; DTcut
         &nbsp;&nbsp;&nbsp;&nbsp;rm(DTwoe, DTdum, X.woe, X.bin, X.dum)
         },
         env)
         </pre>
         <br>
         <p>Uma vez que a WOE é um valor percentual, nós podemos dividir o WOE
         por 100 e obter valores de variáveis ​​que podem ser enviadas para as
         entradas da rede neural sem normalização adicional. Vejamos a estrutura
         do slot obtido, por exemplo de DTcut$<i>val.</i></p>
         <pre class="code">&gt; env$DTcut$val %&gt;% str()
         List of 4
         $ bin:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;10 variables:
         &nbsp;&nbsp;..$ v.fatl.binned: Factor w/ 5 levels "(-Inf,-0.3904381926]",..: 1 1 3 2 4 3 4 4 4 4 ...
         &nbsp;&nbsp;..$ ftlm.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.2344708291]",..: 2 1 1 1 2 2 3 4 4 4 ...
         &nbsp;&nbsp;..$ rbci.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.1718377948]",..: 2 1 2 1 2 3 3 3 4 4 ...
         &nbsp;&nbsp;..$ v.rbci.binned: Factor w/ 5 levels "(-Inf,-0.1837437563]",..: 1 1 3 2 4 3 4 4 4 4 ...
         &nbsp;&nbsp;..$ v.satl.binned: Factor w/ 5 levels "(-Inf,-0.01840058612]",..: 1 1 1 1 1 1 1 1 1 2 ...
         &nbsp;&nbsp;..$ v.stlm.binned: Factor w/ 5 levels "(-Inf,-0.4030051922]",..: 2 2 3 2 3 2 3 3 4 4 ...
         &nbsp;&nbsp;..$ pcci.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.1738420887]",..: 1 1 4 2 4 2 4 2 2 3 ...
         &nbsp;&nbsp;..$ v.ftlm.binned: Factor w/ 4 levels "(-Inf,-0.03697698898]",..: 1 1 3 2 3 2 3 3 2 2 ...
         &nbsp;&nbsp;..$ v.rftl.binned: Factor w/ 5 levels "(-Inf,-0.1578370554]",..: 2 1 1 1 1 1 1 2 2 2 ...
         &nbsp;&nbsp;..$ stlm.binned&nbsp;&nbsp;: Factor w/ 7 levels "(-Inf,-0.4586732186]",..: 2 2 2 2 1 1 1 1 1 2 ...
         $ woe:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;10 variables:
         &nbsp;&nbsp;..$ woe.v.fatl.binned: num [1:501] 1.713 1.713 -0.145 0.632 -0.897 ...
         &nbsp;&nbsp;..$ woe.ftlm.binned&nbsp;&nbsp;: num [1:501] 0.352 0.972 0.972 0.972 0.352 ...
         &nbsp;&nbsp;..$ woe.rbci.binned&nbsp;&nbsp;: num [1:501] 0.274 0.794 0.274 0.794 0.274 ...
         &nbsp;&nbsp;..$ woe.v.rbci.binned: num [1:501] 0.683 0.683 -0.295 0.22 -0.861 ...
         &nbsp;&nbsp;..$ woe.v.satl.binned: num [1:501] 0.573 0.573 0.573 0.573 0.573 ...
         &nbsp;&nbsp;..$ woe.v.stlm.binned: num [1:501] 0.473 0.473 -0.0183 0.473 -0.0183 ...
         &nbsp;&nbsp;..$ woe.pcci.binned&nbsp;&nbsp;: num [1:501] 0.619 0.619 -0.65 0.174 -0.65 ...
         &nbsp;&nbsp;..$ woe.v.ftlm.binned: num [1:501] 0.435 0.435 -0.669 -0.162 -0.669 ...
         &nbsp;&nbsp;..$ woe.v.rftl.binned: num [1:501] 0.024 0.446 0.446 0.446 0.446 ...
         &nbsp;&nbsp;..$ woe.stlm.binned&nbsp;&nbsp;: num [1:501] 0.329 0.329 0.329 0.329 -0.425 ...
         $ dum:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;41 variables:
         &nbsp;&nbsp;..$ dum.v.fatl.-Inf.-0.3904381926.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.v.fatl.-0.03713814085.0.1130198981.binned : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.v.fatl.-0.3904381926.-0.03713814085.binned: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.v.fatl.0.1130198981.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.ftlm.-0.2344708291.-0.01368798447.binned&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.ftlm.-Inf.-0.2344708291.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.ftlm.-0.01368798447.0.1789073635.binned&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.ftlm.0.1789073635.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;.......................................................................................
         &nbsp;&nbsp;..$ dum.stlm.-Inf.-0.4586732186.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.stlm.-0.1688696056.0.2631157075.binned&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.stlm.0.2631157075.0.3592235072.binned&nbsp;&nbsp;&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.stlm.0.3592235072.0.4846279843.binned&nbsp;&nbsp;&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         &nbsp;&nbsp;..$ dum.stlm.0.4846279843.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
         $ raw:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;14 variables:
         &nbsp;&nbsp;..$ Data&nbsp;&nbsp;: POSIXct[1:501], format: "2017-02-23 15:30:00" "2017-02-23 15:45:00" ...
         &nbsp;&nbsp;..$ ftlm&nbsp;&nbsp;: num [1:501] -0.223 -0.374 -0.262 -0.31 -0.201 ...
         &nbsp;&nbsp;..$ stlm&nbsp;&nbsp;: num [1:501] -0.189 -0.257 -0.271 -0.389 -0.473 ...
         &nbsp;&nbsp;..$ rbci&nbsp;&nbsp;: num [1:501] -0.0945 -0.1925 -0.1348 -0.1801 -0.1192 ...
         &nbsp;&nbsp;..$ pcci&nbsp;&nbsp;: num [1:501] -0.5714 -0.2602 0.4459 -0.0478 0.2596 ...
         &nbsp;&nbsp;..$ v.fatl: num [1:501] -0.426 -0.3977 0.0936 -0.1512 0.1178 ...
         &nbsp;&nbsp;..$ v.satl: num [1:501] -0.35 -0.392 -0.177 -0.356 -0.316 ...
         &nbsp;&nbsp;..$ v.rftl: num [1:501] -0.0547 -0.2065 -0.3253 -0.4185 -0.4589 ...
         &nbsp;&nbsp;..$ v.rstl: num [1:501] 0.0153 -0.0273 -0.0636 -0.1281 -0.15 ...
         &nbsp;&nbsp;..$ v.ftlm: num [1:501] -0.321 -0.217 0.253 0.101 0.345 ...
         &nbsp;&nbsp;..$ v.stlm: num [1:501] -0.288 -0.3 -0.109 -0.219 -0.176 ...
         &nbsp;&nbsp;..$ v.rbci: num [1:501] -0.2923 -0.2403 0.1909 0.0116 0.2868 ...
         &nbsp;&nbsp;..$ v.pcci: num [1:501] -0.0298 0.3738 0.6153 -0.5643 0.2742 ...
         &nbsp;&nbsp;..$ Class : Factor w/ 2 levels "-1","1": 1 1 1 1 2 2 2 2 2 1 ...
         </pre>
         <br>
         <p>Como você pode ver, o slot <i>bin</i> contém 10 variáveis ​​de fatores com diferentes números de níveis. Eles têm o sufixo "binned". O slot <i>woe</i> contém 10 variáveis ​​com níveis de fator alterados para o seu WOE (eles têm o prefixo "woe"). O slot <i>dum </i> tem 41 variáveis ​​numéricas <i>artificiais</i>
         com os valores (0, 1) obtidos das variáveis ​​de fatores através da
         codificação um para um (tem o prefixo "dum"). Existem 14 variáveis ​​no
         slot <i>raw</i>. Eles são Data — timestamp, Class — variável fator objetivo e 12 preditores numéricos.</p>
         <p>Temos todos os dados que precisaremos para outras experiências. Os
         objetos listados abaixo devem estar no ambiente env até agora. Deixe-nos
         salvar a área de trabalho com esses objetos para o arquivo <i>PartIV.RData</i>.</p>
         <pre class="code">&gt; ls(env)
         [<span class="number">1</span>] "<span class="predefines">Close</span>"&nbsp;&nbsp;&nbsp;&nbsp;"Data"&nbsp;&nbsp;&nbsp;&nbsp; "dt"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "DT"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "DTbin"&nbsp;&nbsp;&nbsp;&nbsp;"DTcap"&nbsp;&nbsp;&nbsp;"DTcap.n"&nbsp;"DTcut"&nbsp;&nbsp;"<span class="predefines">High</span>"&nbsp;&nbsp;&nbsp;&nbsp;
         [<span class="number">10</span>] "i"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"<span class="predefines">Low</span>"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"<span class="predefines">Open</span>"&nbsp;&nbsp;&nbsp;&nbsp; "pre.outl" "preCut"&nbsp;&nbsp; "preproc"&nbsp;&nbsp;"<span class="predefines">Volume</span>"
         </pre>
         <br>
         <p id="base"><b>2.1.2. Modelo básico de comparação</b></p>
         <p>Nós vamos usar o modelo <i>OneR</i> implementado no pacote <b><a href="https://www.mql5.com/go?link=https://shiring.github.io/machine_learning/2017/04/23/one_r" rel="nofollow" title="https://shiring.github.io/machine_learning/2017/04/23/one_r" target="_blank">OneR</a></b>
         como modelo base. Este modelo é simples, confiável e fácil de
         interpretar. Informações sobre o algoritmo podem ser encontradas na
         descrição do pacote. Este modelo está funcionando apenas com os dados de
         bin. O pacote contém funções auxiliares que podem ser variáveis
         numéricas discretas de diferentes maneiras. Como já transformamos os
         preditores em fatores, não precisamos deles. </p>
         <p>Agora, eu vou elaborar o cálculo mostrado abaixo. Crie os conjuntos
         train/val/test, extraindo os slots correspondentes de DTcut e
         adicionando a classe de variável objetivo para eles. Vamos treinar o
         modelo com o conjunto train.</p>
         <pre class="code">&gt; evalq({
         +&nbsp;&nbsp; require(OneR)
         +&nbsp;&nbsp; require(dplyr)
         +&nbsp;&nbsp; require(magrittr)
         +&nbsp;&nbsp; train &lt;- cbind(DTcut$train$bin, Class = DTcut$train$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
         +&nbsp;&nbsp; val &lt;- cbind(DTcut$val$bin, Class = DTcut$val$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
         +&nbsp;&nbsp; test &lt;- cbind(DTcut$test$bin, Class = DTcut$test$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
         +&nbsp;&nbsp; model &lt;- OneR(data = train, formula = <span class="macro">NULL</span>, ties.method = <span class="string">"chisq"</span>, <span class="preprocessor">#c(<span class="string">"first"</span>,<span class="string">"chisq"</span>
         </span>+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; verbose = <span class="macro">TRUE</span>) <span class="preprocessor">#FALSE, </span><span class="macro">TRUE</span>
         + }, env)
         Loading required package: OneR
         &nbsp;&nbsp;&nbsp;&nbsp;Attribute&nbsp;&nbsp;&nbsp;&nbsp; Accuracy
         <span class="number">1</span> * v.satl.binned <span class="number">63.14</span>%&nbsp;&nbsp;
         <span class="number">2</span>&nbsp;&nbsp; v.fatl.binned <span class="number">62.64</span>%&nbsp;&nbsp;
         <span class="number">3</span>&nbsp;&nbsp; ftlm.binned&nbsp;&nbsp; <span class="number">62.54</span>%&nbsp;&nbsp;
         <span class="number">4</span>&nbsp;&nbsp; pcci.binned&nbsp;&nbsp; <span class="number">61.44</span>%&nbsp;&nbsp;
         <span class="number">5</span>&nbsp;&nbsp; v.rftl.binned <span class="number">59.74</span>%&nbsp;&nbsp;
         <span class="number">6</span>&nbsp;&nbsp; v.rbci.binned <span class="number">58.94</span>%&nbsp;&nbsp;
         <span class="number">7</span>&nbsp;&nbsp; rbci.binned&nbsp;&nbsp; <span class="number">58.64</span>%&nbsp;&nbsp;
         <span class="number">8</span>&nbsp;&nbsp; stlm.binned&nbsp;&nbsp; <span class="number">58.04</span>%&nbsp;&nbsp;
         <span class="number">9</span>&nbsp;&nbsp; v.stlm.binned <span class="number">57.54</span>%&nbsp;&nbsp;
         <span class="number">10</span>&nbsp;&nbsp;v.ftlm.binned <span class="number">56.14</span>%&nbsp;&nbsp;
         ---
         Chosen attribute due to accuracy
         and ties method (<span class="keyword">if</span> applicable): <span class="string">'*'</span>
         Warning message:
         In OneR(data = train, formula = <span class="macro">NULL</span>, ties.method = <span class="string">"chisq"</span>, verbose = <span class="macro">TRUE</span>) :
         &nbsp;&nbsp;data contains unused factor levels
         </pre>
         O modelo selecionou a variável <span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"></span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"><i>v.satl.binned</i></span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"> com a precisão básica de </span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);">63,14% como base para a criação das regras. </span>Vejamos as informações gerais sobre este modelo:
         <pre class="code">&gt; summary(env$model)
         Call:
         OneR(data = train, formula = NULL, ties.method = "chisq", verbose = FALSE)
         Regras:
         If v.satl.binned = (-Inf,-0.01840058612]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then Class = -1
         If v.satl.binned = (-0.01840058612,0.3247097195] then Class = 1
         If v.satl.binned = (0.3247097195,0.4003869443]&nbsp;&nbsp; then Class = 1
         If v.satl.binned = (0.4003869443, Inf]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then Class = 1
         Accuracy:
         632 of 1001 instances classified correctly (63.14%)
         Contingency table:
         &nbsp;&nbsp;&nbsp;&nbsp; v.satl.binned
         Class (-Inf,-0.01840058612] (-0.01840058612,0.3247097195] (0.3247097195,0.4003869443] (0.4003869443, Inf]&nbsp;&nbsp;Sum
         &nbsp;&nbsp;-1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 325&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 161&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;28&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37&nbsp;&nbsp;551
         &nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 143&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * 229&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 35&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 43&nbsp;&nbsp;450
         &nbsp;&nbsp;Sum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 468&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 390&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;63&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80 1001
         ---
         Maximum in each column: '*'
         Pearson's Chi-squared test:
         X-squared = 74.429, df = 3, p-value = 4.803e-16
         </pre>
         <p>Representação gráfica do resultado de treinamento:</p>
         <pre class="code">plot(env$model)</pre>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/OneR_1.png" title="OneR" alt="OneR" style="vertical-align:middle;" width="750" height="442"></p>
         <p style="text-align:center;"><span class="small">Fig.12. Distribuição das categorias da variável <i>v.satl.binned </i> por classes no modelo</span></p>
         <p>A precisão da previsão durante o treinamento não é muito alta. Nós
         vamos ver a precisão que este modelo mostrará no conjunto de validação:</p>
         <pre class="code">&gt; evalq(res.val &lt;- eval_model(predict(model, val %&gt;% <span class="keyword">as</span>.data.frame()), val$Class),
         +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; env)
         Confusion matrix (absolute):
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
         Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span> Sum
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">106</span>&nbsp;&nbsp;<span class="number">87</span> <span class="number">193</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">100</span> <span class="number">208</span> <span class="number">308</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">206</span> <span class="number">295</span> <span class="number">501</span>
         Confusion matrix (relative):
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
         Prediction&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp;Sum
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">0.21</span> <span class="number">0.17</span> <span class="number">0.39</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">0.20</span> <span class="number">0.42</span> <span class="number">0.61</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">0.41</span> <span class="number">0.59</span> <span class="number">1.00</span>
         Accuracy:
         <span class="number">0.6267</span> (<span class="number">314</span>/<span class="number">501</span>)
         Error rate:
         <span class="number">0.3733</span> (<span class="number">187</span>/<span class="number">501</span>)
         Error rate reduction (vs. <span class="keyword">base</span> rate):
         <span class="number">0.0922</span> (p-<span class="keyword">value</span> = <span class="number">0.04597</span>)
         </pre>
         <p>e no conjunto de teste:</p>
         <pre class="code">&gt; evalq(res.test &lt;- eval_model(predict(model, test %&gt;% <span class="keyword">as</span>.data.frame()), test$Class),
         +&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; env)
         Confusion matrix (absolute):
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
         Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span> Sum
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">130</span> <span class="number">102</span> <span class="number">232</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">76</span> <span class="number">193</span> <span class="number">269</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">206</span> <span class="number">295</span> <span class="number">501</span>
         Confusion matrix (relative):
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
         Prediction&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp;Sum
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">0.26</span> <span class="number">0.20</span> <span class="number">0.46</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">0.15</span> <span class="number">0.39</span> <span class="number">0.54</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">0.41</span> <span class="number">0.59</span> <span class="number">1.00</span>
         Accuracy:
         <span class="number">0.6447</span> (<span class="number">323</span>/<span class="number">501</span>)
         Error rate:
         <span class="number">0.3553</span> (<span class="number">178</span>/<span class="number">501</span>)
         Error rate reduction (vs. <span class="keyword">base</span> rate):
         <span class="number">0.1359</span> (p-<span class="keyword">value</span> = <span class="number">0.005976</span>)
         </pre>
         <p>Os resultados não são encorajadores. A <i>redução da taxa de erro</i>
         mostra como a precisão aumentou em relação ao nível base (0.5). O baixo
         valor de p (&lt; 0.05) indica que este modelo é capaz de produzir
         previsões melhor do que o nível básico. A precisão do conjunto de teste é
         0.6447 (323/501), que é maior do que a precisão do conjunto de
         validação. O conjunto de teste está mais longe do conjunto de
         treinamento do que o conjunto de validação. Este resultado será o ponto
         de referência para comparar resultados de previsão de nossos futuros
         modelos.</p>
         <br>
         <p id="structure"><b> 2.1.3. Estrutura de uma DNN</b></p>
         <p>Nós usaremos três conjuntos de dados para treinar e testar:</p>
         <ol>
         <li> DTcut$$raw — 12 variáveis ​​de entrada (outliers imputados e normalizados).</li>
         <li>DTcut$$dum — 41 variáveis ​​binárias. <br>
         </li>
         <li>DTcut$$woe — 10 variáveis ​​numéricas.</li>
         </ol>
         <p> Nós vamos usar com todos os conjuntos de dados a variável Class = fator com dois níveis. Estrutura das redes neurais:</p>
         <ul>
         <li>DNNraw - layers = c(12, 16, 8(2), 2), funções de ativação c(tanh, maxout(lin), softmax)</li>
         <li>DNNwoe - layers = c(10, 16, 8(2), 2), funções de ativação c(tanh, maxout(lin), softmax)</li>
         <li>DNNdum - layers = c(41, 50, 8(2), 2), funções de ativação c(ReLU, maxout(ReLU), softmax)</li>
         </ul>
         <p>O diagrama abaixo mostra a estrutura da rede neural DNNwoe. A rede
         neural possui uma camada de entrada, duas camadas ocultas e uma camada
         de saída. Duas outras redes neurais ( DNNdum, DNNraw) têm uma estrutura
         semelhante. Eles apenas diferem no número de neurônios em camadas e
         funções de ativação.</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/strDNN__2.png" title="estrutura DNN_!" alt="estrutura DNN_!" style="vertical-align:middle;" width="571" height="634"><br>
         </p>
         <p style="text-align:center;"><span class="small">Fig.13. Estrutura da rede neural DNNwoe</span></p>
         <br>
         <p id="methods"><b> 2.1.4. Variantes de treinamento</b></p>
         <p id="pretraining"> <b>Com pré-treinamento</b></p>
         <p>O treinamento terá duas etapas</p>
         <ul>
         <li>pré-treinamento do SRBM com o conjunto /pretrain seguido do
         treinamento apenas da camada superior da rede neural, validação com o
         conjunto train e parâmetros — par_0;</li>
         <li>ajuste fino de toda a rede com os conjuntos de train/val e parâmetros par_1.</li>
         </ul>
         <p>&nbsp;Nós podemos salvar os modelos intermediários de ajuste fino,
         mas não é obrigatório. O modelo que mostra os melhores resultados de
         treinamento deve ser salvo. Os parâmetros dessas duas etapas devem
         conter:</p>
         <ul>
         <li>par_0 — parâmetros gerais da rede neural, parâmetros de
         treinamento da RBM e parâmetros de treinamento da camada superior da
         DNN;</li>
         <li>par_1 — parâmetros de treinamento de todas as camadas da DNN.</li>
         </ul>
         <p>Todos os parâmetros do DArch têm valores padrão. Se precisamos de
         parâmetros diferentes em uma determinada etapa do treinamento, nós
         podemos configurá-los por uma lista e eles substituirão os parâmetros
         padrão. Após o primeiro estágio de treinamento, nós obteremos a
         estrutura DArch com parâmetros e resultados de treinamento (erro de
         treinamento, erro de teste etc) e também a rede neural iniciada com os
         pesos do SRBM treinado. Para completar o segundo estágio de treinamento,
         você precisa incluir a estrutura DArch obtida na primeira etapa na
         lista de parâmetros para esta etapa de treinamento. Naturalmente, nós
         precisaremos de conjuntos de treinamento e de validação.</p>
         <p>Consideremos os parâmetros necessários para o primeiro estágio de
         treinamento (pré-treinamento do SRBM e treinamento da camada superior da
         rede neural) e executá-lo:</p>
         <pre class="code"><span class="preprocessor">##=====CODE </span>I etap===========================
         evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(dplyr)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// the number of input and output neurons will be identified automatically from the data set </span>
         &nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
         &nbsp;&nbsp;<span class="preprocessor">#------------------
         &nbsp;&nbsp;</span>par_0 &lt;- list(
         &nbsp;&nbsp;&nbsp;&nbsp;layers = Ln, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// let us take this parameter out of the list (for simplicity)</span>
         &nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// if we want to obtain identical data during initialization</span>
         &nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="comment">// what level of information output we require</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor"># </span>params RBM========================
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rbm.consecutive = F, <span class="preprocessor"># </span>each RBM is trained one epoch at a time
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = nEp_0,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.batchSize = <span class="number">50</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.allData = <span class="macro">TRUE</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.lastLayer = -<span class="number">1</span>, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// do not train the upper layer of SRBM</span>
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.learnRate = <span class="number">0.3</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.unitFunction = <span class="string">"tanhUnitRbm"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor"># </span>params NN ========================
         &nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_0,<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="comment">// take this parameter out of the list for simplicity</span>
         &nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(F,F,T), <span class="preprocessor">#обучать&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">//upper layer only </span>
         &nbsp;&nbsp;&nbsp;&nbsp;darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = <span class="number">0.5</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation
         &nbsp;&nbsp;&nbsp;&nbsp;</span>normalizeWeights = T,
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = <span class="string">"linearUnit"</span>)
         <span class="preprocessor">#---------------------------
         &nbsp;&nbsp;
         &nbsp;&nbsp;</span>DNN_default &lt;- darch(darch = <span class="macro">NULL</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; paramsList = par_0,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut$pretrain$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut$pretrain$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut$train$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut$train$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
         }, env)
         </pre>
         Resultado após a conclusão do primeiro estágio de treinamento:
         <pre class="code">...........................<span style="background-color:rgb(255, 246, 200);">
         </span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Train <span class="keyword">set</span> (best model): <span class="number">31.95</span>% (<span class="number">639</span>/<span class="number">2000</span>)</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Train <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.233</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation <span class="keyword">set</span> (best model): <span class="number">35.86</span>% (<span class="number">359</span>/<span class="number">1001</span>)</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Validation <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.306</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Best model was found after epoch <span class="number">3</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Final <span class="number">0.632</span> validation Cross Entropy error: <span class="number">1.279</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Final <span class="number">0.632</span> validation classification error: <span class="number">34.42</span>%
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Fine-tuning finished after <span class="number">5.975</span> secs
         </pre>
         <p>Segunda etapa do treinamento da rede neural:</p>
         <pre class="code"><span class="preprocessor">##=====CODE </span>II etap===========================
         evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(dplyr)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
         &nbsp;&nbsp;bp.learnRate &lt;- <span class="number">1</span>
         &nbsp;&nbsp;par_1 &lt;- list(
         &nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
         &nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);">rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!</span>
         &nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
         &nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);">darch.trainLayers = c(T,T,T)</span>, <span class="preprocessor">#TRUE,
         &nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor">#------------------------------
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>DNN_1 &lt;- darch( darch = DNN_default, paramsList = par_1,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut$train$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut$train$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut$val$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut$val$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
         }, env)
         </pre>
         <p>Resultado da segunda etapa do treinamento: <br>
         </p>
         <pre class="code">...........................
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Finished epoch <span class="number">100</span> of <span class="number">100</span> after <span class="number">0.279</span> secs (<span class="number">3666</span> patterns/sec)
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Train <span class="keyword">set</span> (best model): <span class="number">31.97</span>% (<span class="number">320</span>/<span class="number">1001</span>)</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Train <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.225</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="color:rgb(0, 0, 0); background-color:rgb(255, 246, 200);">Classification error on Validation <span class="keyword">set</span> (best model): <span class="number">31.14</span>% (<span class="number">156</span>/<span class="number">501</span>)</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Validation <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.190</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Best model was found after epoch <span class="number">96</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Final <span class="number">0.632</span> validation Cross Entropy error: <span class="number">1.203</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="background-color:rgb(255, 246, 200);">Final <span class="number">0.632</span> validation classification error: <span class="number">31.44</span>%</span>
         INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Fine-tuning finished after <span class="number">37.22</span> secs
         </pre>
         <p>Gráfico da variação do erro de previsão durante o segundo estágio de treinamento:</p>
         <pre class="code">plot(env$DNN_1, y = <span class="string">"raw"</span>)</pre>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNNwoe_II.png" title="DNNwoe II etap" alt="DNNwoe II etap" style="vertical-align:middle;" width="750" height="442"><br>
         </p>
         <p style="text-align:center;"><span class="small">Fig.14. Variação do erro de classificação durante o segundo estágio de treinamento</span></p>
         <p>Vejamos o erro de classificação do modelo final no conjunto de teste:</p>
         <pre class="code">#-----------
         evalq({
         &nbsp;&nbsp;xValid = DTcut$test$woe %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;Ypredict &lt;- predict(DNN_1, newdata = xValid, type = "<span class="keyword">class</span>")
         &nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
         &nbsp;&nbsp;cat(paste0("Incorrect classifications on all examples: ", numIncorrect, " (",
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), "%)\n"))
         &nbsp;&nbsp; caret::confusionMatrix(yValid, Ypredict)
         }, env)
         <span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: <span class="number">166</span> (<span class="number">33.13</span>%)</span>
         Confusion Matrix and Statistics
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
         Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-<span class="number">1</span> <span class="number">129</span>&nbsp;&nbsp;<span class="number">77</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp; <span class="number">89</span> <span class="number">206</span>
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);"> Accuracy : <span class="number">0.6687</span>&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">95</span>% CI : (<span class="number">0.6255</span>, <span class="number">0.7098</span>)
         &nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : <span class="number">0.5649</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : <span class="number">1.307</span>e-<span class="number">06</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : <span class="number">0.3217</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         Mcnemar's Test P-Value : <span class="number">0.3932</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : <span class="number">0.5917</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : <span class="number">0.7279</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : <span class="number">0.6262</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : <span class="number">0.6983</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : <span class="number">0.4351</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : <span class="number">0.2575</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp; Detection Prevalence : <span class="number">0.4112</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : <span class="number">0.6598</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -<span class="number">1</span>
         #----------------------------------------
         </pre>
         <p>A precisão neste conjunto de dados (woe) com esses parâmetros que
         estão longe de ser ótimos, é muito maior do que a precisão do modelo
         básico. Existe um potencial significativo para aumentar a precisão ao
         otimizar os hiperparâmetros da DNN. Se o cálculo for repetido, os dados
         podem não ser exatamente os mesmos que no artigo.&nbsp;</p>
         <p>Vamos levar nossos scripts para um formulário mais compacto para
         cálculos adicionais com outros conjuntos de dados. Vamos escrever uma
         função para o conjunto woe:</p>
         <pre class="code"><span class="preprocessor">#-------------------
         </span>DNN.train.woe &lt;- function(param, X){
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;darch( darch = <span class="macro">NULL</span>, paramsList = param[[<span class="number">1</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">1</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;) %&gt;%
         &nbsp;&nbsp;&nbsp;&nbsp;darch( ., paramsList = param[[<span class="number">2</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">3</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;&nbsp;&nbsp;) -&gt; Darch
         &nbsp;&nbsp;<span class="keyword">return</span>(Darch)
         }
         </pre>
         <p>Repita os cálculos para o conjunto de dados DTcut$$woe em uma forma compacta:</p>
         <pre class="code">evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
         &nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
         &nbsp;&nbsp;nEp_1 &lt;- <span class="number">25</span>
         &nbsp;&nbsp;rbm.learnRate = c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;list(par_0, par_1) %&gt;% DNN.train.woe(DTcut) -&gt; Dnn.woe
         &nbsp;&nbsp;xValid = DTcut$test$woe %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;Ypredict &lt;- predict(Dnn.woe, newdata = xValid, type = "<span class="keyword">class</span>")
         &nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
         &nbsp;&nbsp;cat(paste0("Incorrect classifications on all examples: ", numIncorrect, " (",
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), "%)\n"))
         &nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.woe
         }, env)
         </pre>
         <p>Faça o cálculo para o conjunto de dados DTcut$$raw:</p>
         <pre class="code">#-------------------------
         DNN.train.raw &lt;- function(param, X){
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;darch( darch = NULL, paramsList = param[[<span class="number">1</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">1</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;) %&gt;%
         &nbsp;&nbsp;&nbsp;&nbsp;darch( ., paramsList = param[[<span class="number">2</span>]],
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">3</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;&nbsp;&nbsp;) -&gt; Darch
         &nbsp;&nbsp;<span class="keyword">return</span>(Darch)
         }
         #-------------------------------
         evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
         &nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
         &nbsp;&nbsp;nEp_1 &lt;- <span class="number">25</span>
         &nbsp;&nbsp;rbm.learnRate = c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;list(par_0, par_1) %&gt;% DNN.train.raw(DTcut) -&gt; Dnn.raw
         &nbsp;&nbsp;xValid = DTcut$test$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class))
         &nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;Ypredict &lt;- predict(Dnn.raw, newdata = xValid, type = <span class="string">"class"</span>)
         &nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
         &nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; round(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
         &nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.raw
         }, env)
         #----------------------------
         </pre>
         <p>Abaixo está o resultado e o gráfico da variação de erro de classificação para este conjunto:</p>
         <pre class="code">&gt; env$cM.raw
         Confusion Matrix and Statistics
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
         Prediction&nbsp;&nbsp;-1&nbsp;&nbsp; 1
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1 133&nbsp;&nbsp;73
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp; 86 209
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color:rgb(255, 246, 200);">Accuracy : 0.6826</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 95% CI : (0.6399, 0.7232)
         &nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : 0.5629&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : 2.667e-08&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : 0.3508&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         Mcnemar's Test P-Value : 0.3413&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : 0.6073&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : 0.7411&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : 0.6456&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : 0.7085&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : 0.4371&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : 0.2655&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp; Detection Prevalence : 0.4112&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : 0.6742&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -1
         #--------------------------------------
         </pre>
         <pre class="code">plot(env$Dnn.raw, y = <span class="string">"raw"</span>)</pre>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn.png" title="Dnn.raw  error" alt="Dnn.raw  error" style="vertical-align:middle;" width="750" height="442"></p>
         <p style="text-align:center;"><span class="small">Fig.15.&nbsp;</span><span>Variação do erro de classificação no segundo estágio</span></p>
         <p>Eu não consegui treinar a rede neural com os dados DTcut$$dum. Você
         pode tentar fazer isso sozinho. Por exemplo, insira os dados DTcut$$bin e
         organize os parâmetros de treinamento para que os preditores sejam
         convertidos artificialmente.<br>
         </p>
         <br>
         <p id="nopretraining"><b> Treinamento sem pré-treinamento </b></p>
         <p>Vamos treinar a rede neural sem pré-treinamento com os mesmos dados
         (woe, raw) nos conjuntos pretrain/train/val. Vamos ver o resultado.</p>
         <pre class="code"><span class="preprocessor">#-------WOE----------------
         </span>evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
         &nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
         &nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;<span class="preprocessor">#--param----------------
         &nbsp;&nbsp;</span>par_1 &lt;- list(
         &nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
         &nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!
         &nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(T,T,T), <span class="preprocessor">#TRUE,
         &nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.0</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
         &nbsp;&nbsp;<span class="preprocessor">#--train---------------------------
         &nbsp;&nbsp;</span>darch( darch = <span class="macro">NULL</span>, paramsList = par_1,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut[[<span class="number">1</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;) -&gt; Dnn.woe.I
         &nbsp;&nbsp;<span class="preprocessor">#---test--------------------------
         &nbsp;&nbsp;</span>xValid = DTcut$val$woe %&gt;% <span class="keyword">as</span>.data.frame()
         &nbsp;&nbsp;yValid = DTcut$val$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;Ypredict &lt;- predict(Dnn.woe.I, newdata = xValid, type = <span class="string">"class"</span>)
         &nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
         &nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
         &nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.woe.I
         }, env)
         <span class="preprocessor">#---------Ris16------------------------------------
         </span>plot(env$Dnn.woe.I, type = <span class="string">"class"</span>)
         env$cM.woe.I
         </pre>
         <p>Métricas:</p>
         <pre class="code">.......................................................
         INFO [2017-09-14 10:38:01] <span style="background-color:rgb(255, 246, 200);">Classification error on Train set (best model): 28.7% (574/2000)</span>
         INFO [2017-09-14 10:38:01] Train set (best model) Cross Entropy error: 1.140
         INFO [2017-09-14 10:38:02] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation set (best model): 35.86% (359/1001)</span>
         INFO [2017-09-14 10:38:02] Validation set (best model) Cross Entropy error: 1.299
         INFO [2017-09-14 10:38:02] Best model was found after epoch 67
         INFO [2017-09-14 10:38:02] Final 0.632 validation Cross Entropy error: 1.241
         INFO [2017-09-14 10:38:02] Final 0.632 validation classification error: 33.23%
         INFO [2017-09-14 10:38:02] Fine-tuning finished after 37.13 secs
         <span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: 150 (29.94%)</span>
         &gt; env$cM.woe.I
         Confusion Matrix and Statistics
         Reference
         Prediction  -1   1
         -1 144  62
         1   88 207
         <span style="background-color:rgb(255, 246, 200);"> Accuracy : 0.7006 </span>
         95% CI : (0.6584, 0.7404)
         No Information Rate : 0.5369
         P-Value [Acc &gt; NIR] : 5.393e-14
         Kappa : 0.3932
         Mcnemar's Test P-Value : 0.04123
         Sensitivity : 0.6207
         Specificity : 0.7695
         Pos Pred Value : 0.6990
         Neg Pred Value : 0.7017
         Prevalence : 0.4631
         Detection Rate : 0.2874
         Detection Prevalence : 0.4112
         Balanced Accuracy : 0.6951
         'Positive' Class : -1
         </pre>
         <p>Gráfico da variação do erro de classificação durante o treinamento:</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn_002.png" title="Dnn.woe.I" alt="Dnn.woe.I" style="vertical-align:middle;" width="750" height="442"></p>
         <p style="text-align:center;"><span class="small">Fig.16.&nbsp;<span>Variação do erro de classificação sem pré-treinamento com o conjunto $woe</span></span></p>
         <p>O mesmo para o conjunto /raw:</p>
         <pre class="code">evalq({
         &nbsp;&nbsp;require(darch)
         &nbsp;&nbsp;require(magrittr)
         &nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
         &nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
         &nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.1</span>)
         &nbsp;&nbsp;<span class="preprocessor">#--param-----------------------------
         &nbsp;&nbsp;</span>par_1 &lt;- list(
         &nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
         &nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!
         &nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(T,T,T), <span class="preprocessor">#TRUE,
         &nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
         &nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
         &nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
         &nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
         &nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
         &nbsp;&nbsp;<span class="preprocessor">#---train------------------------------
         &nbsp;&nbsp;</span>darch( darch = <span class="macro">NULL</span>, paramsList = par_1,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut[[<span class="number">1</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class)) ,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector(),
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class)) ,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;) -&gt; Dnn.raw.I
         &nbsp;&nbsp;<span class="preprocessor">#---test--------------------------------
         &nbsp;&nbsp;</span>xValid = DTcut[[<span class="number">3</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class))
         &nbsp;&nbsp;yValid = DTcut[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector()
         &nbsp;&nbsp;Ypredict &lt;- predict(Dnn.raw.I, newdata = xValid, type = <span class="string">"class"</span>)
         &nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
         &nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
         &nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.raw.I
         }, env)
         <span class="preprocessor">#---------Ris17----------------------------------
         </span>env$cM.raw.I
         plot(env$Dnn.raw.I, type = <span class="string">"class"</span>)
         </pre>
         <p>Métricas:</p>
         <pre class="code">INFO [2017-09-14 11:06:13] <span style="background-color:rgb(255, 246, 200);">Classification error on Train set (best model): 30.75% (615/2000)</span>
         INFO [2017-09-14 11:06:13] Train set (best model) Cross Entropy error: 1.189
         INFO [2017-09-14 11:06:13] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation set (best model): 33.67% (337/1001)</span>
         INFO [2017-09-14 11:06:13] Validation set (best model) Cross Entropy error: 1.236
         INFO [2017-09-14 11:06:13] Best model was found after epoch 45
         INFO [2017-09-14 11:06:13] Final 0.632 validation Cross Entropy error: 1.219
         INFO [2017-09-14 11:06:13] Final 0.632 validation classification error: 32.59%
         INFO [2017-09-14 11:06:13] Fine-tuning finished after 35.47 secs
         <span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: 161 (32.14%)</span>
         &gt; #---------Ris17----------------------------------
         &gt; env$cM.raw.I
         Confusion Matrix and Statistics
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
         Prediction&nbsp;&nbsp;-1&nbsp;&nbsp; 1
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1 140&nbsp;&nbsp;66
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp; 95 200
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);"> Accuracy : 0.6786&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 95% CI : (0.6358, 0.7194)
         &nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : 0.5309&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : 1.283e-11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : 0.3501&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         Mcnemar's Test P-Value : 0.02733&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : 0.5957&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : 0.7519&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : 0.6796&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : 0.6780&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : 0.4691&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : 0.2794&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp; Detection Prevalence : 0.4112&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : 0.6738&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
         </pre>
         <p>Chart of the classification error change:</p>
         <p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn_003.png" title="Dnn.raw.I" alt="Dnn.raw.I" style="vertical-align:middle;" width="750" height="442"></p>
         <p style="text-align:center;"><span class="small"><span>Fig.17.&nbsp;</span><span>Change of the classification error without pretraining with the $raw set</span></span></p>
         <br>
         <h4 id="analythics">2.2. Análise de resultados</h4>
         <p>Vamos colocar o resultado de nossos experimentos em uma tabela:</p>
         <table class="standart" width="100%" cellspacing="0" cellpadding="2">
         <thead>
         <tr>
         <th style="text-align:left;">Tipo de treinamento</th>
         <th>Conjunto /woe</th>
         <th>Conjunto /raw</th>
         </tr>
         </thead>
         <tbody>
         <tr>
         <td style="text-align:left;">Com pré-treinamento</td>
         <td style="text-align:center;">0.6687 (0.6255 - 0.7098)</td>
         <td style="text-align:center;">0.6826(0.6399 - 0.7232)</td>
         </tr>
         <tr>
         <td>Sem pré-treinamento</td>
         <td style="text-align:center;">0.7006(0.6589 - 0.7404)</td>
         <td style="text-align:center;">0.6786(0.6359 - 0.7194)</td>
         </tr>
         </tbody>
         </table>
         <p>O erro de classificação com o pré-treinamento é quase o mesmo em
         ambos os conjuntos. Está na faixa de 30+/-4%. Apesar de um erro menor,
         fica claro a partir do gráfico de variação do erro de classificação que
         houve uma reconversão durante o treinamento sem pré-treinamento (o erro
         nos conjuntos de validação e teste foi significativamente maior que o
         erro de treinamento). Portanto, nós usaremos o treino com o
         pré-treinamento em nossos experimentos adicionais.</p>
         <p>O resultado não é muito maior do que o resultado do modelo básico.
         Nós temos a possibilidade de melhorar as características ao otimizar
         alguns hiperparâmetros. Nós vamos fazer isso no próximo artigo.&nbsp;<br>
         </p>
         <br>
         <h3 id="final">Conclusão</h3>
         <p>Apesar das limitações (por exemplo, apenas dois métodos básicos de
         treinamento), o pacote darch permite criar redes neurais diferentes em
         estrutura e parâmetros. Este pacote é uma boa ferramenta para o estudo
         profundo das redes neurais.<br>
         </p>
         <p>As características fracas da DNN são explicadas principalmente pelo
         uso dos parâmetros ou parâmetros padrão próximos a eles. O conjunto woe
         não mostrou vantagens antes do conjunto bruto. Portanto, no próximo
         artigo, nós iremos:<br>
         </p>
         <ul>
         <li>otimizar uma parte dos hiperparâmetros na DNN.woe criado anteriormente;</li>
         <li>criaremos uma DNN, usando a biblioteca TensorFlow, testá-la e comparar os resultados com a DNN (darch);</li>
         <li>criaremos um conjunto de redes neurais de diferentes tipos
         (empacotamento, empilhamento) e veremos como isso melhora a qualidade
         das previsões.</li>
         </ul>
         <br>
         <h3 id="programs">Aplicação</h3>
         <div>
         <p><a href="https://www.mql5.com/go?link=https://github.com/VladPerervenko/darch12/tree/master/Part_IV" rel="nofollow" title="https://github.com/VladPerervenko/darch12/tree/master/Part_IV" target="_blank">GitHub/PartIV</a> contém:</p>
         <ol>
         <li>&nbsp;FunPrepareData.R — funções usadas para a preparação de dados</li>
         <li>&nbsp;RunPrepareData.R — scripts para a preparação de dados</li>
         <li>&nbsp;Experiment.R — scripts para executar experimentos</li>
         <li>&nbsp;Part_IV.RData — imagem da área de trabalho com todos os objetos obtidos após o estágio de preparação de dados</li>
         <li>&nbsp;SessionInfo.txt — informações sobre o software usado</li>
         <li>&nbsp;Darch_default.txt — lista de parâmetros da estrutura DArch com os valores padrão</li>
         </ol>
         </div>
         <p><br>
         </p>
         </div>
         </div>
         </div>
         </div>
         <script src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/all.js" type="text/javascript" defer="defer"></script>
         <script src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/articles.js" type="text/javascript" defer="defer"></script>
         </div>
         <div class="row post-footer">
         <div class="col-sm-6">
         <div class="tags">
         Tags:
         <a href="#">Artigos</a>
         </div>
         </div>
         <div class="col-sm-6">
         <div class="share-post-footer">
         Share
         <ul class="share">
         <li><a href="https://fb.me/datacryptopy" class="facebook" target="_blank" data-toggle="tooltip" data-placement="top" title="Facebook" data-original-title="crowdindicator/"><ion-icon name="logo-facebook"></ion-icon></a></li>
         <li><a href="https://twitter.com/DataCryptoML" class="twitter" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="@DataCryptoML"><ion-icon name="logo-twitter"></ion-icon></a></li>
         <li><a href="https://www.youtube.com/" class="youtube" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Youtube"><ion-icon name="logo-youtube"></ion-icon></a></li>
         <li><a href="https://www.instagram.com/" class="instagram" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Instagram"><ion-icon name="logo-instagram"></ion-icon></a></li>
         <li><a href="" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="GitHub"><ion-icon name="logo-github"></ion-icon></a></li>
         </ul>
         </div>
         </div>
         </div>
         </div>
         <div class="col-lg-4 col-md-12">
         <div class="sidebar">
         </div>
         </div>
         </div>
         </div>
         </div>
         </div>
      </div>
      </article>
      </main>
      <footer class="site-footer clearfix">
         <div class="container">
            <div class="row">
               <div class="col-sm-12">
               </div>
            </div>
         </div>
         <div class="container-fluid copyright">
            <div class="container ">
               <div class="row">
                  <section class="social-links-wrapper">
                     <div class="social-links-inner">
                        <a href="https://github.com/datacryptoanalytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-medium">
                           <ion-icon size="large"  name="logo-github"></ion-icon>
                           <span class="social-label">GitHub</span>
                        </a>
                        <a href="https://twitter.com/dc_analytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-twitter">
                           <ion-icon size="large" name="logo-twitter"></ion-icon>
                           <span class="social-label">Twitter</span>
                        </a>
                        <a href="https://www.instagram.com/dc_analytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-reddit">
                           <ion-icon size="large" name="logo-instagram"></ion-icon>
                           <span class="social-label">Instagram</span>
                        </a>
                        <a href="https://discord.com/invite/5ywpZMt6Kp" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-discord">
                           <ion-icon size="large" name="logo-discord"></ion-icon>
                           <span class="social-label">Discord</span>
                        </a>
                        <a href="https://cos.tv/channel/40545870367073280" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-youtube">
                           <ion-icon size="large" name="logo-youtube"></ion-icon>
                           <span class="social-label">Youtube</span>
                        </a>
                     </div>
                  </section>
                  <div class="col-md-12 text-center footer-copycat">
                     <p class="footer-rights">Copyright © 2019-2020, All Rights Reserved.</p>
                     <a href="mailto:datacryptoanalytics@protonmail.com" class="footer-support">
                        <span>
                           <ion-icon name="mail-outline"></ion-icon>
                           contato@datacryptoanalytics.com
                        </span>
                     </a>
                  </div>
               </div>
            </div>
         </div>
      </footer>
      </div>
      <script type="text/javascript" src="page_files/libraries.js"></script>
      <script type="module" src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"></script>
      <script nomodule src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"></script>
      <script type="text/javascript" src="page_files/main.js"></script>
      <div class="share-selected-text-main-container" style="height: 50px; top: 0px; left: 0px;">
         <div class="share-selected-text-inner"><a class="share-selected-text-btn share-selected-text-btn-twitter" href="https://twitter.com/intent/tweet?url=https://blog.cindicator.com/bitcoin-is-not-digital-gold/&amp;text=%22%22"><i class="icon-sst-twitter fab fa-twitter" style="pointer-events: none;"></i></a></div>
      </div>
   </body>
</html>