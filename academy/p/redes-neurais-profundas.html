<!DOCTYPE html>
<html lang="en"><head>
     <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-160618603-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-160618603-1');
</script>


<script data-ad-client="ca-pub-5399975515756281" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

        <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="shortcut icon" href="https://datacryptoanalytics.github.io/logo-web-datacrypto-analytics.png">

    <link rel="stylesheet" href="page_files/all.css">
    <link rel="stylesheet" type="text/css" href="page_files/libraries.css">
    <link rel="stylesheet" type="text/css" href="page_files/style.css">

    <meta name="description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
    <link rel="shortcut icon" href="https://datacryptoml.000webhostapp.com/img/icon_DataCrypto.png" type="https://datacryptoml.000webhostapp.com/img/datacrypto-analytics.png">
    <link rel="canonical" href="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
    <meta name="referrer" content="no-referrer-when-downgrade">

    <meta property="og:site_name" content="DataCrypto Analytics Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
    <meta property="og:description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
    <meta property="og:url" content="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
    <meta property="og:image" content="https://www.datacryptoanalytics.ml/img/fundo.jpg">
    <meta property="article:published_time" content="2020-03-20T16:51:04.000Z">
    <meta property="article:modified_time" content="2020-03-20T16:51:04.000Z">
    <meta property="article:tag" content="Articles">

    <meta property="article:publisher" content="https://fb.me/datacryptopy">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="DataCrypto Analytics - Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
    <meta name="twitter:description" content="Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural">
    <meta name="twitter:url" content="https://www.datacryptoanalytics.ml/blog/page/redes-neurais-profundas.html">
    <meta name="twitter:image" content="https://upload.wikimedia.org/wikipedia/commons/3/3c/Neuralnetwork.png">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="DataCrypto Analytics Dev Team">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Articles">
    <meta name="twitter:site" content="@DataCryptoML">
    <meta property="og:image:width" content="1800">
    <meta property="og:image:height" content="1200">

    <script type="text/javascript" async="" src="page_files/analytics.js"></script><script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "DataCrypto Analytics Blog",
        "logo": "https://upload.wikimedia.org/wikipedia/commons/3/3c/Neuralnetwork.png"
    },
    "author": {
        "@type": "Person",
        "name": "DataCrypto Analytics Dev Team",
        "url": "https://datacryptoanalytics.ml/about-us.html",
        "sameAs": []
    },
    "headline": "DataCrypto Analytics Dev",
    "url": "https://www.datacryptoanalytics.ml/",
    "datePublished": "2020-03-20T16:51:04.000Z",
    "dateModified": "2020-03-20T16:51:04.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://upload.wikimedia.org/wikipedia/commons/3/3c/Neuralnetwork.png",
        "width": 1800,
        "height": 1200
    },
    "keywords": "Articles",
    "description": "Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://www.datacryptoanalytics.ml/"
    }
}
    </script>

    <script defer="defer" src="page_files/a.js"></script>
    <meta name="generator" content="Ghost 3.9">
    <link rel="alternate" type="application/rss+xml" title="Cindicator Blog" href="https://www.datacryptoanalytics.ml/blog/">
    <script async="" src="page_files/js.js"></script>


</head>
<body class="post-template tag-articles nav-closed scroll">

    <div class="main-container">

        <header>
    <div class="container">
        <div class="row">
            <div class="col-3 ml-auto">
                <div class="blog-logo">
                    <a href="https://datacryptoanalytics.github.io/page/blog/" title="DataCrypto Analytics Blog">
                         <img src="https://datacryptoanalytics.github.io/logo-datacrypto-analytics.png" alt="DataCrypto Analytics Blog">
                    </a>
                </div>
            </div>
            <div class="col-9">
                <div class="inner">
                    <div class="navigation">
                        <a href="#" class="navigation-trigger" data-toggle="modal" data-target="#menu">
                            <svg width="19" height="16" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink"><use xlink:href="#path0_fill" transform="translate(.344 .195)"></use><defs><path id="path0_fill" d="M1.044 2.128h15.912c.577 0 1.044-.476 1.044-1.064C18 .477 17.533 0 16.956 0H1.044C.467 0 0 .477 0 1.064c0 .588.467 1.064 1.044 1.064zm15.912 4.387H1.044C.467 6.515 0 6.993 0 7.58c0 .589.467 1.064 1.044 1.064h15.912c.577 0 1.044-.475 1.044-1.064 0-.586-.467-1.064-1.044-1.064zm0 6.515H1.044C.467 13.03 0 13.508 0 14.094c0 .589.467 1.064 1.044 1.064h15.912c.577 0 1.044-.476 1.044-1.064 0-.586-.467-1.064-1.044-1.064z"></path></defs></svg>
                        </a>
                        <nav class="nav">
                          <ul>
                                  <li class="nav-home"><a href="https://datacryptoanalytics.github.io/" title="Inicio"target="_blank">Inicio</a></li>
                                  <li class="nav-news"><a href="https://datacryptoanalytics.github.io/downloads" title="Downloads" target="_blank">Downloads</a></li>
                                  <li class="nav-articles"><a href="https://datacryptoanalytics.github.io/#algorithm" title="Algoritmos" target="_blank">Algoritmos</a></li>
                                  <li class="nav-events"><a href="https://datacryptoanalytics.github.io/#invest" title="Fundo Forex & Bitcoin" target="_blank">Fundo Forex & Bitcoin</a></li>
                                  <li class="nav-about"><a href="https://datacryptoanalytics.github.io/#about" title="Sobre-nós" target="_blank">Sobre-nós</a></li>
                         </ul>
</nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="info-bar">
        <div class="progress" data-read="read" style="width: 0%;" data-original-title="1% read" title=""></div>
    </div>
</header>


<!-- Menu Modal -->
<div class="modal fade" id="menu" tabindex="-1" role="dialog" aria-hidden="true">
    <div class="modal-dialog" role="document">
        <div class="modal-content">
            <div class="modal-body">
                <a href="#" data-dismiss="modal" aria-label="Close" class="close btn">
                    <i class="close-button"></i>
                </a>
                <div class="container">
                    <div class="row">
                        <div class="col-md-12">
                            <h3 class="modal-title">Menu</h3>
                            <div class="modal-nav"></div>
                        </div>
                    </div>
                    <div class="row additional">
                        <div class="col-sm-12">
                            <h3 class="modal-title">Social</h3>
                            <ul class="share">
                                   <li><a href="https://fb.me/datacryptopy" class="facebook" target="_blank" data-toggle="tooltip" data-placement="top" title="Facebook" data-original-title="crowdindicator/"><ion-icon name="logo-facebook"></ion-icon></a></li>
                                    <li><a href="https://twitter.com/DataCryptoML" class="twitter" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="@DataCryptoML"><ion-icon name="logo-twitter"></ion-icon></a></li>
                                <li><a href="https://www.youtube.com/" class="youtube" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Youtube"><ion-icon name="logo-youtube"></ion-icon></a></li>
                                <li><a href="https://www.instagram.com/" class="instagram" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Instagram"><ion-icon name="logo-instagram"></ion-icon></a></li>
                                <li><a href="" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="GitHub"><ion-icon name="logo-github"></ion-icon></a></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>


    <main id="content" role="main" data-id="0">
    <article class="post tag-articles">
        <div class="content-inner">
            <div class="container">
                <div class="row">
                    <div class="col-xl-12 ml-auto mr-auto">
                        <div class="row">
                            <div class="col-lg-8 col-md-12">
                                <div class="post-meta">
                                    <div>    <!-- GTranslate: https://gtranslate.io/ -->
<a href="#" onclick="doGTranslate('pt|zh-CN');return false;" title="Chinese (Simplified)" class="gflag nturl" style="background-position:-300px -0px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Chinese (Simplified)" /></a><a href="#" onclick="doGTranslate('pt|en');return false;" title="English" class="gflag nturl" style="background-position:-0px -0px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="English" /></a><a href="#" onclick="doGTranslate('pt|fr');return false;" title="French" class="gflag nturl" style="background-position:-200px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="French" /></a><a href="#" onclick="doGTranslate('pt|de');return false;" title="German" class="gflag nturl" style="background-position:-300px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="German" /></a><a href="#" onclick="doGTranslate('pt|it');return false;" title="Italian" class="gflag nturl" style="background-position:-600px -100px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Italian" /></a><a href="#" onclick="doGTranslate('pt|pt');return false;" title="Portuguese" class="gflag nturl" style="background-position:-300px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Portuguese" /></a><a href="#" onclick="doGTranslate('pt|ru');return false;" title="Russian" class="gflag nturl" style="background-position:-500px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Russian" /></a><a href="#" onclick="doGTranslate('pt|es');return false;" title="Spanish" class="gflag nturl" style="background-position:-600px -200px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Spanish" /></a><a href="#" onclick="doGTranslate('pt|vi');return false;" title="Vietnamese" class="gflag nturl" style="background-position:-200px -400px;"><img src="//gtranslate.net/flags/blank.png" height="16" width="16" alt="Vietnamese" /></a>

<style type="text/css">
<!--
a.gflag {vertical-align:middle;font-size:16px;padding:1px 0;background-repeat:no-repeat;background-image:url(//gtranslate.net/flags/16.png);}
a.gflag img {border:0;}
a.gflag:hover {background-image:url(//gtranslate.net/flags/16a.png);}
#goog-gt-tt {display:none !important;}
.goog-te-banner-frame {display:none !important;}
.goog-te-menu-value:hover {text-decoration:none !important;}
body {top:0 !important;}
#google_translate_element2 {display:none!important;}
-->
</style>

<br /><select onchange="doGTranslate(this);"><option value="">Select Language</option><option value="pt|af">Afrikaans</option><option value="pt|sq">Albanian</option><option value="pt|ar">Arabic</option><option value="pt|hy">Armenian</option><option value="pt|az">Azerbaijani</option><option value="pt|eu">Basque</option><option value="pt|be">Belarusian</option><option value="pt|bg">Bulgarian</option><option value="pt|ca">Catalan</option><option value="pt|zh-CN">Chinese (Simplified)</option><option value="pt|zh-TW">Chinese (Traditional)</option><option value="pt|hr">Croatian</option><option value="pt|cs">Czech</option><option value="pt|da">Danish</option><option value="pt|nl">Dutch</option><option value="pt|en">English</option><option value="pt|et">Estonian</option><option value="pt|tl">Filipino</option><option value="pt|fi">Finnish</option><option value="pt|fr">French</option><option value="pt|gl">Galician</option><option value="pt|ka">Georgian</option><option value="pt|de">German</option><option value="pt|el">Greek</option><option value="pt|ht">Haitian Creole</option><option value="pt|iw">Hebrew</option><option value="pt|hi">Hindi</option><option value="pt|hu">Hungarian</option><option value="pt|is">Icelandic</option><option value="pt|id">Indonesian</option><option value="pt|ga">Irish</option><option value="pt|it">Italian</option><option value="pt|ja">Japanese</option><option value="pt|ko">Korean</option><option value="pt|lv">Latvian</option><option value="pt|lt">Lithuanian</option><option value="pt|mk">Macedonian</option><option value="pt|ms">Malay</option><option value="pt|mt">Maltese</option><option value="pt|no">Norwegian</option><option value="pt|fa">Persian</option><option value="pt|pl">Polish</option><option value="pt|pt">Portuguese</option><option value="pt|ro">Romanian</option><option value="pt|ru">Russian</option><option value="pt|sr">Serbian</option><option value="pt|sk">Slovak</option><option value="pt|sl">Slovenian</option><option value="pt|es">Spanish</option><option value="pt|sw">Swahili</option><option value="pt|sv">Swedish</option><option value="pt|th">Thai</option><option value="pt|tr">Turkish</option><option value="pt|uk">Ukrainian</option><option value="pt|ur">Urdu</option><option value="pt|vi">Vietnamese</option><option value="pt|cy">Welsh</option><option value="pt|yi">Yiddish</option></select><div id="google_translate_element2"></div>
<script type="text/javascript">
function googleTranslateElementInit2() {new google.translate.TranslateElement({pageLanguage: 'pt',autoDisplay: false}, 'google_translate_element2');}
</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit2"></script>


<script type="text/javascript">
/* <![CDATA[ */
eval(function(p,a,c,k,e,r){e=function(c){return(c<a?'':e(parseInt(c/a)))+((c=c%a)>35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)r[e(c)]=k[c]||e(c);k=[function(e){return r[e]}];e=function(){return'\\w+'};c=1};while(c--)if(k[c])p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c]);return p}('6 7(a,b){n{4(2.9){3 c=2.9("o");c.p(b,f,f);a.q(c)}g{3 c=2.r();a.s(\'t\'+b,c)}}u(e){}}6 h(a){4(a.8)a=a.8;4(a==\'\')v;3 b=a.w(\'|\')[1];3 c;3 d=2.x(\'y\');z(3 i=0;i<d.5;i++)4(d[i].A==\'B-C-D\')c=d[i];4(2.j(\'k\')==E||2.j(\'k\').l.5==0||c.5==0||c.l.5==0){F(6(){h(a)},G)}g{c.8=b;7(c,\'m\');7(c,\'m\')}}',43,43,'||document|var|if|length|function|GTranslateFireEvent|value|createEvent||||||true|else|doGTranslate||getElementById|google_translate_element2|innerHTML|change|try|HTMLEvents|initEvent|dispatchEvent|createEventObject|fireEvent|on|catch|return|split|getElementsByTagName|select|for|className|goog|te|combo|null|setTimeout|500'.split('|'),0,{}))
/* ]]> */
</script></div>
                                    <time datetime="2020-03-20">20 March 2020</time>
                                    <div class="tags"><a href="https://www.datacryptoanalytics.ml/">Articles</a></div>
                                </div>
                                <h1 class="post-title">Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</h1>
                            </div>
                        </div>
                        <div class="row">
                            <div class="col-lg-8 col-md-12">
                                <div class="editor-content">
                                    <h2>Redes Neurais Profundas (Parte IV). Criação, treinamento e teste de um modelo de rede neural</h2>
      <p class="scribe">
        <a href="https://www.mql5.com/pt/users/vlad1949" class="author">Vladimir Perervenko</a>  | 20 outubro, 2017
      </p>
    <br><div id="content"><div id="content">
  <h3>Conteúdo</h3>
<ul>
  <li><a href="https://www.mql5.com/pt/articles/3473#intro">Introdução</a></li>
  <li><a href="https://www.mql5.com/pt/articles/3473#darch">1. Uma breve descrição dos recursos do pacote </a></li>
  <ul>
    <li><a href="https://www.mql5.com/pt/articles/3473#initialization">1.1. Funções de inicialização dos neurônios</a></li>
    <li><a href="https://www.mql5.com/pt/articles/3473#activation">1.2. Funções de ativação dos neurônios</a></li>
    <li><a href="https://www.mql5.com/pt/articles/3473#training">1.3. Métodos de treinamento</a></li>
    <li><a href="https://www.mql5.com/pt/articles/3473#regularization">1.4. Métodos de regulação e estabilização</a></li>
    <li><a href="https://www.mql5.com/pt/articles/3473#rbm">1.5. Métodos e parâmetros de treinamento de um RBM</a></li>
    <li><a href="https://www.mql5.com/pt/articles/3473#dnn">1.6. Métodos e parâmetros de treinamento da DNN</a></li>
  </ul>
  <li><a href="https://www.mql5.com/pt/articles/3473#validation">2. Testando a qualidade do trabalho de uma DNN dependendo dos parâmetros usados</a></li>
  <ul>
    <li><a href="https://www.mql5.com/pt/articles/3473#experiments">2.1. Experimentos</a></li>
    <ul>
      <li><a href="https://www.mql5.com/pt/articles/3473#preparation">2.1.1. Dados de entrada (preparação)</a></li>
      <li><a href="https://www.mql5.com/pt/articles/3473#base">2.1.2. Modelo básico de comparação</a></li>
      <li><a href="https://www.mql5.com/pt/articles/3473#structure">2.1.3. Estrutura de uma DNN</a></li>
      <li><a href="https://www.mql5.com/pt/articles/3473#methods">2.1.4. Variantes de treinamento</a></li>
      <ul>
        <li><a href="https://www.mql5.com/pt/articles/3473#pretraining">Com pré-treinamento</a></li>
        <li><a href="https://www.mql5.com/pt/articles/3473#nopretraining">Sem pré-treinamento</a></li>
      </ul>
    </ul>
    <li><a href="https://www.mql5.com/pt/articles/3473#analythics">2.2. Análise de resultados</a></li>
  </ul>
  <li><a href="https://www.mql5.com/pt/articles/3473#final">Conclusão</a></li>
  <li><a href="https://www.mql5.com/pt/articles/3473#programs">Aplicação</a></li>
</ul>
<h3 id="intro">Introdução</h3>
<h4>Principais direções de estudo e aplicação</h4>
<p>Atualmente, existem dois fluxos principais no estudo e aplicação de
redes neurais profundas. Eles diferem na abordagem da inicialização dos
pesos dos neurônios em camadas ocultas.</p>
<p><b><i>Abordagem 1. </i></b>As redes neurais são muito sensíveis ao
método de inicialização de neurônios em camadas ocultas, especialmente
se o número de camadas ocultas aumentar (maior que 3). O professor
G.Hynton foi o primeiro a tentar e resolver esse problema. A ideia por
trás de sua abordagem foi iniciar os pesos dos neurônios em camadas
ocultas com os pesos obtidos durante o treinamento não supervisionado
das redes neurais auto-associativas constituídas por RBM (máquina de
Boltzmann restrita) ou AE (autoencoder). Esses RBMs empilhados (SRBM) e
AE empilhados (SAE) são treinados com uma grande variedade de dados
não-rotulados. O objetivo desse treinamento é destacar estruturas
ocultas (representações, imagens) e relacionamentos nos dados. A
inicialização de neurônios com pesos MLP, obtidos durante o
pré-treinamento, coloca o MLP ao espaço de soluções muito próximo ao
ótimo. Isso permite diminuir o número de dados rotulados e as épocas
durante o seguinte ajuste fino (treinamento) do MLP. Estas são vantagens
 extremamente importantes para muitas esferas de aplicação prática,
especialmente ao processar muitos dados.</p>
<p><i><b>Abordagem 2: </b></i>Outro grupo de cientistas liderados por
Yoshua Benjio criou métodos específicos de inicialização de neurônios
ocultos, funções específicas de ativação, métodos de estabilização e
treinamento. O sucesso desta direção está relacionada com um extenso
desenvolvimento de redes neurais convolutivas profundas e redes
neuronais recorrentes (DCNN, RNN). Tais redes neurais apresentaram alta
eficiência no reconhecimento de imagens, análise e classificação de
textos, juntamente com a tradução do discurso falado de um idioma para
outro. A ideias e métodos desenvolvidos para essas redes neurais também
começaram a ser usadas ​​para a MLP.</p>
<p>Atualmente, ambas abordagens são usadas ativamente. Deve-se notar que
 com quase os mesmos resultados, as redes neurais com pré-treinamento
exigem menos recursos computacionais e menos amostras para treinamento.
Esta é uma vantagem importante. Eu pessoalmente sou a favor das redes
neurais profundas com pré-treinamento. Eu acredito que o futuro pertence
 à aprendizagem não supervisionada.</p>
<h4>Pacotes em R que permitem desenvolver e usar DNN</h4>
<p>R possui uma série de pacotes para criar e usar uma DNN com um nível diferente de complexidade e conjunto de recursos.</p>
<p><i>Pacotes que permitem criar, treinar e testar um DNN com pré-treinamento:</i></p>
<ul>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/deepnet/" rel="nofollow" title="https://rdrr.io/cran/deepnet/" target="_blank">deepnet</a></b>
 é um pacote simples que não possui muitas configurações e parâmetros.
Permite criar ambas as redes neurais SAE com pré-treinamento e SRBM. No <a href="https://www.mql5.com/pt/articles/3526" target="_blank">artigo anterior</a>
 nós consideramos uma implementação prática de Experts usando este
pacote. O uso de RBM para pré-treinamento produz resultados menos
estáveis. Este pacote é adequado para o primeiro encontro com este tema e
 sobre as peculiaridades de tais redes. Com a abordagem certa, ela pode
ser usada em um Expert. <a href="https://www.mql5.com/go?link=https://rdrr.io/cran/RcppDL/" rel="nofollow" title="https://rdrr.io/cran/RcppDL/" target="_blank">RcppDL</a> é uma versão deste pacote ligeiramente menor em С++.</li>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/darch/" rel="nofollow" title="https://rdrr.io/cran/darch/" target="_blank">darch v.0.12</a></b>
 é um pacote complexo e flexível que tem muitos parâmetros e
configurações. As configurações recomendadas são definidas como padrão.
Este pacote permite criar e configurar uma rede neural de qualquer
complexidade e configuração. Ela utiliza a SRBM para pré-treinamento.
Este pacote é para usuários avançados. Nós vamos discutir os seus
recursos em detalhes mais tarde. <br>
  </li>
</ul>
<p><i>Abaixo estão os pacotes que permitem criar, treinar e testar uma DNN sem pré-treinamento:</i></p>
<ul>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/h2o/" rel="nofollow" title="https://rdrr.io/cran/h2o/" target="_blank">H2O</a> é</b>
 um pacote para processamento de dados grandes (&gt;1M de linhas e
&gt;1K de colunas). A rede neural profunda utilizada nela possui um
sistema de regularização desenvolvido. Suas capacidades são excessivas
para o nosso campo, mas isso não deve nos parar de usá-lo. </li>
  <li><a href="https://www.mql5.com/go?link=http://mxnet.incubator.apache.org/get_started/install.html" rel="nofollow" title="http://mxnet.io/get_started/install.html" target="_blank"><b>mxnet</b> </a>permite
 criar não só uma MLP, mas também redes recorrentes complexas, por
convolução e LSTM. Este pacote tem uma API para vários idiomas,
incluindo R e Python. A filosofia do pacote é diferente das listadas
acima. Isso ocorre porque os desenvolvedores escreveram pacotes
principalmente para Python. O pacote mxnet para R é mais leve e
possuindo menos recursos do que o pacote para Python. Isso não faz este
pacote pior. <br>
  </li>
</ul>
<p>O tema das redes profundas e recorrentes está bem desenvolvido no
ambiente Python. Existem muitos pacotes interessantes para a construção
de redes neurais desse tipo que a R não possui. Estes são pacotes R que
permitem executar programas/módulos escritos em Python:</p>
<ul>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/PythonInR/" rel="nofollow" title="https://rdrr.io/cran/PythonInR/" target="_blank">PythonInR</a> e<a href="https://www.mql5.com/go?link=https://cran.r-project.org/web/packages/reticulate/index.html" rel="nofollow" title="https://cran.r-project.org/web/packages/reticulate/index.html" target="_blank"> reticulate</a></b>
 são dois pacotes que permitem execução de qualquer código Python em R.
Para isso, você precisa ter o Python 2/3 instalado no seu computador.</li>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/kerasR/" rel="nofollow" title="https://rdrr.io/cran/kerasR/" target="_blank">kerasr</a></b> é uma interface R para uma biblioteca popular de aprendizagem profunda - <b>keras</b>.</li>
  <li><b><a href="https://www.mql5.com/go?link=https://rdrr.io/cran/tensorflow/" rel="nofollow" title="https://rdrr.io/cran/tensorflow/" target="_blank">tensorflow</a></b> é um pacote que fornece acesso à API TensorFow completa no ambiente R.</li>
</ul>
<p>Recentemente, a Microsoft publicou a biblioteca <a href="https://www.mql5.com/go?link=https://github.com/Microsoft/CNTK" rel="nofollow" title="https://github.com/Microsoft/CNTK" target="_blank">cntk v.2.1 (Computational Network Toolkit)</a> no GitHub. Ela pode ser usada como backend em comparação com Keras. Recomenda-se testá-lo em nossos problemas.</p>
<p>Yandex está se mantendo - a sua própria biblioteca O CatBoost está
disponível em código aberto. Esta biblioteca pode ser usada para
treinamento de modelos com dados de diferentes tipos. Isso inclui dados
difíceis de apresentar como números, por exemplo, tipos de nuvens e
tipos de mercadorias. <a href="https://www.mql5.com/go?link=https://github.com/catboost/catboost" rel="nofollow" title="https://github.com/catboost/catboost" target="_blank">O código-fonte</a>, <a href="https://www.mql5.com/go?link=https://tech.yandex.ru/CatBoost/" rel="nofollow" title="https://tech.yandex.ru/CatBoost/" target="_blank">documentação</a>, benchmarks e ferramentas necessárias já foram <a href="https://www.mql5.com/go?link=https://github.com/catboost/catboost" rel="nofollow" target="_blank" title="https://github.com/catboost/catboost">publicadas no GitHub</a><span style="color:rgb(34, 34, 34);">
 com a licença Apache 2.0. Apesar de não serem redes neurais, mas
árvores melhoradas, é aconselhável testar o algoritmo, especialmente
porque contém a API da R.</span></p>
<br>
<h3 id="darch">1. Uma breve descrição dos recursos do pacote</h3>
<p>O <b>pacote darch ver. 0.12.0</b> fornece uma ampla gama de
funcionalidades, permitindo que você não apenas crie e treine um modelo,
 mas faça o que quiser para suas necessidades e preferências. Mudanças
significativas foram introduzidas na versão do pacote (0.10.0),
considerado no <a href="https://www.mql5.com/pt/articles/1628" target="_blank">artigo anterior</a>.
 Foram adicionadas novas funções de ativação, inicialização e
estabilização. A novidade mais notável é que tudo foi trazido para uma
única função <i>darch()</i>, que é um construtor ao mesmo tempo. As
placas gráficas são suportadas. Após o treinamento, a função retorna um
objeto da classe <a href="https://www.mql5.com/go?link=https://cran.r-project.org/web/packages/darch/darch.pdf" rel="nofollow" title="https://cran.r-project.org/web/packages/darch/darch.pdf" target="_blank">DArch</a>. A estrutura do objeto é apresentada na Fig. 1. As funções <i>predict()</i> e <i>darchTest()</i> retornam uma previsão sobre os novos dados ou métricas de classificação.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/strDarch.png" title="StrDarch" alt="StrDarch" style="vertical-align:middle;"></p>
<p style="text-align:center;"><span class="small">Fig.1. Estrutura do objeto DArch&nbsp;</span></p>
<p>Todos os parâmetros têm valores padrão. Esses valores geralmente não
são ótimos. Todos esses parâmetros podem ser divididos em três grupos -
geral, para RBM e para NN. Nós vamos considerar alguns deles em detalhes
 mais tarde.</p>
<table class="standart" width="100%" cellspacing="0" cellpadding="2">
  <thead>
    <tr>
      <th>Funções</th>
      <th>Tipos</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Funções de inicialização</td>
      <td><ul>
          <li><b><i>generateWeightsFunction</i></b> = с(<a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v9/glorot10a.html" rel="nofollow" title="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank">generateWeightsGlorotUniform</a>, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v9/glorot10a.html" rel="nofollow" title="http://proceedings.mlr.press/v9/glorot10a.html" target="_blank">generateWeightsGlorotNormal</a>,&nbsp;</li>
        </ul>



        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generateWeightsUniform,
 generateWeightsNormal,&nbsp;<br>
        <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1502.01852" rel="nofollow" title="https://arxiv.org/abs/1502.01852" target="_blank">&nbsp;generateWeightsHeUniform</a>, <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1502.01852" rel="nofollow" title="https://arxiv.org/abs/1502.01852" target="_blank">generateWeightsHeNormal</a>)</p></td>
    </tr>
    <tr>
      <td>Funções de ativação</td>
      <td><ul>
          <li><b><i>darch.unitFunction</i></b> = c(sigmoidUnit, linearUnit, tanhUnit, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" rel="nofollow" title="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" target="_blank">rectifiedLinearUnit</a>, &nbsp; &nbsp;</li>
        </ul>
        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1511.07289" rel="nofollow" title="https://arxiv.org/abs/1511.07289" target="_blank">exponentialLinearUnit</a>,&nbsp;<a href="https://www.mql5.com/go?link=http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf" rel="nofollow" title="http://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf" target="_blank">softplusUnit</a>, <a href="https://www.mql5.com/go?link=http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html" rel="nofollow" title="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-12.html" target="_blank">softmaxUnit</a>, <a href="https://www.mql5.com/go?link=http://proceedings.mlr.press/v28/goodfellow13.pdf" rel="nofollow" title="http://proceedings.mlr.press/v28/goodfellow13.pdf" target="_blank">maxoutUnit</a>)</td>
    </tr>
    <tr>
      <td>Funções de treinamento</td>
      <td><ul>
          <li><b><i>darch.fineTuneFunction</i></b> = c(<a href="https://www.mql5.com/go?link=https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" rel="nofollow" title="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" target="_blank">backpropagation</a>, <a href="https://en.wikipedia.org/wiki/Rprop" rel="nofollow" title="http://sci2s.ugr.es/keel/pdf/algorithm/articulo/2003-Neuro-Igel-IRprop+.pdf" target="_blank">rpropagation</a>(<a href="https://en.wikipedia.org/wiki/Rprop" rel="nofollow" title="https://en.wikipedia.org/wiki/Rprop" target="_blank">Rprop+</a>, Rprop-, iRprop+, iRprop-),</li>
        </ul>
        &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
&nbsp;<a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" title="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" target="_blank"> minimizeAutoencoder</a>, <a href="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" rel="nofollow" title="http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html" target="_blank">minimizaClassifier</a>)</td>
    </tr>
    <tr>
      <td>Nível de treinamento</td>
      <td><ul>
          <li><b><i>bp.learnRate</i></b> = 1 é o nível de treinamento
para backpropagation. Isso pode ser um vetor se diferentes níveis de
treinamento forem usados ​​em cada camada de NN</li>
          <li><b><i>bp.learnRateScale</i></b> = 1. O nível de treinamento é multiplicado por esse valor após cada época</li>
        </ul></td>
    </tr>
    <tr>
      <td>Funções de estabilização</td>
      <td><ul>
          <li><b><i>darch.dropout</i></b> = 0 é um número (0,1) ou um vetor com o nível de eliminação para cada camada de NN <br>
          </li>
          <li><b><i>darch.dropout.dropConnect</i></b> = F indica se&nbsp;DropConnect deve ser usado em vez de ser abandonado</li>
          <li>
            <p><b><i>darch.dropout.momentMatching</i></b> = 0</p>
          </li>
          <li><b><i>darch.dropout.oneMaskPerEpoch</i></b> = F indica se deve ser gerado uma nova máscara para um novo lote (FALSE, padrão) ou para cada época (TRUE)</li>
          <li><b><i>darch.dither</i></b> = F mostra se <a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1508.04826" rel="nofollow" title="https://arxiv.org/abs/1508.04826" target="_blank">dither</a> deve ser aplicado a todos os dados de entrada do conjunto de treinamento</li>
          <li><b><i>darch.nesterovMomentum</i></b> = T</li>
          <li><i><b>darch.weightDecay</b></i> = 0</li>
          <li><b><i>normalizeWeights</i></b> = F</li>
          <li> <b><i>normalizeWeightsBound</i></b> é o limite superior para a norma L2 do vetor de entrada de pesos. Isso é usado apenas se <b><i>normalizeWeights</i></b> = TRUE</li>
        </ul></td>
    </tr>
    <tr>
      <td>Momentum</td>
      <td><ul>
          <li><b><i>darch.initialMomentum</i></b> = 0.5</li>
          <li><b><i>darch.finalMomentum</i></b> = 0.9</li>
          <li><b><i>darch.momentumRampLength</i></b> = 1</li>
        </ul></td>
    </tr>
    <tr>
      <td>&nbsp;Condições de parada</td>
      <td><ul>
          <li><b><i>darch.stopClassErr</i></b> = 100</li>
          <li><b><i>darch.stopErr </i></b>= -Inf<br>
          </li>
          <li><b><i>darch.stopValidClassErr</i></b> = 100</li>
          <li><b><i>darch.stopValidErr</i></b> = -Inf    </li>
        </ul></td>
    </tr>
  </tbody>
</table>
<p>Uma rede neural profunda é composta por n RBM (n = camadas -1)
conectadas em uma rede auto-associativa (SRBM) e as redes neurais reais
MLP com várias camadas. O treinamento em camada da RBM é um treinamento
não supervisionado em dados não-rotulados. O ajuste fino da rede neural
requer supervisão e é realizado em dados rotulados. </p>
<p>A divisão desses estágios de treinamento com parâmetros nos dá a
oportunidade de usar dados de diferentes volumes (não uma estrutura
diferente!!) e obter vários modelos ajustados com base em um
pré-treinamento. Se os dados para pré-treinamento e ajuste fino forem
iguais, o treinamento pode ser realizado de uma só vez sem dividi-lo em
duas etapas. O pré-treinamento pode ser ignorado (<i>rbm.numEpochs = 0;&nbsp;</i><i>darch.numEpochs = 10</i>)). Nesse caso, você pode usar apenas uma rede neural de várias camadas ou treinar apenas a RBM (<i>rbm.numEpochs = 10; darch.numEpochs = 0</i>). Você ainda terá acesso a todos os parâmetros internos. </p>
<p>A rede neural treinada pode ser treinada em novos dados quantas vezes
 for necessário. Isso só é possível com um número limitado de modelos. O
 diagrama estrutural de uma rede neural profunda inicializada por
máquinas de Boltzmann complexas e restritas (DNRBM) é exibido na Fig.2.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNSRBM_2.png" title="DNSRBM" alt="DNSRBM" style="vertical-align:middle;" width="690" height="434"></p>
<p style="text-align:center;"><span class="small">Fig.2. Diagrama estrutural de DNSRBM</span></p>
<br>
<p id="initialization"><b>1.1. Funções de inicialização dos neurônios</b></p>
<p>Existem duas funções principais de inicialização de neurônios no pacote.&nbsp; </p>
<ul>
  <li><i>generateWeightsUniform()</i> usa a função <i>runif(n, min, max)</i>, sendo implementada da seguinte forma:</li>
</ul>
<pre class="code">&gt; generateWeightsUniform
function (numUnits1, numUnits2, weights.min = getParameter(<span class="string">".weights.min"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;-<span class="number">0.1</span>, ...), weights.max = getParameter(<span class="string">".weights.max"</span>, <span class="number">0.1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;...), ...)
{
&nbsp;&nbsp;&nbsp;&nbsp;matrix(runif(numUnits1 * numUnits2, weights.min, weights.max),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nrow = numUnits1, ncol = numUnits2)
}
&lt;environment: namespace:darch&gt;

</pre>


<p><i>numUnits1</i> é o número de neurônios na camada anterior e <i>numUnits2</i> é o número de neurônios na camada atual.&nbsp;</p>
<ul>
  <li><i>generateWeightsNormal()</i>usa a função <i>rnorm(n, mean, sd)</i> sendo implementada no pacote da seguinte forma:</li>
</ul>
<pre class="code">&gt; generateWeightsNormal
function (numUnits1, numUnits2, weights.mean = getParameter(<span class="string">".weights.mean"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">0</span>, ...), weights.sd = getParameter(<span class="string">".weights.sd"</span>, <span class="number">0.01</span>, ...),
&nbsp;&nbsp;&nbsp;&nbsp;...)
{
&nbsp;&nbsp;&nbsp;&nbsp;matrix(rnorm(numUnits1 * numUnits2, weights.mean, weights.sd),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;nrow = numUnits1, ncol = numUnits2)
}
&lt;environment: namespace:darch&gt;

</pre>


<p>Outras quatro funções estão usando essas duas funções, mas definem <i>min, max, média </i>e<i> sd</i> com funções específicas. Você pode estudá-las se você inserir o nome da função sem colchetes no terminal.</p>
<br>
<p id="activation"><b>1.2. Funções de ativação dos neurônios</b></p>
<p>Além das funções de ativação padrão, o pacote sugere uma ampla gama de novas funções. Aqui estão algumas delas:<br>
</p>
<pre class="code">x &lt;- seq(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">0.1</span>)
par(mfrow = c(<span class="number">2</span>,<span class="number">3</span>))
plot(x, y = <span class="number">1</span>/(<span class="number">1</span> + <span class="functions">exp</span>(-x)), t = <span class="string">"l"</span>, main = <span class="string">"sigmoid"</span>)
abline(v = <span class="number">0</span>, col = <span class="number">2</span>)
plot(x, y = tanh(x), t = <span class="string">"l"</span>, main = <span class="string">"tanh"</span>)
abline(v = <span class="number">0</span>, h = <span class="number">0</span>, col = <span class="number">2</span>)
plot(x, y = <span class="functions">log</span>(<span class="number">1</span> + <span class="functions">exp</span>(x)), t = <span class="string">"l"</span>, main = <span class="string">"softplus"</span>);
abline(v = <span class="number">0</span>, col = <span class="number">2</span>)
plot(x, y = ifelse(x &gt; <span class="number">0</span>, x ,<span class="functions">exp</span>(x) - <span class="number">1</span>), t = <span class="string">"l"</span>,
&nbsp;&nbsp;&nbsp;&nbsp; main = <span class="string">"ELU"</span>)
abline(h = <span class="number">0</span>, v = <span class="number">0</span>, col = <span class="number">2</span>)
plot(x, y = ifelse(x &gt; <span class="number">0</span>, x , <span class="number">0</span>), t = <span class="string">"l"</span>, main = <span class="string">"ReLU"</span>)
abline(h = <span class="number">0</span>, v = <span class="number">0</span>, col = <span class="number">2</span>)
par(mfrow = c(<span class="number">1</span>,<span class="number">1</span>))

</pre>


<p>&nbsp;</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/actFun_3.png" title="activFun" alt="activFun" style="vertical-align:middle;" width="750" height="442"></p>
<p style="text-align:center;"><span class="small">Fig.3. Funções de ativação dos neurônios</span></p>
<p>Vamos considerar a <i>função de ativação maxout separadamente. </i>Esta
 função vem de redes por convolução. A camada oculta da rede neural é
dividida por módulos do tamanho da agregação (poolSize). O número de
neurônios na camada oculta deve ser divisível pelo tamanho da agregação
(pool). Para o treinamento, um neurônio com uma ativação máxima é
selecionado da pool e enviado para a entrada. A função de ativação dos
neurônios na pool é definida separadamente. Em palavras simples, esta é
uma camada dupla (convolução + maxpooling) com capacidades limitadas no
passo de filtração. De acordo com várias publicações, ela produz bons
resultados em conjunto com <i>dropout</i>. Fig. 4. É exibido esquematicamente uma camada oculta com 8 neurônios e dois tamanhos da pool</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/maxout_4__1.png" title="Maxout" alt="Maxout" style="vertical-align:middle;" width="610" height="305"></p>
<p style="text-align:center;"><span class="small">Fig.4. A função de ativação <i>maxout</i></span></p>
<p id="training"><b>1.3. Métodos de treinamento</b></p>
<br>
<p>Infelizmente, existem apenas dois métodos de treinamento no pacote - <b><i>backpropagation</i> </b>e <b><i>rprop </i></b>da
 versão básica e melhorada com atualização do peso durante a
backpropagation e sem ela. Existe também a possibilidade de alterar o
nível de treinamento com a ajuda do multiplicador <b><i>bp.learnRateScale.</i></b>.</p>
<br>
<p id="regularization"><b>1.4. Métodos de regulação e estabilização</b></p>
<ul>
  <li><b>dropout </b> é uma técnica de eliminação (método de
regularização) de uma parte dos neurônios da camada oculta durante o
treinamento. Os neurônios são zerados em uma ordem aleatória. O número
relativo de neurônios a serem descartados é definido pelo parâmetro <i>darch.dropout</i><i>.</i>
 O nível de regularização em cada camada oculta pode ser diferente. A
máscara de regularização pode ser gerada para cada lote ou para cada
época.</li>
  <li><b>dropconnect</b> desliga as conexões entre uma parte dos
neurônios da camada atual e os neurônios da camada anterior. As conexões
 são cortadas em uma ordem aleatória. O número relativo de conexões a
serem cortadas é definido pelo mesmo parâmetro <i>darch.dropout (</i>geralmente
 não superior a 0.5). De acordo com algumas publicações, o dropconnect
exibe melhores resultados do que o método de regularização (dropout).</li>
  <li><a href="https://www.mql5.com/go?link=https://arxiv.org/abs/1508.04826" rel="nofollow" title="/go?link=https://arxiv.org/abs/1508.04826" target="_blank"><b>dither</b></a> é uma maneira de evitar um retreinamento através da redução (dithering) dos dados de entrada. <br>
  </li>
  <li><i><b>weightDecay</b></i> o peso de cada neurônio será multiplicado por (1 — <i><b>weightDecay)</b></i>antes da atualização.</li>
  <li><b><i>normalizeWeights</i></b> é uma maneira de normalizar um vetor de entrada de pesos de neurônios com uma possível limitação acima (norma L2)</li>
</ul>
<p>Os três primeiros métodos são usados ​​apenas de maneira separada.&nbsp;</p>
<br>
<p id="rbm"><b>1.5. Métodos e parâmetros de treinamento de um RBM</b></p>
<p>Há duas maneiras de treinar uma SRBM. Qualquer RBM é treinada uma a
uma durante a rbm.numEpochs ou cada RBM é treinada em uma única época ao
 mesmo tempo. A escolha de um desses métodos é feita pelo parâmetro <i>rbm.consecutive:</i> TRUE ou padrão é o primeiro método e FALSE é o segundo método. Fig.5 apresenta um esquema de treinamento em duas variantes. O <i>rbm.lastLayer</i>
 pode ser usado para especificar a camada de SRBM em que o
pré-treinamento deve ser interrompido. Se 0, todas as camadas devem ser
treinadas e se (-1) a camada superior é para deixar sem treino. Isso faz
 sentido, já que a camada superior precisa ser treinada separadamente e
leva muito mais tempo. Outros parâmetros não precisam de explicações
adicionais.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/SRBMtarin_5.png" title="SRBMtrain" alt="SRBMtrain" style="vertical-align:middle;" width="690" height="434"></p>
<p style="text-align:center;"><span class="small">Fig.5. Dois métodos de treinamento de uma SRBM</span></p>
<br>
<p id="dnn"><b>1.6. Métodos e parâmetros de treinamento da DNN</b></p>
<p>Uma DNN pode ser treinada de duas maneiras - com pré-treinamento e
sem ele. Os parâmetros utilizados nestes métodos são totalmente
diferentes. Por exemplo, não é necessário usar métodos específicos de
inicialização e regularização no treinamento <b>com pré-treinamento</b>.
 De fato, usar esses métodos pode piorar o resultado. A razão por trás
disso é que, após um pré-treinamento, os pesos dos neurônios nas camadas
 ocultas serão colocados na área próxima aos valores ótimos e eles
precisarão apenas de uma pequena afinação fina. Para obter o mesmo
resultado no treinamento <b>sem pré-treinamento</b>, todos os métodos
disponíveis de inicialização e regularização terão de ser utilizados.
Geralmente, o treinamento de uma rede neural dessa maneira demora mais.</p>
<p>Então, vamos nos concentrar no <b>treinamento com pré-treinamento</b>. Normalmente, ele acontece em duas etapas.</p>
<ol>
  <li>Treinando a SRBM em um grande conjunto de dados não gravados. Os
parâmetros de pré-treinamento são definidos separadamente. Como
resultado, nós temos uma rede neural iniciada por pesos da SRBM. Em
seguida, a camada superior da rede neural é treinada com dados rotulados
 com seus próprios parâmetros de treinamento. Desta forma, nós temos uma
 rede neural com uma camada superior treinada e pesos iniciados nas
camadas inferiores. Salve-o como um objeto independente para uso
posterior.&nbsp; <br>
  </li>
  <li>Na segunda etapa, nós usaremos algumas amostras rotuladas, baixo
nível de treinamento e um pequeno número de épocas de treinamento para
todas as camadas da rede neural. Este é um ajuste fino da rede. A rede
neural é treinada.</li>
</ol>
<p>A possibilidade de divisão de estágios de pré-treinamento, ajuste
fino e treinamentos adicionais oferece uma flexibilidade incrível na
criação de algoritmos de treinamento não só para uma DNN, mas para
treinamento de comitês de DNN. Fig.6. representa várias variantes de
treinamento de DNN e comitês de DNN.&nbsp;</p>
<ul>
  <li>Variante <i>а</i>. Salve a DNN em todas as épocas durante o ajuste
 fino. Desta forma, nós teremos um número de DNN com um grau de
treinamento diferente. Mais tarde, cada uma dessas redes neurais pode
ser usada separadamente ou como parte de um comitê. A desvantagem deste
cenário é que todos as DNN são treinadas nos mesmos dados, pois todos
elas tinham os mesmos parâmetros de treinamento. <br>
  </li>
  <li>Variante <i>b</i>. Faça o ajuste fino da DNN iniciada <b>em paralelo</b>
 com diferentes conjuntos de dados (janela deslizante, janela em
crescimento, etc.) e diferentes parâmetros. Como resultado, nós teremos
uma DNN que produzirá previsões menos correlacionadas do que as
variantes <i>a.</i></li>
  <li>Faça o ajuste fino da variante<i> c</i> na DNN iniciada <b>sequencialmente</b>
 com diferentes conjuntos de dados e diferentes parâmetros. Salve os
modelos intermediários. Isto é o que anteriormente nós chamamos de
treinamento adicional. Isso pode ser executado sempre que há dados novos
 suficientes.</li>
</ul>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNNtrain_6.png" title="DNNtrain" alt="DNNtrain" style="vertical-align:middle;" width="749" height="296"></p>
<p style="text-align:center;"><span class="small">Fig.6. Variantes de treinamento da DNN</span></p>
<br>
<h3 id="validation">2. Testando a qualidade do trabalho de uma DNN, dependendo dos parâmetros utilizados.</h3>
<h4 id="experiments">2.1. Experimentos</h4>
<br>
<p id="preparation"><b>2.1.1. Dados de entrada (preparação)</b></p>
<p>Nós usaremos os dados e funções da parte anterior do artigo (<a href="https://www.mql5.com/pt/articles/3486" target="_blank">1</a>, <a href="https://www.mql5.com/pt/articles/3507" target="_blank">2</a>, <a href="https://www.mql5.com/pt/articles/3526" target="_blank">3</a>).
 Lá foram discutidos em detalhe várias variantes da preparação
preliminar de dados. Vou mencionar brevemente as etapas de preparação
preliminar que nós vamos realizar. OHLCV é o dado inicial, o mesmo que
antes. Os dados de entrada são filtros digitais e os dados de saída são o
 ZigZag. As funções e a imagem do espaço de trabalho Cotir.RData podem
ser usadas.</p>
<p>Os estágios de preparação de dados para realizar serão reunidos em funções separadas:</p>
<ul>
  <li>PrepareData() — cria o DataSet inicial e remove os NA;</li>
  <li>SplitData() — divide o conjunto de dados iniciais nos subconjuntos de pré-treinamento, train, val, test;</li>
  <li>CappingData() — identifica e imputa os outliers em todos os subconjuntos.</li>
</ul>
<p>Para salvar o espaço no artigo, eu não vou trazer a lista dessas
funções aqui. Elas podem ser baixadas do GitHub já que elas foram
consideradas em detalhes nos artigos anteriores. Nós vamos analisar os
resultados mais tarde. Nós não vamos discutir todos os métodos de
transformação de dados durante o processamento preliminar. Muitos deles
são bem conhecidos e amplamente utilizados. Nós usaremos um método menos
 conhecido de <i><b>discretização </b></i>(supervisionado e não supervisionado). No segundo artigo <a href="https://www.mql5.com/pt/articles/3507#discret" target="_blank">nós consideramos</a> dois pacotes de discretização supervisionada (<b>discretization </b>e <b>smbinning</b>). Eles contêm diferentes algoritmos de discretização.</p>
<p>Nós examinaremos os diferentes métodos de dividir variáveis
​​contínuas em bins e as formas de usar essas variáveis ​​discretizadas
em modelos.</p>
<p><i>O que é binning?</i></p>
<p>Binning é um termo usado na modelagem de pontuação. Ela é conhecida
como discretização na aprendizagem por máquinas. Este é um processo de
transformar uma variável contínua em um número finito de intervalos
(caixas). Isso ajuda a entender sua distribuição e relacionamento com a
variável objetivo binária. Os bins criados neste processo podem se
tornar características da característica preditiva para uso em modelos.</p>
<p><i>Por que o binning?</i></p>
<p>Apesar de algumas resalvas sobre o binning, ele possui vantagens significativas.&nbsp; </p>
<ul>
  <li>Ele permite incluir os dados ausentes (NA) e outros cálculos específicos (divisão por zero, por exemplo) no modelo.</li>
  <li>Ele controla ou mitiga o impacto dos outliers no modelo.</li>
  <li>Ele resolve o problema de diferentes escalas em preditores,
tornando os coeficientes ponderados comparáveis ​​no modelo final.&nbsp;</li>
</ul>
<p><i>Discretização não supervisionada</i></p>
<p>A discretização não supervisionada divide uma função contínua em bins
 sem levar em conta outras informações. Esta divisão tem duas opções.
Elas são bins de mesmo comprimento e bins de mesma frequência.</p>
<table class="standart" width="100%" cellspacing="0" cellpadding="3">
  <thead>
    <tr>
      <th>Opção <br></th>
      <th>Alvo <br></th>
      <th>Exemplo <br></th>
      <th>Desvantagem <br></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bins de mesmo comprimento<br></td>
      <td>Comprensão da distribuição da variável<br></td>
      <td>Histograma clássico com bunkers de mesmo comprimento, que pode ser calculado usando regras diferentes (sturges, rice ets)<br></td>
      <td>O número de registros no bunker pode ser muito pequeno para um cálculo correto<br></td>
    </tr>
    <tr>
      <td>Bins de frequência qual<br></td>
      <td>Analisa o relacionamento com a variável objetivo binária usando índices como a taxa incorreta<br></td>
      <td>Quartilies ou Percentis</td>
      <td>Os pontos de corte selecionados não podem maximizar a diferença entre as caixas na verificação contra a variável objetivo<br></td>
    </tr>
  </tbody>
</table>
<p><i>Discretização supervisionada</i></p>
<p>A discretização supervisionada divide a variável contínua em caixas
projetadas na variável objetivo. A ideia-chave aqui é encontrar esses
pontos de corte que maximizarão a diferença entre os grupos.</p>
<p>Usando algoritmos como ChiMerge ou Particionamento Recursivo, os
analistas podem rapidamente encontrar pontos ótimos em segundos e
avaliar sua relação com a variável objetivo, usando índices como peso de
 evidência (WoE) e valor de informação (IV).</p>
<p>WoE pode ser usado como um instrumento para transformar preditores no
 estágio de pré-processamento para algoritmos de aprendizagem
supervisionada. Durante a discretização dos preditores, nós podemos
substituí-los por suas novas variáveis ​​nominais ou pelos valores de
seu WoE. A segunda variante é interessante porque permite afastar-se da
transformação das variáveis ​​nominais (fatores) para as artificiais.
Isso dá uma melhoria significativa na qualidade da classificação.&nbsp;</p>
<p>WOE e IV desempenham duas funções diferentes na análise de dados:</p>
<ul>
  <li><b>WOE descreve a relação da variável preditiva e a variável objetivo binária.</b></li>
  <li><b>IV mede a força dessas relações.</b></li>
</ul>
<p>Vamos descobrir o qual WOE e IV estão usando diagramas e fórmulas.
Lembre-se do gráfico da variável v.fatl dividido em 10 áreas equitativas
 na <a href="https://www.mql5.com/pt/articles/3507" target="_blank">segunda parte do artigo</a>.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Discret6.png" title="vfatl_discr" alt="vfatl_discr" style="vertical-align:middle;" width="750" height="484"></p>
<p style="text-align:center;"><span class="small">Fig.7. A variável v.fatl é dividida em 10 áreas equitativas</span></p>
<p><b><i>Capacidade preditiva de dados (WOE)</i></b></p>
<p>Como você pode ver, cada bin possui amostras que entram na classe "1"
 e na classe "-1". A capacidade preditiva dos bins WoEi são calculados
com a fórmula&nbsp;</p>
<p style="text-align:center;">WoEi = ln(Gi/Bi)*100</p>
<p>где:</p>
<p> Gi — frequência relativa das amostras "boas" (no nosso caso "bom" = "1") em cada bin da variável;</p>
<p>Bi — frequência relativa das amostras "ruins" (no nosso caso "ruim" = "-1") em cada bin da variável.</p>
<p>Se WoEi = 1, o que significa que o número de amostras "boas" e
"ruins" neste compartimento é aproximadamente o mesmo, então a
habilidade preditiva desse bin é 0. Se as amostras "boas" superarem em
número as "ruins", WOE &gt;0 e vice-versa.</p>
<p><b><i>Valor de informação (IV)&nbsp;</i></b></p>
<p> Esta é a medida mais comum de identificar a significância de
variáveis ​​e medir a diferença na distribuição de amostras "boas" e
"ruins". O valor da informação pode ser calculado com a fórmula:</p>
<p style="text-align:center;">IV = ∑ (Gi – Bi) ln (Gi/Bi)</p>
<p>O valor da informação de uma variável é igual à soma de todas bins da
 variável. Os valores desse coeficiente podem ser interpretados da
seguinte forma:</p>
<ul>
  <li>abaixo de 0,02 — variável estatisticamente insignificante;</li>
  <li>0,02 - 0,1 — variável estatisticamente fraca;</li>
  <li>0,1 - 0,3 — variável estatisticamente significativa;</li>
  <li>0,3 e acima — variável estatisticamente forte.</li>
</ul>
<p>Em seguida, os bins são unidos/divididos usando vários algoritmos e
critérios de otimização para tornar a diferença entre esses bins tão
grande quanto possível. Por exemplo, o pacote <b>smbinning </b> usa o
Particionamento Recursivo para categorizar valores numéricos e o valor
de informação para resolver os pontos de corte ótimos. O pacote <b>discretization </b> resolve esse problema com o ChiMerge e MDL. Deve-se ter em mente que os<b> pontos de corte são obtidos no conjunto de treinamento</b> e costuma-se dividir os conjuntos de validação e teste.&nbsp;</p>
<p>Existem vários pacotes que permitem fazer variáveis ​​numéricas discretas de uma maneira ou de outra. Sendo elas: <b>discretization</b><i>, </i><b>smbinning</b><i>, </i><b>Information</b><i>, </i><b>InformationValue </b><i>e </i><b>woebinning</b>.
 Nós precisamos tornar o conjunto de dados de teste discreto, dividir os
 conjuntos de validação e teste usando essas informações. Nós também
queremos ter controle visual dos resultados. Por causa desses
requisitos, eu escolhi o pacote <b>woebinning</b>.&nbsp;</p>
<p>O pacote se divide automaticamente em <b><i>valores numéricos e fatores</i></b> e os liga à variável objetivo binária. Aqui são contempladas duas abordagens: </p>
<ul>
  <li>a implementação de classificação fina e bruta uni sequencialmente classes e níveis granulados;</li>
  <li>um segmento de abordagem semelhante a uma árvore através de bins iniciais de iteração através da divisão binária. <br>
  </li>
</ul>
<p>Ambos os procedimentos combinam bins divididos com base nos valores
semelhantes de WOE e a parada com base nos critérios IV. O pacote pode
ser usado tanto com variáveis ​​autônomas quanto com todo o quadro de
dados. Isso fornece ferramentas flexíveis para estudar várias soluções
para binning e para expandir novos dados.</p>
<p>Vamos fazer o cálculo. Nós já temos as cotações do terminal carregado
 em nosso ambiente de trabalho (ou imagem do ambiente de trabalho
Cotir.RData do GitHub). Sequência de cálculos e resultados:</p>
<div style="margin:0px; padding:0px;">
  <ol>
    <li>&nbsp;PrepareData() — cria o conjunto de dados inicial dt[7906,
14], limpo de NA. O conjunto inclui o rótulo temporário Data, variáveis
​​de entrada (12) e a variável objetivo Class (fator com dois níveis
"-1" e "+1").</li>
    <li>&nbsp;SplitData() — divide o conjunto de dados inicial dt[] em
subconjunto de pré-treinamento, train, val, test na proporção de
2000/1000/500/500, unindo eles em um dataframe DT[4, 4000, 14].</li>
    <li>&nbsp;CappingData() — identifica e imputa os outliers em todos
os subconjuntos, obtém o conjunto DTcap[4, 4000, 14]. Apesar do fato de
que a discretização é toleranta aos outliers, nós os imputaremos. Você
pode experimentar sem esta etapa. Como você pode lembrar, os parâmetros
de outliers (pre.outl) são definidos no subconjunto de pré-treinamento.
Processe os conjuntos de train/val/test usando esses parâmetros.</li>
    <li>&nbsp;NormData() — normaliza o conjunto, usando o método <i>spatialSing</i> do pacote <b>caret</b>.
 Semelhante ao imputamento dos outliers, os parâmetros de normalização
(preproc) são definidos no subconjunto pretrain. As amostras
train/val/test são processadas usando esses parâmetros. Nós temos
DTcap.n[4, 4000, 14] como resultado.</li>
    <li>DiscretizeData() — define os parâmetros de discretização
(preCut), a qualidade das variáveis ​​e suas bins à luz de WOE e
IV.&nbsp;</li>
  </ol>
</div>
<div style="margin:0px; padding:0px;">
  <pre class="code">evalq({
&nbsp;&nbsp;dt &lt;- PrepareData(Data, <span class="predefines">Open</span>, <span class="predefines">High</span>, <span class="predefines">Low</span>, <span class="predefines">Close</span>, <span class="predefines">Volume</span>)
&nbsp;&nbsp;DT &lt;- SplitData(dt, <span class="number">2000</span>, <span class="number">1000</span>, <span class="number">500</span>,<span class="number">500</span>)
&nbsp;&nbsp;pre.outl &lt;- PreOutlier(DT$pretrain)
&nbsp;&nbsp;DTcap &lt;- CappingData(DT, impute = T, fill = T, dither = F,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pre.outl = pre.outl)
&nbsp;&nbsp;preproc &lt;- PreNorm(DTcap, meth = meth)
&nbsp;&nbsp;DTcap.n &lt;- NormData(DTcap, preproc = preproc)
&nbsp;&nbsp;preCut &lt;- PreDiscret(DTcap.n)
}, env)

</pre>


  <p>Vamos colocar os dados de discretização em todas as variáveis ​​em uma tabela e olhar para elas:</p>
  <pre class="code">evalq(tabulate.binning &lt;- woe.binning.table(preCut), env)
&gt; env$tabulate.binning
$`WOE Table <span class="keyword">for</span> v.fatl`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.3904381926</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">130</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">2.4</span>%&nbsp;&nbsp;<span class="number">15.6</span>% <span class="number">171.3</span> <span class="number">0.185</span>
<span class="number">2</span> &lt;= -<span class="number">0.03713814085</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">498</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">271</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">50.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.8</span>%&nbsp;&nbsp;<span class="number">35.2</span>%&nbsp;&nbsp;<span class="number">63.2</span> <span class="number">0.149</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1130198981</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">141</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">167</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.5</span>%&nbsp;&nbsp;<span class="number">54.2</span>% -<span class="number">14.5</span> <span class="number">0.003</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">219</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">550</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">22.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">54.3</span>%&nbsp;&nbsp;<span class="number">71.5</span>% -<span class="number">89.7</span> <span class="number">0.289</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.626</span>

$`WOE Table <span class="keyword">for</span> ftlm`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.2344708291</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">333</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">129</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">12.7</span>%&nbsp;&nbsp;<span class="number">27.9</span>%&nbsp;&nbsp;<span class="number">97.2</span> <span class="number">0.204</span>
<span class="number">2</span> &lt;= -<span class="number">0.01368798447</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">268</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">193</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">27.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.1</span>%&nbsp;&nbsp;<span class="number">41.9</span>%&nbsp;&nbsp;<span class="number">35.2</span> <span class="number">0.028</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1789073635</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">210</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">251</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24.8</span>%&nbsp;&nbsp;<span class="number">54.4</span>% -<span class="number">15.4</span> <span class="number">0.005</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">177</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">439</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">43.4</span>%&nbsp;&nbsp;<span class="number">71.3</span>% -<span class="number">88.4</span> <span class="number">0.225</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.463</span>

$`WOE Table <span class="keyword">for</span> rbci`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.1718377948</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">421</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">195</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">42.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.3</span>%&nbsp;&nbsp;<span class="number">31.7</span>%&nbsp;&nbsp;<span class="number">79.4</span> <span class="number">0.185</span>
<span class="number">2</span> &lt;= -<span class="number">0.09060410462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">153</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">86</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">67</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.6</span>%&nbsp;&nbsp;<span class="number">43.8</span>%&nbsp;&nbsp;<span class="number">27.4</span> <span class="number">0.006</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.3208178176</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">391</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">532</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">39.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">52.6</span>%&nbsp;&nbsp;<span class="number">57.6</span>% -<span class="number">28.4</span> <span class="number">0.037</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">90</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">218</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.5</span>%&nbsp;&nbsp;<span class="number">70.8</span>% -<span class="number">86.1</span> <span class="number">0.107</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.335</span>

$`WOE Table <span class="keyword">for</span> v.rbci`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.1837437563</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">406</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">210</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">41.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.8</span>%&nbsp;&nbsp;<span class="number">34.1</span>%&nbsp;&nbsp;<span class="number">68.3</span> <span class="number">0.139</span>
<span class="number">2</span> &lt;= <span class="number">0.03581374495</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">253</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">208</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">25.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.6</span>%&nbsp;&nbsp;<span class="number">45.1</span>%&nbsp;&nbsp;<span class="number">22.0</span> <span class="number">0.011</span>
<span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.2503922644</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">194</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">267</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.4</span>%&nbsp;&nbsp;<span class="number">57.9</span>% -<span class="number">29.5</span> <span class="number">0.020</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">135</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">327</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32.3</span>%&nbsp;&nbsp;<span class="number">70.8</span>% -<span class="number">86.1</span> <span class="number">0.161</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.331</span>

$`WOE Table <span class="keyword">for</span> v.satl`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp;&nbsp;&nbsp;WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.01840058612</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">585</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">338</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">59.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.4</span>%&nbsp;&nbsp;<span class="number">36.6</span>%&nbsp;&nbsp; <span class="number">57.3</span> <span class="number">0.148</span>
<span class="number">2</span>&nbsp;&nbsp; &lt;= <span class="number">0.3247097195</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">316</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">453</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32.0</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44.8</span>%&nbsp;&nbsp;<span class="number">58.9</span>%&nbsp;&nbsp;-<span class="number">33.6</span> <span class="number">0.043</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.4003869443</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">32</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">122</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">3.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">12.1</span>%&nbsp;&nbsp;<span class="number">79.2</span>% -<span class="number">131.4</span> <span class="number">0.116</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">55</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">99</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.6</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.8</span>%&nbsp;&nbsp;<span class="number">64.3</span>%&nbsp;&nbsp;-<span class="number">56.4</span> <span class="number">0.024</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp; NA <span class="number">0.330</span>

$`WOE Table <span class="keyword">for</span> v.stlm`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.4030051922</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">118</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">11.9</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">3.6</span>%&nbsp;&nbsp;<span class="number">23.4</span>% <span class="number">121.1</span> <span class="number">0.102</span>
<span class="number">2</span> &lt;= -<span class="number">0.1867821117</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">282</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">180</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">28.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.8</span>%&nbsp;&nbsp;<span class="number">39.0</span>%&nbsp;&nbsp;<span class="number">47.3</span> <span class="number">0.051</span>
<span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.1141896118</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">301</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">314</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">31.0</span>%&nbsp;&nbsp;<span class="number">51.1</span>%&nbsp;&nbsp;-<span class="number">1.8</span> <span class="number">0.000</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">287</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">482</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">29.0</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">47.6</span>%&nbsp;&nbsp;<span class="number">62.7</span>% -<span class="number">49.4</span> <span class="number">0.092</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.244</span>

$`WOE Table <span class="keyword">for</span> pcci`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.1738420887</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">397</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">219</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">40.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">21.6</span>%&nbsp;&nbsp;<span class="number">35.6</span>%&nbsp;&nbsp;<span class="number">61.9</span> <span class="number">0.115</span>
<span class="number">2</span> &lt;= -<span class="number">0.03163945242</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">307</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">165</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">142</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.0</span>%&nbsp;&nbsp;<span class="number">46.3</span>%&nbsp;&nbsp;<span class="number">17.4</span> <span class="number">0.005</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.2553612644</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">270</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">345</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">27.3</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">34.1</span>%&nbsp;&nbsp;<span class="number">56.1</span>% -<span class="number">22.1</span> <span class="number">0.015</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">156</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">306</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.8</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.2</span>%&nbsp;&nbsp;<span class="number">66.2</span>% -<span class="number">65.0</span> <span class="number">0.094</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.228</span>

$`WOE Table <span class="keyword">for</span> v.ftlm`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.03697698898</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">555</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">368</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">56.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36.4</span>%&nbsp;&nbsp;<span class="number">39.9</span>%&nbsp;&nbsp;<span class="number">43.5</span> <span class="number">0.086</span>
<span class="number">2</span>&nbsp;&nbsp; &lt;= <span class="number">0.2437475615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">279</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">336</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">28.2</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">33.2</span>%&nbsp;&nbsp;<span class="number">54.6</span>% -<span class="number">16.2</span> <span class="number">0.008</span>
<span class="number">3</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.4</span>%&nbsp;&nbsp;<span class="number">66.7</span>% -<span class="number">66.9</span> <span class="number">0.099</span>
<span class="number">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.194</span>

$`WOE Table <span class="keyword">for</span> v.rftl`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.1578370554</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">616</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">30.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">372</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">244</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">37.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">24.1</span>%&nbsp;&nbsp;<span class="number">39.6</span>%&nbsp;&nbsp;<span class="number">44.6</span> <span class="number">0.060</span>
<span class="number">2</span>&nbsp;&nbsp;&lt;= <span class="number">0.1880959621</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">768</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">384</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">384</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">37.9</span>%&nbsp;&nbsp;<span class="number">50.0</span>%&nbsp;&nbsp; <span class="number">2.4</span> <span class="number">0.000</span>
<span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.3289762494</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">129</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">179</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">13.1</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">17.7</span>%&nbsp;&nbsp;<span class="number">58.1</span>% -<span class="number">30.4</span> <span class="number">0.014</span>
<span class="number">4</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">103</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">205</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.3</span>%&nbsp;&nbsp;<span class="number">66.6</span>% -<span class="number">66.4</span> <span class="number">0.065</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.140</span>

$`WOE Table <span class="keyword">for</span> stlm`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span> &lt;= -<span class="number">0.4586732186</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">60</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">94</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.3</span>%&nbsp;&nbsp;<span class="number">61.0</span>% -<span class="number">42.5</span> <span class="number">0.014</span>
<span class="number">2</span> &lt;= -<span class="number">0.1688696056</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">462</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">266</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">196</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">26.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">19.4</span>%&nbsp;&nbsp;<span class="number">42.4</span>%&nbsp;&nbsp;<span class="number">32.9</span> <span class="number">0.025</span>
<span class="number">3</span>&nbsp;&nbsp;&lt;= <span class="number">0.2631157075</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">922</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">440</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">482</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">47.6</span>%&nbsp;&nbsp;<span class="number">52.3</span>%&nbsp;&nbsp;-<span class="number">6.7</span> <span class="number">0.002</span>
<span class="number">4</span>&nbsp;&nbsp;&lt;= <span class="number">0.3592235072</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">97</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">57</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.6</span>%&nbsp;&nbsp;<span class="number">37.0</span>%&nbsp;&nbsp;<span class="number">55.6</span> <span class="number">0.023</span>
<span class="number">5</span>&nbsp;&nbsp;&lt;= <span class="number">0.4846279843</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">81</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">73</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.2</span>%&nbsp;&nbsp;<span class="number">47.4</span>%&nbsp;&nbsp;<span class="number">12.8</span> <span class="number">0.001</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">44</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">110</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">4.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.9</span>%&nbsp;&nbsp;<span class="number">71.4</span>% -<span class="number">89.2</span> <span class="number">0.057</span>
<span class="number">8</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.122</span>

$`WOE Table <span class="keyword">for</span> v.rstl`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.4541701981</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">94</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">60</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.9</span>%&nbsp;&nbsp;<span class="number">39.0</span>%&nbsp;&nbsp;<span class="number">47.3</span> <span class="number">0.017</span>
<span class="number">2</span>&nbsp;&nbsp;&lt;= -<span class="number">0.3526306487</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">62</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">92</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.1</span>%&nbsp;&nbsp;<span class="number">59.7</span>% -<span class="number">37.1</span> <span class="number">0.010</span>
<span class="number">3</span>&nbsp;&nbsp;&lt;= -<span class="number">0.2496412214</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">53</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">101</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">5.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">10.0</span>%&nbsp;&nbsp;<span class="number">65.6</span>% -<span class="number">62.1</span> <span class="number">0.029</span>
<span class="number">4</span> &lt;= -<span class="number">0.08554320418</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">307</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">142</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">165</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.4</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.3</span>%&nbsp;&nbsp;<span class="number">53.7</span>% -<span class="number">12.6</span> <span class="number">0.002</span>
<span class="number">5</span>&nbsp;&nbsp;&nbsp;&nbsp;&lt;= <span class="number">0.360854678</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">923</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">46.2</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">491</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">432</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">49.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">42.7</span>%&nbsp;&nbsp;<span class="number">46.8</span>%&nbsp;&nbsp;<span class="number">15.2</span> <span class="number">0.011</span>
<span class="number">6</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">308</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">15.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">146</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">162</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">14.8</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">16.0</span>%&nbsp;&nbsp;<span class="number">52.6</span>%&nbsp;&nbsp;-<span class="number">8.0</span> <span class="number">0.001</span>
<span class="number">8</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.070</span>

$`WOE Table <span class="keyword">for</span> v.pcci`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final.Bin Total.Count Total.Distr. <span class="number">0</span>.Count <span class="number">1</span>.Count <span class="number">0</span>.Distr. <span class="number">1</span>.Distr. <span class="number">1</span>.Rate&nbsp;&nbsp; WOE&nbsp;&nbsp;&nbsp;&nbsp;IV
<span class="number">1</span>&nbsp;&nbsp;&lt;= -<span class="number">0.4410911486</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">92</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">62</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.3</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.1</span>%&nbsp;&nbsp;<span class="number">40.3</span>%&nbsp;&nbsp;<span class="number">41.9</span> <span class="number">0.013</span>
<span class="number">2</span> &lt;= -<span class="number">0.03637567714</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">769</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">38.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">400</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">369</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">40.5</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">36.5</span>%&nbsp;&nbsp;<span class="number">48.0</span>%&nbsp;&nbsp;<span class="number">10.5</span> <span class="number">0.004</span>
<span class="number">3</span>&nbsp;&nbsp; &lt;= <span class="number">0.1801156117</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">461</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">23.1</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">206</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">255</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">20.9</span>%&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">25.2</span>%&nbsp;&nbsp;<span class="number">55.3</span>% -<span class="number">18.9</span> <span class="number">0.008</span>
<span class="number">4</span>&nbsp;&nbsp; &lt;= <span class="number">0.2480148615</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">84</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">70</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.5</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.9</span>%&nbsp;&nbsp;<span class="number">45.5</span>%&nbsp;&nbsp;<span class="number">20.6</span> <span class="number">0.003</span>
<span class="number">5</span>&nbsp;&nbsp; &lt;= <span class="number">0.3348752487</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">67</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">87</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.8</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">8.6</span>%&nbsp;&nbsp;<span class="number">56.5</span>% -<span class="number">23.7</span> <span class="number">0.004</span>
<span class="number">6</span>&nbsp;&nbsp; &lt;= <span class="number">0.4397404288</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">76</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">78</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;-<span class="number">0.2</span> <span class="number">0.000</span>
<span class="number">7</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;= Inf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">154</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">7.7</span>%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">63</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">91</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">6.4</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">9.0</span>%&nbsp;&nbsp;<span class="number">59.1</span>% -<span class="number">34.4</span> <span class="number">0.009</span>
<span class="number">9</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Total&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">2000</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">988</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1012</span>&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp; <span class="number">100.0</span>%&nbsp;&nbsp;<span class="number">50.6</span>%&nbsp;&nbsp;&nbsp;&nbsp;NA <span class="number">0.042</span>

</pre>


  <p>A tabela possui os seguintes valores para cada variável:&nbsp;</p>
  <ul>
    <li><i>Final.Bin</i> — limites de bin;</li>
    <li><i>Total.Count</i>&nbsp; — número total de amostras em bin;</li>
    <li><i>Total.Distr</i> — número relativo de amostras em bin;</li>
    <li><i>0.Count</i> — número de amostras pertencentes à classe&nbsp;"0";</li>
    <li><i>1.Count</i> — número de amostras pertencentes à classe&nbsp;"1";</li>
    <li><i>0.Distr —</i> — número relativo de amostras pertencentes à classe&nbsp;"0";</li>
    <li><i>1.Distr</i> — número relativo de amostras pertencentes à classe&nbsp;"1";</li>
    <li><i>1.Rate</i> — razão percentual das amostras da classe "1" para o número de amostras da classe "0";</li>
    <li>&nbsp;<i>WOE</i> — capacidade preditiva das bins;</li>
    <li><i>&nbsp;IV </i> — importância estatística das bins. <br>
    </li>
  </ul>
</div>
<p>A representação gráfica será mais ilustrativa. Desenhe os
gráficos&nbsp;WOE de todas as variáveis ​​na ordem crescente da sua IV
com base nesta tabela:</p>
<pre class="code">&gt; evalq(woe.binning.plot(preCut), env)</pre>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_9.png" title="WOE 8" alt="WOE 8" style="vertical-align:middle;" width="734" height="592"></p>
<p style="text-align:center;"><span class="small">Fig.8. WOE das 4 melhores variáveis</span></p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_10.png" title="WOE 10" alt="WOE 10" style="vertical-align:middle;" width="750" height="778"></p>
<p style="text-align:center;"><span class="small">Fig.9. WOE das variáveis ​​5-8</span></p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/WOE_11.png" title="WOE 11" alt="WOE 11" style="vertical-align:middle;" width="750" height="750"></p>
<p style="text-align:center;"><span class="small">Fig.10. WOE de variáveis ​​de entrada 9-12&nbsp;</span></p>
<p>Gráfico da variável total das variáveis ​​por sua IV.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/IV_8.png" title="IV  " alt="IV  " style="vertical-align:middle;" width="741" height="602"></p>
<p style="text-align:center;"><span class="small">Fig.11. Intervalo de variáveis ​​pela sua IV</span></p>
<p>Nós não vamos usar duas variáveis ​​insignificantes v.rstl e v.pcci,
que têm IV &lt; 0.1. Nós podemos ver das tabelas que de 10 variáveis
​​significativas, apenas a v.satl e stlm têm uma relação não-linear com a
 variável objetivo. Outras variáveis ​​têm uma relação linear. <br>
</p>
<p>Para experiências adicionais, nós precisamos criar três conjuntos. Eles são:</p>
<ul>
  <li><i>DTbin</i> é um conjunto de dados onde os preditores numéricos
contínuos são transformados em fatores com o número de níveis iguais ao
número de caixas nas quais eles são divididos;</li>
  <li><i>DTdum</i> é um conjunto de dados onde os preditores de fatores
do conjunto de dados DTbin são transformados em variáveis ​​binárias
artificiais;</li>
  <li><i>DTwoe</i> é um conjunto de dados onde os preditores de fatores
são transformados em variáveis ​​numéricas, substituindo seus níveis
pelos valores de WOE desses níveis.</li>
</ul>
<p>O primeiro conjunto de DTbin é necessário para treinamento e obtenção
 das métricas do modelo básico. O segundo e terceiro conjuntos serão
utilizados para o treinamento da DNN e para comparar a eficiência desses
 dois métodos de transformação.</p>
<p>A função <b>woe.binning.deploy()</b> do pacote <i>woebinning</i> nos permitirá resolver esse problema com bastante facilidade. Os seguintes dados devem ser passados ​​para a função:</p>
<ul>
  <li>&nbsp;quadro de dados com preditores e a variável objetivo, onde a variável objetivo deve ter o valor de 0 ou 1;</li>
  <li>&nbsp;parâmetros de discretização, obtidos na fase anterior (preCut);</li>
  <li>&nbsp;nomes das variáveis ​​que precisam ser categorizadas. Se
todas as variáveis ​​devem ser categorizadas, basta especificar o nome
do quadro de dados;</li>
  <li>&nbsp;especificar o IV mínimo para que as variáveis ​​não sejam categorizadas;</li>
  <li>&nbsp;especificar quais variáveis ​​adicionais (exceto as
categorizadas) que queremos obter. Existem duas variantes - "woe" e
"dum".&nbsp;</li>
</ul>
<p>A função retorna um quadro de dados contendo variáveis ​​iniciais,
variáveis ​​categorizadas e variáveis ​​adicionais (se elas foram
especificadas). Os nomes das variáveis ​​recém-criadas são criados
adicionando um prefixo ou sufixo correspondente ao nome da variável
inicial. Dessa forma, os prefixos de todas as variáveis ​​adicionais são
 "dum" ou "woe" e as variáveis ​​categorizadas possuem o sufixo
"binned". Vamos escrever uma função <span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"><i>DiscretizeData()</i></span>, o que transformará o conjunto de dados inicial usando a woe.binning.deploy().</p>
<pre class="code">DiscretizeData &lt;- function(X, preCut, <span class="keyword">var</span>){
&nbsp;&nbsp;require(<span class="keyword">foreach</span>)
&nbsp;&nbsp;require(woeBinning)
&nbsp;&nbsp;DTd &lt;- list()
&nbsp;&nbsp;<span class="keyword">foreach</span>(i = <span class="number">1</span>:length(X)) %<span class="keyword">do</span>% {
&nbsp;&nbsp;&nbsp;&nbsp;X[[i]] %&gt;% <span class="keyword">select</span>(-Data) %&gt;% targ.<span class="keyword">int</span>() %&gt;%
&nbsp;&nbsp;&nbsp;&nbsp;woe.binning.deploy(preCut, min.iv.total = <span class="number">0.1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; add.woe.or.dum.<span class="keyword">var</span> = <span class="keyword">var</span>) -&gt; res
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>(res)
&nbsp;&nbsp;} -&gt; DTd
&nbsp;&nbsp;list(pretrain = DTd[[<span class="number">1</span>]] ,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; train = DTd[[<span class="number">2</span>]] ,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; val =&nbsp;&nbsp; DTd[[<span class="number">3</span>]] ,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test =&nbsp;&nbsp;DTd[[<span class="number">4</span>]] ) -&gt; DTd
&nbsp;&nbsp;<span class="keyword">return</span>(DTd)
}

</pre>


<br>
<p>Os parâmetros de entrada da função são dados iniciais (lista X) com os slots <i>pretrain/train/val/test</i>, parâmetros de discretização<i> preCut</i> e o tipo da variável adicional<i> (string </i><i>var</i><i>)</i>. </p>
<p>A função removerá a variável "Data" de cada slot e alterará a
variável objetivo - fator "Class" para a variável de objetivo numérico
"Cl". Com base nisso, ela enviará woe.binning.deploy() para a entrada da
 função. Além disso, especificamos nos parâmetros de entrada desta
função o IV mínimo = 0.1 para incluir as variáveis ​​no conjunto de
saída. Na saída, receberemos uma lista com os mesmos slots <i>pretrain/train/val/test</i>.
 Em cada slot, as variáveis ​​categorizadas e, se solicitadas, as
variáveis ​​adicionais serão adicionadas às variáveis ​​iniciais. Vamos
calcular todos os conjuntos necessários e adicionar os dados brutos do
conjunto DTcap.n para eles.</p>
<pre class="code">evalq({
&nbsp;&nbsp;require(dplyr)
&nbsp;&nbsp;require(<span class="keyword">foreach</span>)
&nbsp;&nbsp;&nbsp;&nbsp;DTbin = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">""</span>)
&nbsp;&nbsp;&nbsp;&nbsp;DTwoe = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">"woe"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;DTdum = DiscretizeData(DTcap.n, preCut = preCut, <span class="keyword">var</span> = <span class="string">"dum"</span>)
&nbsp;&nbsp;&nbsp;&nbsp;X.woe &lt;- list()
&nbsp;&nbsp;&nbsp;&nbsp;X.bin &lt;- list()
&nbsp;&nbsp;&nbsp;&nbsp;X.dum &lt;- list()
&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">foreach</span>(i = <span class="number">1</span>:length(DTcap.n)) %<span class="keyword">do</span>% {
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTbin[[i]] %&gt;% <span class="keyword">select</span>(contains(<span class="string">"binned"</span>)) -&gt; X.bin[[i]]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTdum[[i]] %&gt;% <span class="keyword">select</span>(starts_with(<span class="string">"dum"</span>)) -&gt; X.dum[[i]]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DTwoe[[i]] %&gt;% <span class="keyword">select</span>(starts_with(<span class="string">"woe"</span>)) %&gt;%
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;divide_by(<span class="number">100</span>) -&gt; X.woe[[i]]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword">return</span>(list(bin =&nbsp;&nbsp;X.bin[[i]], woe = X.woe[[i]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dum = X.dum[[i]], raw = DTcap.n[[i]]))
&nbsp;&nbsp;&nbsp;&nbsp;} -&gt; DTcut
&nbsp;&nbsp;&nbsp;&nbsp;list(pretrain = DTcut[[<span class="number">1</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;train = DTcut[[<span class="number">2</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val =&nbsp;&nbsp; DTcut[[<span class="number">3</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; test =&nbsp;&nbsp;DTcut[[<span class="number">4</span>]] ) -&gt; DTcut
&nbsp;&nbsp;&nbsp;&nbsp;rm(DTwoe, DTdum, X.woe, X.bin, X.dum)
},
env)

</pre>


<br>
<p>Uma vez que a WOE é um valor percentual, nós podemos dividir o WOE
por 100 e obter valores de variáveis ​​que podem ser enviadas para as
entradas da rede neural sem normalização adicional. Vejamos a estrutura
do slot obtido, por exemplo de DTcut$<i>val.</i></p>
<pre class="code">&gt; env$DTcut$val %&gt;% str()
List of 4
 $ bin:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;10 variables:
&nbsp;&nbsp;..$ v.fatl.binned: Factor w/ 5 levels "(-Inf,-0.3904381926]",..: 1 1 3 2 4 3 4 4 4 4 ...
&nbsp;&nbsp;..$ ftlm.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.2344708291]",..: 2 1 1 1 2 2 3 4 4 4 ...
&nbsp;&nbsp;..$ rbci.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.1718377948]",..: 2 1 2 1 2 3 3 3 4 4 ...
&nbsp;&nbsp;..$ v.rbci.binned: Factor w/ 5 levels "(-Inf,-0.1837437563]",..: 1 1 3 2 4 3 4 4 4 4 ...
&nbsp;&nbsp;..$ v.satl.binned: Factor w/ 5 levels "(-Inf,-0.01840058612]",..: 1 1 1 1 1 1 1 1 1 2 ...
&nbsp;&nbsp;..$ v.stlm.binned: Factor w/ 5 levels "(-Inf,-0.4030051922]",..: 2 2 3 2 3 2 3 3 4 4 ...
&nbsp;&nbsp;..$ pcci.binned&nbsp;&nbsp;: Factor w/ 5 levels "(-Inf,-0.1738420887]",..: 1 1 4 2 4 2 4 2 2 3 ...
&nbsp;&nbsp;..$ v.ftlm.binned: Factor w/ 4 levels "(-Inf,-0.03697698898]",..: 1 1 3 2 3 2 3 3 2 2 ...
&nbsp;&nbsp;..$ v.rftl.binned: Factor w/ 5 levels "(-Inf,-0.1578370554]",..: 2 1 1 1 1 1 1 2 2 2 ...
&nbsp;&nbsp;..$ stlm.binned&nbsp;&nbsp;: Factor w/ 7 levels "(-Inf,-0.4586732186]",..: 2 2 2 2 1 1 1 1 1 2 ...
 $ woe:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;10 variables:
&nbsp;&nbsp;..$ woe.v.fatl.binned: num [1:501] 1.713 1.713 -0.145 0.632 -0.897 ...
&nbsp;&nbsp;..$ woe.ftlm.binned&nbsp;&nbsp;: num [1:501] 0.352 0.972 0.972 0.972 0.352 ...
&nbsp;&nbsp;..$ woe.rbci.binned&nbsp;&nbsp;: num [1:501] 0.274 0.794 0.274 0.794 0.274 ...
&nbsp;&nbsp;..$ woe.v.rbci.binned: num [1:501] 0.683 0.683 -0.295 0.22 -0.861 ...
&nbsp;&nbsp;..$ woe.v.satl.binned: num [1:501] 0.573 0.573 0.573 0.573 0.573 ...
&nbsp;&nbsp;..$ woe.v.stlm.binned: num [1:501] 0.473 0.473 -0.0183 0.473 -0.0183 ...
&nbsp;&nbsp;..$ woe.pcci.binned&nbsp;&nbsp;: num [1:501] 0.619 0.619 -0.65 0.174 -0.65 ...
&nbsp;&nbsp;..$ woe.v.ftlm.binned: num [1:501] 0.435 0.435 -0.669 -0.162 -0.669 ...
&nbsp;&nbsp;..$ woe.v.rftl.binned: num [1:501] 0.024 0.446 0.446 0.446 0.446 ...
&nbsp;&nbsp;..$ woe.stlm.binned&nbsp;&nbsp;: num [1:501] 0.329 0.329 0.329 0.329 -0.425 ...
 $ dum:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;41 variables:
&nbsp;&nbsp;..$ dum.v.fatl.-Inf.-0.3904381926.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.v.fatl.-0.03713814085.0.1130198981.binned : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.v.fatl.-0.3904381926.-0.03713814085.binned: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.v.fatl.0.1130198981.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.ftlm.-0.2344708291.-0.01368798447.binned&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.ftlm.-Inf.-0.2344708291.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.ftlm.-0.01368798447.0.1789073635.binned&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.ftlm.0.1789073635.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;.......................................................................................
&nbsp;&nbsp;..$ dum.stlm.-Inf.-0.4586732186.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.stlm.-0.1688696056.0.2631157075.binned&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.stlm.0.2631157075.0.3592235072.binned&nbsp;&nbsp;&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.stlm.0.3592235072.0.4846279843.binned&nbsp;&nbsp;&nbsp;&nbsp; : num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
&nbsp;&nbsp;..$ dum.stlm.0.4846279843.Inf.binned&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: num [1:501] 0 0 0 0 0 0 0 0 0 0 ...
 $ raw:'data.frame':&nbsp;&nbsp;&nbsp;&nbsp;501 obs. of&nbsp;&nbsp;14 variables:
&nbsp;&nbsp;..$ Data&nbsp;&nbsp;: POSIXct[1:501], format: "2017-02-23 15:30:00" "2017-02-23 15:45:00" ...
&nbsp;&nbsp;..$ ftlm&nbsp;&nbsp;: num [1:501] -0.223 -0.374 -0.262 -0.31 -0.201 ...
&nbsp;&nbsp;..$ stlm&nbsp;&nbsp;: num [1:501] -0.189 -0.257 -0.271 -0.389 -0.473 ...
&nbsp;&nbsp;..$ rbci&nbsp;&nbsp;: num [1:501] -0.0945 -0.1925 -0.1348 -0.1801 -0.1192 ...
&nbsp;&nbsp;..$ pcci&nbsp;&nbsp;: num [1:501] -0.5714 -0.2602 0.4459 -0.0478 0.2596 ...
&nbsp;&nbsp;..$ v.fatl: num [1:501] -0.426 -0.3977 0.0936 -0.1512 0.1178 ...
&nbsp;&nbsp;..$ v.satl: num [1:501] -0.35 -0.392 -0.177 -0.356 -0.316 ...
&nbsp;&nbsp;..$ v.rftl: num [1:501] -0.0547 -0.2065 -0.3253 -0.4185 -0.4589 ...
&nbsp;&nbsp;..$ v.rstl: num [1:501] 0.0153 -0.0273 -0.0636 -0.1281 -0.15 ...
&nbsp;&nbsp;..$ v.ftlm: num [1:501] -0.321 -0.217 0.253 0.101 0.345 ...
&nbsp;&nbsp;..$ v.stlm: num [1:501] -0.288 -0.3 -0.109 -0.219 -0.176 ...
&nbsp;&nbsp;..$ v.rbci: num [1:501] -0.2923 -0.2403 0.1909 0.0116 0.2868 ...
&nbsp;&nbsp;..$ v.pcci: num [1:501] -0.0298 0.3738 0.6153 -0.5643 0.2742 ...
&nbsp;&nbsp;..$ Class : Factor w/ 2 levels "-1","1": 1 1 1 1 2 2 2 2 2 1 ...

</pre>


<br>
<p>Como você pode ver, o slot <i>bin</i> contém 10 variáveis ​​de fatores com diferentes números de níveis. Eles têm o sufixo "binned". O slot <i>woe</i> contém 10 variáveis ​​com níveis de fator alterados para o seu WOE (eles têm o prefixo "woe"). O slot <i>dum </i> tem 41 variáveis ​​numéricas <i>artificiais</i>
 com os valores (0, 1) obtidos das variáveis ​​de fatores através da
codificação um para um (tem o prefixo "dum"). Existem 14 variáveis ​​no
slot <i>raw</i>. Eles são Data — timestamp, Class — variável fator objetivo e 12 preditores numéricos.</p>
<p>Temos todos os dados que precisaremos para outras experiências. Os
objetos listados abaixo devem estar no ambiente env até agora. Deixe-nos
 salvar a área de trabalho com esses objetos para o arquivo <i>PartIV.RData</i>.</p>
<pre class="code">&gt; ls(env)
 [<span class="number">1</span>] "<span class="predefines">Close</span>"&nbsp;&nbsp;&nbsp;&nbsp;"Data"&nbsp;&nbsp;&nbsp;&nbsp; "dt"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "DT"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "DTbin"&nbsp;&nbsp;&nbsp;&nbsp;"DTcap"&nbsp;&nbsp;&nbsp;"DTcap.n"&nbsp;"DTcut"&nbsp;&nbsp;"<span class="predefines">High</span>"&nbsp;&nbsp;&nbsp;&nbsp;
[<span class="number">10</span>] "i"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"<span class="predefines">Low</span>"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"<span class="predefines">Open</span>"&nbsp;&nbsp;&nbsp;&nbsp; "pre.outl" "preCut"&nbsp;&nbsp; "preproc"&nbsp;&nbsp;"<span class="predefines">Volume</span>"

</pre>


<br>
<p id="base"><b>2.1.2. Modelo básico de comparação</b></p>
<p>Nós vamos usar o modelo <i>OneR</i> implementado no pacote <b><a href="https://www.mql5.com/go?link=https://shiring.github.io/machine_learning/2017/04/23/one_r" rel="nofollow" title="https://shiring.github.io/machine_learning/2017/04/23/one_r" target="_blank">OneR</a></b>
 como modelo base. Este modelo é simples, confiável e fácil de
interpretar. Informações sobre o algoritmo podem ser encontradas na
descrição do pacote. Este modelo está funcionando apenas com os dados de
 bin. O pacote contém funções auxiliares que podem ser variáveis
​​numéricas discretas de diferentes maneiras. Como já transformamos os
preditores em fatores, não precisamos deles. </p>
<p>Agora, eu vou elaborar o cálculo mostrado abaixo. Crie os conjuntos
train/val/test, extraindo os slots correspondentes de DTcut e
adicionando a classe de variável objetivo para eles. Vamos treinar o
modelo com o conjunto train.</p>
<pre class="code">&gt; evalq({
+&nbsp;&nbsp; require(OneR)
+&nbsp;&nbsp; require(dplyr)
+&nbsp;&nbsp; require(magrittr)
+&nbsp;&nbsp; train &lt;- cbind(DTcut$train$bin, Class = DTcut$train$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
+&nbsp;&nbsp; val &lt;- cbind(DTcut$val$bin, Class = DTcut$val$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
+&nbsp;&nbsp; test &lt;- cbind(DTcut$test$bin, Class = DTcut$test$raw$Class) %&gt;% <span class="keyword">as</span>.data.frame()
+&nbsp;&nbsp; model &lt;- OneR(data = train, formula = <span class="macro">NULL</span>, ties.method = <span class="string">"chisq"</span>, <span class="preprocessor">#c(<span class="string">"first"</span>,<span class="string">"chisq"</span>
</span>+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; verbose = <span class="macro">TRUE</span>) <span class="preprocessor">#FALSE, </span><span class="macro">TRUE</span>
+ }, env)
Loading required package: OneR

&nbsp;&nbsp;&nbsp;&nbsp;Attribute&nbsp;&nbsp;&nbsp;&nbsp; Accuracy
<span class="number">1</span> * v.satl.binned <span class="number">63.14</span>%&nbsp;&nbsp;
<span class="number">2</span>&nbsp;&nbsp; v.fatl.binned <span class="number">62.64</span>%&nbsp;&nbsp;
<span class="number">3</span>&nbsp;&nbsp; ftlm.binned&nbsp;&nbsp; <span class="number">62.54</span>%&nbsp;&nbsp;
<span class="number">4</span>&nbsp;&nbsp; pcci.binned&nbsp;&nbsp; <span class="number">61.44</span>%&nbsp;&nbsp;
<span class="number">5</span>&nbsp;&nbsp; v.rftl.binned <span class="number">59.74</span>%&nbsp;&nbsp;
<span class="number">6</span>&nbsp;&nbsp; v.rbci.binned <span class="number">58.94</span>%&nbsp;&nbsp;
<span class="number">7</span>&nbsp;&nbsp; rbci.binned&nbsp;&nbsp; <span class="number">58.64</span>%&nbsp;&nbsp;
<span class="number">8</span>&nbsp;&nbsp; stlm.binned&nbsp;&nbsp; <span class="number">58.04</span>%&nbsp;&nbsp;
<span class="number">9</span>&nbsp;&nbsp; v.stlm.binned <span class="number">57.54</span>%&nbsp;&nbsp;
<span class="number">10</span>&nbsp;&nbsp;v.ftlm.binned <span class="number">56.14</span>%&nbsp;&nbsp;
---
Chosen attribute due to accuracy
and ties method (<span class="keyword">if</span> applicable): <span class="string">'*'</span>

Warning message:
In OneR(data = train, formula = <span class="macro">NULL</span>, ties.method = <span class="string">"chisq"</span>, verbose = <span class="macro">TRUE</span>) :
&nbsp;&nbsp;data contains unused factor levels

</pre>


O modelo selecionou a variável <span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"></span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"><i>v.satl.binned</i></span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);"> com a precisão básica de </span><span style="background-color:rgb(251, 249, 245); color:rgb(0, 0, 0);">63,14% como base para a criação das regras. </span>Vejamos as informações gerais sobre este modelo:
<pre class="code">&gt; summary(env$model)

Call:
OneR(data = train, formula = NULL, ties.method = "chisq", verbose = FALSE)

Regras:
If v.satl.binned = (-Inf,-0.01840058612]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then Class = -1
If v.satl.binned = (-0.01840058612,0.3247097195] then Class = 1
If v.satl.binned = (0.3247097195,0.4003869443]&nbsp;&nbsp; then Class = 1
If v.satl.binned = (0.4003869443, Inf]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; then Class = 1

Accuracy:
632 of 1001 instances classified correctly (63.14%)

Contingency table:
&nbsp;&nbsp;&nbsp;&nbsp; v.satl.binned
Class (-Inf,-0.01840058612] (-0.01840058612,0.3247097195] (0.3247097195,0.4003869443] (0.4003869443, Inf]&nbsp;&nbsp;Sum
&nbsp;&nbsp;-1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 325&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 161&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;28&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;37&nbsp;&nbsp;551
&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 143&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * 229&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 35&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;* 43&nbsp;&nbsp;450
&nbsp;&nbsp;Sum&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 468&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 390&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;63&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;80 1001
---
Maximum in each column: '*'

Pearson's Chi-squared test:
X-squared = 74.429, df = 3, p-value = 4.803e-16

</pre>


<p>Representação gráfica do resultado de treinamento:</p>
<pre class="code">plot(env$model)</pre>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/OneR_1.png" title="OneR" alt="OneR" style="vertical-align:middle;" width="750" height="442"></p>
<p style="text-align:center;"><span class="small">Fig.12. Distribuição das categorias da variável <i>v.satl.binned </i> por classes no modelo</span></p>
<p>A precisão da previsão durante o treinamento não é muito alta. Nós
vamos ver a precisão que este modelo mostrará no conjunto de validação:</p>
<pre class="code">&gt; evalq(res.val &lt;- eval_model(predict(model, val %&gt;% <span class="keyword">as</span>.data.frame()), val$Class),
+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; env)

Confusion matrix (absolute):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span> Sum
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">106</span>&nbsp;&nbsp;<span class="number">87</span> <span class="number">193</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">100</span> <span class="number">208</span> <span class="number">308</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">206</span> <span class="number">295</span> <span class="number">501</span>

Confusion matrix (relative):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
Prediction&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp;Sum
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">0.21</span> <span class="number">0.17</span> <span class="number">0.39</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">0.20</span> <span class="number">0.42</span> <span class="number">0.61</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">0.41</span> <span class="number">0.59</span> <span class="number">1.00</span>

Accuracy:
<span class="number">0.6267</span> (<span class="number">314</span>/<span class="number">501</span>)

Error rate:
<span class="number">0.3733</span> (<span class="number">187</span>/<span class="number">501</span>)

Error rate reduction (vs. <span class="keyword">base</span> rate):
<span class="number">0.0922</span> (p-<span class="keyword">value</span> = <span class="number">0.04597</span>)

</pre>


<p>e no conjunto de teste:</p>
<pre class="code">&gt; evalq(res.test &lt;- eval_model(predict(model, test %&gt;% <span class="keyword">as</span>.data.frame()), test$Class),
+&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; env)

Confusion matrix (absolute):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span> Sum
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">130</span> <span class="number">102</span> <span class="number">232</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">76</span> <span class="number">193</span> <span class="number">269</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">206</span> <span class="number">295</span> <span class="number">501</span>

Confusion matrix (relative):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Actual
Prediction&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp;Sum
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -<span class="number">1</span>&nbsp;&nbsp;<span class="number">0.26</span> <span class="number">0.20</span> <span class="number">0.46</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">1</span>&nbsp;&nbsp; <span class="number">0.15</span> <span class="number">0.39</span> <span class="number">0.54</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sum <span class="number">0.41</span> <span class="number">0.59</span> <span class="number">1.00</span>

Accuracy:
<span class="number">0.6447</span> (<span class="number">323</span>/<span class="number">501</span>)

Error rate:
<span class="number">0.3553</span> (<span class="number">178</span>/<span class="number">501</span>)

Error rate reduction (vs. <span class="keyword">base</span> rate):
<span class="number">0.1359</span> (p-<span class="keyword">value</span> = <span class="number">0.005976</span>)

</pre>


<p>Os resultados não são encorajadores. A <i>redução da taxa de erro</i>
 mostra como a precisão aumentou em relação ao nível base (0.5). O baixo
 valor de p (&lt; 0.05) indica que este modelo é capaz de produzir
previsões melhor do que o nível básico. A precisão do conjunto de teste é
 0.6447 (323/501), que é maior do que a precisão do conjunto de
validação. O conjunto de teste está mais longe do conjunto de
treinamento do que o conjunto de validação. Este resultado será o ponto
de referência para comparar resultados de previsão de nossos futuros
modelos.</p>
<br>
<p id="structure"><b> 2.1.3. Estrutura de uma DNN</b></p>
<p>Nós usaremos três conjuntos de dados para treinar e testar:</p>
<ol>
  <li> DTcut$$raw — 12 variáveis ​​de entrada (outliers imputados e normalizados).</li>
  <li>DTcut$$dum — 41 variáveis ​​binárias. <br>
  </li>
  <li>DTcut$$woe — 10 variáveis ​​numéricas.</li>
</ol>
<p> Nós vamos usar com todos os conjuntos de dados a variável Class = fator com dois níveis. Estrutura das redes neurais:</p>
<ul>
  <li>DNNraw - layers = c(12, 16, 8(2), 2), funções de ativação c(tanh, maxout(lin), softmax)</li>
  <li>DNNwoe - layers = c(10, 16, 8(2), 2), funções de ativação c(tanh, maxout(lin), softmax)</li>
  <li>DNNdum - layers = c(41, 50, 8(2), 2), funções de ativação c(ReLU, maxout(ReLU), softmax)</li>
</ul>
<p>O diagrama abaixo mostra a estrutura da rede neural DNNwoe. A rede
neural possui uma camada de entrada, duas camadas ocultas e uma camada
de saída. Duas outras redes neurais ( DNNdum, DNNraw) têm uma estrutura
semelhante. Eles apenas diferem no número de neurônios em camadas e
funções de ativação.</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/strDNN__2.png" title="estrutura DNN_!" alt="estrutura DNN_!" style="vertical-align:middle;" width="571" height="634"><br>
</p>
<p style="text-align:center;"><span class="small">Fig.13. Estrutura da rede neural DNNwoe</span></p>
<br>
<p id="methods"><b> 2.1.4. Variantes de treinamento</b></p>
<p id="pretraining"> <b>Com pré-treinamento</b></p>
<p>O treinamento terá duas etapas</p>
<ul>
  <li>pré-treinamento do SRBM com o conjunto /pretrain seguido do
treinamento apenas da camada superior da rede neural, validação com o
conjunto train e parâmetros — par_0;</li>
  <li>ajuste fino de toda a rede com os conjuntos de train/val e parâmetros par_1.</li>
</ul>
<p>&nbsp;Nós podemos salvar os modelos intermediários de ajuste fino,
mas não é obrigatório. O modelo que mostra os melhores resultados de
treinamento deve ser salvo. Os parâmetros dessas duas etapas devem
conter:</p>
<ul>
  <li>par_0 — parâmetros gerais da rede neural, parâmetros de
treinamento da RBM e parâmetros de treinamento da camada superior da
DNN;</li>
  <li>par_1 — parâmetros de treinamento de todas as camadas da DNN.</li>
</ul>
<p>Todos os parâmetros do DArch têm valores padrão. Se precisamos de
parâmetros diferentes em uma determinada etapa do treinamento, nós
podemos configurá-los por uma lista e eles substituirão os parâmetros
padrão. Após o primeiro estágio de treinamento, nós obteremos a
estrutura DArch com parâmetros e resultados de treinamento (erro de
treinamento, erro de teste etc) e também a rede neural iniciada com os
pesos do SRBM treinado. Para completar o segundo estágio de treinamento,
 você precisa incluir a estrutura DArch obtida na primeira etapa na
lista de parâmetros para esta etapa de treinamento. Naturalmente, nós
precisaremos de conjuntos de treinamento e de validação.</p>
<p>Consideremos os parâmetros necessários para o primeiro estágio de
treinamento (pré-treinamento do SRBM e treinamento da camada superior da
 rede neural) e executá-lo:</p>
<pre class="code"><span class="preprocessor">##=====CODE </span>I etap===========================
evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(dplyr)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// the number of input and output neurons will be identified automatically from the data set </span>
&nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
&nbsp;&nbsp;<span class="preprocessor">#------------------
&nbsp;&nbsp;</span>par_0 &lt;- list(
&nbsp;&nbsp;&nbsp;&nbsp;layers = Ln, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// let us take this parameter out of the list (for simplicity)</span>
&nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// if we want to obtain identical data during initialization</span>
&nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="comment">// what level of information output we require</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor"># </span>params RBM========================
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rbm.consecutive = F, <span class="preprocessor"># </span>each RBM is trained one epoch at a time
&nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = nEp_0,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.batchSize = <span class="number">50</span>,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.allData = <span class="macro">TRUE</span>,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.lastLayer = -<span class="number">1</span>, <span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">// do not train the upper layer of SRBM</span>
&nbsp;&nbsp;&nbsp;&nbsp;rbm.learnRate = <span class="number">0.3</span>,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.unitFunction = <span class="string">"tanhUnitRbm"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor"># </span>params NN ========================
&nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_0,<span class="preprocessor">#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="comment">// take this parameter out of the list for simplicity</span>
&nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(F,F,T), <span class="preprocessor">#обучать&nbsp;&nbsp;&nbsp;&nbsp; </span><span class="comment">//upper layer only </span>
&nbsp;&nbsp;&nbsp;&nbsp;darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = <span class="number">0.5</span>,
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation
&nbsp;&nbsp;&nbsp;&nbsp;</span>normalizeWeights = T,
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = <span class="string">"linearUnit"</span>)
<span class="preprocessor">#---------------------------
&nbsp;&nbsp;
&nbsp;&nbsp;</span>DNN_default &lt;- darch(darch = <span class="macro">NULL</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; paramsList = par_0,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut$pretrain$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut$pretrain$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut$train$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut$train$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
}, env)

</pre>


Resultado após a conclusão do primeiro estágio de treinamento:
<pre class="code">...........................<span style="background-color:rgb(255, 246, 200);">
</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Train <span class="keyword">set</span> (best model): <span class="number">31.95</span>% (<span class="number">639</span>/<span class="number">2000</span>)</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Train <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.233</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation <span class="keyword">set</span> (best model): <span class="number">35.86</span>% (<span class="number">359</span>/<span class="number">1001</span>)</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Validation <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.306</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Best model was found after epoch <span class="number">3</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Final <span class="number">0.632</span> validation Cross Entropy error: <span class="number">1.279</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Final <span class="number">0.632</span> validation classification error: <span class="number">34.42</span>%
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">12</span>:<span class="number">19</span>] Fine-tuning finished after <span class="number">5.975</span> secs

</pre>


<p>Segunda etapa do treinamento da rede neural:</p>
<pre class="code"><span class="preprocessor">##=====CODE </span>II etap===========================
evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(dplyr)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
&nbsp;&nbsp;bp.learnRate &lt;- <span class="number">1</span>
&nbsp;&nbsp;par_1 &lt;- list(
&nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
&nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
&nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);">rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!</span>
&nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);">darch.trainLayers = c(T,T,T)</span>, <span class="preprocessor">#TRUE,
&nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
&nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="preprocessor">#------------------------------
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span>DNN_1 &lt;- darch( darch = DNN_default, paramsList = par_1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut$train$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut$train$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut$val$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut$val$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; )
}, env)

</pre>


<p>Resultado da segunda etapa do treinamento: <br>
</p>
<pre class="code">...........................
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Finished epoch <span class="number">100</span> of <span class="number">100</span> after <span class="number">0.279</span> secs (<span class="number">3666</span> patterns/sec)
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="background-color:rgb(255, 246, 200);">Classification error on Train <span class="keyword">set</span> (best model): <span class="number">31.97</span>% (<span class="number">320</span>/<span class="number">1001</span>)</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Train <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.225</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="color:rgb(0, 0, 0); background-color:rgb(255, 246, 200);">Classification error on Validation <span class="keyword">set</span> (best model): <span class="number">31.14</span>% (<span class="number">156</span>/<span class="number">501</span>)</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Validation <span class="keyword">set</span> (best model) Cross Entropy error: <span class="number">1.190</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Best model was found after epoch <span class="number">96</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Final <span class="number">0.632</span> validation Cross Entropy error: <span class="number">1.203</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] <span style="background-color:rgb(255, 246, 200);">Final <span class="number">0.632</span> validation classification error: <span class="number">31.44</span>%</span>
INFO [<span class="number">2017</span>-<span class="number">09</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">48</span>:<span class="number">37</span>] Fine-tuning finished after <span class="number">37.22</span> secs

</pre>


<p>Gráfico da variação do erro de previsão durante o segundo estágio de treinamento:</p>
<pre class="code">plot(env$DNN_1, y = <span class="string">"raw"</span>)</pre>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/DNNwoe_II.png" title="DNNwoe II etap" alt="DNNwoe II etap" style="vertical-align:middle;" width="750" height="442"><br>
</p>
<p style="text-align:center;"><span class="small">Fig.14. Variação do erro de classificação durante o segundo estágio de treinamento</span></p>
<p>Vejamos o erro de classificação do modelo final no conjunto de teste:</p>
<pre class="code">#-----------
evalq({
&nbsp;&nbsp;xValid = DTcut$test$woe %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;Ypredict &lt;- predict(DNN_1, newdata = xValid, type = "<span class="keyword">class</span>")
&nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
&nbsp;&nbsp;cat(paste0("Incorrect classifications on all examples: ", numIncorrect, " (",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), "%)\n"))
&nbsp;&nbsp; caret::confusionMatrix(yValid, Ypredict)
}, env)
<span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: <span class="number">166</span> (<span class="number">33.13</span>%)</span>
Confusion Matrix and Statistics

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
Prediction&nbsp;&nbsp;-<span class="number">1</span>&nbsp;&nbsp; <span class="number">1</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-<span class="number">1</span> <span class="number">129</span>&nbsp;&nbsp;<span class="number">77</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="number">1</span>&nbsp;&nbsp; <span class="number">89</span> <span class="number">206</span>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);"> Accuracy : <span class="number">0.6687</span>&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="number">95</span>% CI : (<span class="number">0.6255</span>, <span class="number">0.7098</span>)
&nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : <span class="number">0.5649</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : <span class="number">1.307</span>e-<span class="number">06</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : <span class="number">0.3217</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Mcnemar's Test P-Value : <span class="number">0.3932</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : <span class="number">0.5917</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : <span class="number">0.7279</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : <span class="number">0.6262</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : <span class="number">0.6983</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : <span class="number">0.4351</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : <span class="number">0.2575</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; Detection Prevalence : <span class="number">0.4112</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : <span class="number">0.6598</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -<span class="number">1</span>
#----------------------------------------

</pre>


<p>A precisão neste conjunto de dados (woe) com esses parâmetros que
estão longe de ser ótimos, é muito maior do que a precisão do modelo
básico. Existe um potencial significativo para aumentar a precisão ao
otimizar os hiperparâmetros da DNN. Se o cálculo for repetido, os dados
podem não ser exatamente os mesmos que no artigo.&nbsp;</p>
<p>Vamos levar nossos scripts para um formulário mais compacto para
cálculos adicionais com outros conjuntos de dados. Vamos escrever uma
função para o conjunto woe:</p>
<pre class="code"><span class="preprocessor">#-------------------
</span>DNN.train.woe &lt;- function(param, X){
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;darch( darch = <span class="macro">NULL</span>, paramsList = param[[<span class="number">1</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">1</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;) %&gt;%
&nbsp;&nbsp;&nbsp;&nbsp;darch( ., paramsList = param[[<span class="number">2</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">3</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;&nbsp;&nbsp;) -&gt; Darch
&nbsp;&nbsp;<span class="keyword">return</span>(Darch)
}

</pre>


<p>Repita os cálculos para o conjunto de dados DTcut$$woe em uma forma compacta:</p>
<pre class="code">evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
&nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
&nbsp;&nbsp;nEp_1 &lt;- <span class="number">25</span>
&nbsp;&nbsp;rbm.learnRate = c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;list(par_0, par_1) %&gt;% DNN.train.woe(DTcut) -&gt; Dnn.woe
&nbsp;&nbsp;xValid = DTcut$test$woe %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;Ypredict &lt;- predict(Dnn.woe, newdata = xValid, type = "<span class="keyword">class</span>")
&nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
&nbsp;&nbsp;cat(paste0("Incorrect classifications on all examples: ", numIncorrect, " (",
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), "%)\n"))
&nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.woe
}, env)

</pre>


<p>Faça o cálculo para o conjunto de dados DTcut$$raw:</p>
<pre class="code">#-------------------------
DNN.train.raw &lt;- function(param, X){
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;darch( darch = NULL, paramsList = param[[<span class="number">1</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">1</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;) %&gt;%
&nbsp;&nbsp;&nbsp;&nbsp;darch( ., paramsList = param[[<span class="number">2</span>]],
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = X[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = X[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = X[[<span class="number">3</span>]]$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class)),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = X[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;&nbsp;&nbsp;) -&gt; Darch
&nbsp;&nbsp;<span class="keyword">return</span>(Darch)
}
#-------------------------------
evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
&nbsp;&nbsp;nEp_0 &lt;- <span class="number">25</span>
&nbsp;&nbsp;nEp_1 &lt;- <span class="number">25</span>
&nbsp;&nbsp;rbm.learnRate = c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.3</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;list(par_0, par_1) %&gt;% DNN.train.raw(DTcut) -&gt; Dnn.raw
&nbsp;&nbsp;xValid = DTcut$test$raw %&gt;% tbl_df %&gt;% <span class="keyword">select</span>(-c(Data, Class))
&nbsp;&nbsp;yValid = DTcut$test$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;Ypredict &lt;- predict(Dnn.raw, newdata = xValid, type = <span class="string">"class"</span>)
&nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
&nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; round(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
&nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.raw
}, env)
#----------------------------

</pre>


<p>Abaixo está o resultado e o gráfico da variação de erro de classificação para este conjunto:</p>
<pre class="code">&gt; env$cM.raw
Confusion Matrix and Statistics

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
Prediction&nbsp;&nbsp;-1&nbsp;&nbsp; 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1 133&nbsp;&nbsp;73
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp; 86 209
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span style="background-color:rgb(255, 246, 200);">Accuracy : 0.6826</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 95% CI : (0.6399, 0.7232)
&nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : 0.5629&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : 2.667e-08&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : 0.3508&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Mcnemar's Test P-Value : 0.3413&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : 0.6073&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : 0.7411&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : 0.6456&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : 0.7085&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : 0.4371&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : 0.2655&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; Detection Prevalence : 0.4112&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : 0.6742&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -1
#--------------------------------------

</pre>


<pre class="code">plot(env$Dnn.raw, y = <span class="string">"raw"</span>)</pre>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn.png" title="Dnn.raw  error" alt="Dnn.raw  error" style="vertical-align:middle;" width="750" height="442"></p>
<p style="text-align:center;"><span class="small">Fig.15.&nbsp;</span><span>Variação do erro de classificação no segundo estágio</span></p>
<p>Eu não consegui treinar a rede neural com os dados DTcut$$dum. Você
pode tentar fazer isso sozinho. Por exemplo, insira os dados DTcut$$bin e
 organize os parâmetros de treinamento para que os preditores sejam
convertidos artificialmente.<br>
</p>
<br>
<p id="nopretraining"><b> Treinamento sem pré-treinamento </b></p>
<p>Vamos treinar a rede neural sem pré-treinamento com os mesmos dados
(woe, raw) nos conjuntos pretrain/train/val. Vamos ver o resultado.</p>
<pre class="code"><span class="preprocessor">#-------WOE----------------
</span>evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
&nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
&nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;<span class="preprocessor">#--param----------------
&nbsp;&nbsp;</span>par_1 &lt;- list(
&nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
&nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
&nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!
&nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
&nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(T,T,T), <span class="preprocessor">#TRUE,
&nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.0</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
&nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
&nbsp;&nbsp;<span class="preprocessor">#--train---------------------------
&nbsp;&nbsp;</span>darch( darch = <span class="macro">NULL</span>, paramsList = par_1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut[[<span class="number">1</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut[[<span class="number">2</span>]]$woe %&gt;% <span class="keyword">as</span>.data.frame(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;) -&gt; Dnn.woe.I
&nbsp;&nbsp;<span class="preprocessor">#---test--------------------------
&nbsp;&nbsp;</span>xValid = DTcut$val$woe %&gt;% <span class="keyword">as</span>.data.frame()
&nbsp;&nbsp;yValid = DTcut$val$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;Ypredict &lt;- predict(Dnn.woe.I, newdata = xValid, type = <span class="string">"class"</span>)
&nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
&nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
&nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.woe.I
}, env)
<span class="preprocessor">#---------Ris16------------------------------------
</span>plot(env$Dnn.woe.I, type = <span class="string">"class"</span>)
env$cM.woe.I

</pre>


<p>Métricas:</p>
<pre class="code">.......................................................
INFO [2017-09-14 10:38:01] <span style="background-color:rgb(255, 246, 200);">Classification error on Train set (best model): 28.7% (574/2000)</span>
INFO [2017-09-14 10:38:01] Train set (best model) Cross Entropy error: 1.140
INFO [2017-09-14 10:38:02] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation set (best model): 35.86% (359/1001)</span>
INFO [2017-09-14 10:38:02] Validation set (best model) Cross Entropy error: 1.299
INFO [2017-09-14 10:38:02] Best model was found after epoch 67
INFO [2017-09-14 10:38:02] Final 0.632 validation Cross Entropy error: 1.241
INFO [2017-09-14 10:38:02] Final 0.632 validation classification error: 33.23%
INFO [2017-09-14 10:38:02] Fine-tuning finished after 37.13 secs
<span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: 150 (29.94%)</span>
&gt; env$cM.woe.I
Confusion Matrix and Statistics

          Reference
Prediction  -1   1
        -1 144  62
        1   88 207

              <span style="background-color:rgb(255, 246, 200);"> Accuracy : 0.7006 </span>
                 95% CI : (0.6584, 0.7404)
    No Information Rate : 0.5369
    P-Value [Acc &gt; NIR] : 5.393e-14

                  Kappa : 0.3932
 Mcnemar's Test P-Value : 0.04123

            Sensitivity : 0.6207
            Specificity : 0.7695
         Pos Pred Value : 0.6990
         Neg Pred Value : 0.7017
             Prevalence : 0.4631
         Detection Rate : 0.2874
   Detection Prevalence : 0.4112
      Balanced Accuracy : 0.6951

       'Positive' Class : -1

</pre>


<p>Gráfico da variação do erro de classificação durante o treinamento:</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn_002.png" title="Dnn.woe.I" alt="Dnn.woe.I" style="vertical-align:middle;" width="750" height="442"></p>
<p style="text-align:center;"><span class="small">Fig.16.&nbsp;<span>Variação do erro de classificação sem pré-treinamento com o conjunto $woe</span></span></p>
<p>O mesmo para o conjunto /raw:</p>
<pre class="code">evalq({
&nbsp;&nbsp;require(darch)
&nbsp;&nbsp;require(magrittr)
&nbsp;&nbsp;Ln &lt;- c(<span class="number">0</span>, <span class="number">16</span>, <span class="number">8</span>, <span class="number">0</span>)
&nbsp;&nbsp;nEp_1 &lt;- <span class="number">100</span>
&nbsp;&nbsp;bp.learnRate &lt;- c(<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.1</span>)
&nbsp;&nbsp;<span class="preprocessor">#--param-----------------------------
&nbsp;&nbsp;</span>par_1 &lt;- list(
&nbsp;&nbsp;&nbsp;&nbsp;layers = Ln,
&nbsp;&nbsp;&nbsp;&nbsp;seed = <span class="number">54321</span>,
&nbsp;&nbsp;&nbsp;&nbsp;logLevel = <span class="number">5</span>,
&nbsp;&nbsp;&nbsp;&nbsp;rbm.numEpochs = <span class="number">0</span>,<span class="preprocessor"># </span>SRBM is not to be trained!
&nbsp;&nbsp;&nbsp;&nbsp;darch.batchSize = <span class="number">50</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.numEpochs = nEp_1,
&nbsp;&nbsp;&nbsp;&nbsp;darch.trainLayers = c(T,T,T), <span class="preprocessor">#TRUE,
&nbsp;&nbsp;&nbsp;&nbsp;</span>darch.unitFunction = c(<span class="string">"tanhUnit"</span>,<span class="string">"maxoutUnit"</span>, <span class="string">"softmaxUnit"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRate = bp.learnRate,
&nbsp;&nbsp;&nbsp;&nbsp;bp.learnRateScale = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightDecay = <span class="number">0.0002</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dither = F,
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout = c(<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.fineTuneFunction = backpropagation, <span class="preprocessor">#rpropagation </span>backpropagation
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeights = T,
&nbsp;&nbsp;&nbsp;&nbsp;normalizeWeightsBound = <span class="number">1</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.weightUpdateFunction = c(<span class="string">"weightDecayWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"maxoutWeightUpdate"</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="string">"weightDecayWeightUpdate"</span>),
&nbsp;&nbsp;&nbsp;&nbsp;darch.dropout.oneMaskPerEpoch = T,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.poolSize = <span class="number">2</span>,
&nbsp;&nbsp;&nbsp;&nbsp;darch.maxout.unitFunction = exponentialLinearUnit,
&nbsp;&nbsp;&nbsp;&nbsp;darch.elu.alpha = <span class="number">2</span>)
&nbsp;&nbsp;<span class="preprocessor">#---train------------------------------
&nbsp;&nbsp;</span>darch( darch = <span class="macro">NULL</span>, paramsList = par_1,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; x = DTcut[[<span class="number">1</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class)) ,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; y = DTcut[[<span class="number">1</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector(),
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; xValid = DTcut[[<span class="number">2</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class)) ,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; yValid = DTcut[[<span class="number">2</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;) -&gt; Dnn.raw.I
&nbsp;&nbsp;<span class="preprocessor">#---test--------------------------------
&nbsp;&nbsp;</span>xValid = DTcut[[<span class="number">3</span>]]$raw %&gt;% tbl_df %&gt;% select(-c(Data, Class))
&nbsp;&nbsp;yValid = DTcut[[<span class="number">3</span>]]$raw$Class %&gt;% <span class="keyword">as</span>.vector()
&nbsp;&nbsp;Ypredict &lt;- predict(Dnn.raw.I, newdata = xValid, type = <span class="string">"class"</span>)
&nbsp;&nbsp;numIncorrect &lt;- sum(Ypredict != yValid)
&nbsp;&nbsp;cat(paste0(<span class="string">"Incorrect classifications on all examples: "</span>, numIncorrect, <span class="string">" ("</span>,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="functions">round</span>(numIncorrect/nrow(xValid)*<span class="number">100</span>, <span class="number">2</span>), <span class="string">"%)\n"</span>))
&nbsp;&nbsp;caret::confusionMatrix(yValid, Ypredict) -&gt; cM.raw.I
}, env)
<span class="preprocessor">#---------Ris17----------------------------------
</span>env$cM.raw.I
plot(env$Dnn.raw.I, type = <span class="string">"class"</span>)

</pre>


<p>Métricas:</p>
<pre class="code">INFO [2017-09-14 11:06:13] <span style="background-color:rgb(255, 246, 200);">Classification error on Train set (best model): 30.75% (615/2000)</span>
INFO [2017-09-14 11:06:13] Train set (best model) Cross Entropy error: 1.189
INFO [2017-09-14 11:06:13] <span style="background-color:rgb(255, 246, 200);">Classification error on Validation set (best model): 33.67% (337/1001)</span>
INFO [2017-09-14 11:06:13] Validation set (best model) Cross Entropy error: 1.236
INFO [2017-09-14 11:06:13] Best model was found after epoch 45
INFO [2017-09-14 11:06:13] Final 0.632 validation Cross Entropy error: 1.219
INFO [2017-09-14 11:06:13] Final 0.632 validation classification error: 32.59%
INFO [2017-09-14 11:06:13] Fine-tuning finished after 35.47 secs
<span style="background-color:rgb(255, 246, 200);">Incorrect classifications on all examples: 161 (32.14%)</span>
&gt; #---------Ris17----------------------------------
&gt; env$cM.raw.I
Confusion Matrix and Statistics

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Reference
Prediction&nbsp;&nbsp;-1&nbsp;&nbsp; 1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1 140&nbsp;&nbsp;66
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp; 95 200
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style="background-color:rgb(255, 246, 200);"> Accuracy : 0.6786&nbsp;</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 95% CI : (0.6358, 0.7194)
&nbsp;&nbsp;&nbsp;&nbsp;No Information Rate : 0.5309&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;P-Value [Acc &gt; NIR] : 1.283e-11&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kappa : 0.3501&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Mcnemar's Test P-Value : 0.02733&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sensitivity : 0.5957&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Specificity : 0.7519&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pos Pred Value : 0.6796&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Neg Pred Value : 0.6780&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prevalence : 0.4691&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Detection Rate : 0.2794&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp; Detection Prevalence : 0.4112&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Balanced Accuracy : 0.6738&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 'Positive' Class : -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

</pre>


<p>Chart of the classification error change:</p>
<p style="text-align:center;"><img src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/Dnn_003.png" title="Dnn.raw.I" alt="Dnn.raw.I" style="vertical-align:middle;" width="750" height="442"></p>
<p style="text-align:center;"><span class="small"><span>Fig.17.&nbsp;</span><span>Change of the classification error without pretraining with the $raw set</span></span></p>
<br>
<h4 id="analythics">2.2. Análise de resultados</h4>
<p>Vamos colocar o resultado de nossos experimentos em uma tabela:</p>
<table class="standart" width="100%" cellspacing="0" cellpadding="2">
  <thead>
    <tr>
      <th style="text-align:left;">Tipo de treinamento</th>
      <th>Conjunto /woe</th>
      <th>Conjunto /raw</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:left;">Com pré-treinamento</td>
      <td style="text-align:center;">0.6687 (0.6255 - 0.7098)</td>
      <td style="text-align:center;">0.6826(0.6399 - 0.7232)</td>
    </tr>
    <tr>
      <td>Sem pré-treinamento</td>
      <td style="text-align:center;">0.7006(0.6589 - 0.7404)</td>
      <td style="text-align:center;">0.6786(0.6359 - 0.7194)</td>
    </tr>
  </tbody>
</table>
<p>O erro de classificação com o pré-treinamento é quase o mesmo em
ambos os conjuntos. Está na faixa de 30+/-4%. Apesar de um erro menor,
fica claro a partir do gráfico de variação do erro de classificação que
houve uma reconversão durante o treinamento sem pré-treinamento (o erro
nos conjuntos de validação e teste foi significativamente maior que o
erro de treinamento). Portanto, nós usaremos o treino com o
pré-treinamento em nossos experimentos adicionais.</p>
<p>O resultado não é muito maior do que o resultado do modelo básico.
Nós temos a possibilidade de melhorar as características ao otimizar
alguns hiperparâmetros. Nós vamos fazer isso no próximo artigo.&nbsp;<br>
</p>
<br>
<h3 id="final">Conclusão</h3>
<p>Apesar das limitações (por exemplo, apenas dois métodos básicos de
treinamento), o pacote darch permite criar redes neurais diferentes em
estrutura e parâmetros. Este pacote é uma boa ferramenta para o estudo
profundo das redes neurais.<br>
</p>
<p>As características fracas da DNN são explicadas principalmente pelo
uso dos parâmetros ou parâmetros padrão próximos a eles. O conjunto woe
não mostrou vantagens antes do conjunto bruto. Portanto, no próximo
artigo, nós iremos:<br>
</p>
<ul>
  <li>otimizar uma parte dos hiperparâmetros na DNN.woe criado anteriormente;</li>
  <li>criaremos uma DNN, usando a biblioteca TensorFlow, testá-la e comparar os resultados com a DNN (darch);</li>
  <li>criaremos um conjunto de redes neurais de diferentes tipos
(empacotamento, empilhamento) e veremos como isso melhora a qualidade
das previsões.</li>
</ul>
<br>
<h3 id="programs">Aplicação</h3>
<div>
  <p><a href="https://www.mql5.com/go?link=https://github.com/VladPerervenko/darch12/tree/master/Part_IV" rel="nofollow" title="https://github.com/VladPerervenko/darch12/tree/master/Part_IV" target="_blank">GitHub/PartIV</a> contém:</p>
  <ol>
    <li>&nbsp;FunPrepareData.R — funções usadas para a preparação de dados</li>
    <li>&nbsp;RunPrepareData.R — scripts para a preparação de dados</li>
    <li>&nbsp;Experiment.R — scripts para executar experimentos</li>
    <li>&nbsp;Part_IV.RData — imagem da área de trabalho com todos os objetos obtidos após o estágio de preparação de dados</li>
    <li>&nbsp;SessionInfo.txt — informações sobre o software usado</li>
    <li>&nbsp;Darch_default.txt — lista de parâmetros da estrutura DArch com os valores padrão</li>
  </ol>
</div>
<p><br>
</p>
  </div>
  </div>
    </div>
</div>

       <script src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/all.js" type="text/javascript" defer="defer"></script>
    <script src="MQL5%20Site%20_%20Redes%20Neurais%20Profundas%20(Parte%20IV).%20Cria%C3%A7%C3%A3o,%20treinamento%20e%20teste%20de%20um%20modelo%20de%20rede%20neural%20-%20Artigos%20MQL5_files/articles.js" type="text/javascript" defer="defer"></script>







                                </div>
                                <div class="row post-footer">
                                    <div class="col-sm-6">
                                            <div class="tags">
                                                Tags:
                                                <a href="#">Artigos</a>
                                            </div>
                                    </div>



                                    <div class="col-sm-6">
                                         <div class="share-post-footer">
                                            Share
                                            <ul class="share">
                                                <li><a href="https://fb.me/datacryptopy" class="facebook" target="_blank" data-toggle="tooltip" data-placement="top" title="Facebook" data-original-title="crowdindicator/"><ion-icon name="logo-facebook"></ion-icon></a></li>
                                    <li><a href="https://twitter.com/DataCryptoML" class="twitter" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="@DataCryptoML"><ion-icon name="logo-twitter"></ion-icon></a></li>
                                <li><a href="https://www.youtube.com/" class="youtube" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Youtube"><ion-icon name="logo-youtube"></ion-icon></a></li>
                                <li><a href="https://www.instagram.com/" class="instagram" title="" target="_blank" data-toggle="tooltip" data-placement="top" data-original-title="Instagram"><ion-icon name="logo-instagram"></ion-icon></a></li>
                                <li><a href="" target="_blank" data-toggle="tooltip" data-placement="top" title="" data-original-title="GitHub"><ion-icon name="logo-github"></ion-icon></a></li>
                                            </ul>
                                        </div>

                                    </div>
                                </div>
                            </div>
                            <div class="col-lg-4 col-md-12">
                                <div class="sidebar">


                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </article>
</main>



        <footer class="site-footer clearfix">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
            </div>
        </div>
    </div>
    <div class="container-fluid copyright">
        <div class="container ">
            <div class="row">
               <section class="social-links-wrapper">
                    <div class="social-links-inner">
                        <a href="https://github.com/datacryptoanalytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-medium">
                           <ion-icon size="large"  name="logo-github"></ion-icon>

                            <span class="social-label">GitHub</span>
                        </a>
                        <a href="https://twitter.com/DataCryptoML" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-twitter">
                            <ion-icon size="large" name="logo-twitter"></ion-icon>
                            <span class="social-label">Twitter</span>
                        </a>
                        <a href="https://fb.me/datacryptopy" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-facebook">
                           <ion-icon size="large" name="logo-facebook"></ion-icon>
                            <span class="social-label">Facebook</span>
                        </a>
                        <a href="https://t.me/datacryptoanalytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-telegram">
                            <svg>
                                <path d="M36.667 18.842c0-9.97-8.046-18.05-17.97-18.05S.727 8.871.727 18.841s8.046 18.05 17.97 18.05 17.97-8.08 17.97-18.05z"></path>
                                <path d="M22.17 21.831c1.088-2.988 2.725-7.831 2.986-9.049.222-1.031.179-1.053-1.344-.554-.68.223-1.804.62-3.304 1.165l-.144.052c-.972.353-3.62 1.325-3.796 1.388-.132.047-2.01.696-2.701.94l-.185.064c-1.275.45-2.14.775-2.413.906-.48.259-.569.441-.486.64.135.323.64.715 1.436 1.021.816.394 2.715 1.274 2.923 1.375.766.372 1.18.628 1.528.978.343.344.57.721.915 1.452.085.18.432.937.5 1.084.24.518.469.995.755 1.571.285.708.655 1.356 1 1.69.277.27.333.268.57-.066.165-.254.385-.745.644-1.437.109-.29.224-.61.352-.982.066-.19.633-1.88.763-2.238z"></path>
                            </svg>
                            <span class="social-label">Telegram</span>
                        </a>
                        <a href="https://www.instagram.com/analyticsdatacrypto/" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-reddit">
                           <ion-icon size="large" name="logo-instagram"></ion-icon>

                            <span class="social-label">Instagram</span>
                        </a>
                        <a href="https://www.linkedin.com/company/datacrypto-analytics" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-discord">
                            <ion-icon size="large" name="logo-linkedin"></ion-icon>

                            <span class="social-label">LinkedIn</span>
                        </a>
                        <a href="https://www.youtube.com/channel/UCxfGBCV9E04Uw4flJLjBCqg?view_as=subscriber" target="_blank" rel="noopener nofollow noreferrer" class="social-item social-youtube">
                            <svg>
                                <path d="M39.292 4.717v-.002a4.462 4.462 0 0 0-3.114-3.163c-1.15-.32-3.812-.552-7.466-.697a219.349 219.349 0 0 0-5.716-.144A250.624 250.624 0 0 0 20.4.69a260.674 260.674 0 0 0-2.597.02c-1.95.025-3.899.07-5.716.139-3.656.14-6.32.363-7.465.669-1.483.413-2.708 1.664-3.113 3.193-.25.965-.442 2.34-.58 4.01a73.77 73.77 0 0 0-.208 3.974A84.096 84.096 0 0 0 .69 14.5a84.538 84.538 0 0 0 .03 1.796 73.88 73.88 0 0 0 .204 3.955c.138 1.69.332 3.075.583 4.034a4.462 4.462 0 0 0 3.112 3.163c1.16.319 3.818.55 7.458.696 1.82.073 3.77.12 5.722.145a249.773 249.773 0 0 0 2.6.021c.13 0 .372 0 .713-.002a260.67 260.67 0 0 0 1.885-.018c1.949-.025 3.898-.07 5.716-.139 3.656-.14 6.32-.363 7.467-.67a4.46 4.46 0 0 0 3.11-3.158c.25-.965.443-2.34.58-4.01a73.77 73.77 0 0 0 .209-3.974 84.096 84.096 0 0 0 .03-1.808v-.024a71.544 71.544 0 0 0-.028-1.964 69.505 69.505 0 0 0-.195-3.799c-.137-1.68-.334-3.062-.594-4.027zm1.333-.358v.002-.003.001z"></path>
                                <path d="M17.023 19.507l8.562-5.007-8.562-5.007v10.014z"></path>
                            </svg>
                            <span class="social-label">Youtube</span>
                        </a>
                    </div>
                </section>


                <div class="col-md-12 text-center footer-copycat">
                    <p class="footer-rights">Copyright © 2019-2020, All Rights Reserved.</p>
                    <a href="mailto:datacryptoanalytics@protonmail.com" class="footer-support">
                        <span>datacryptoanalytics@protonmail.com</span>
                    </a>
                </div>
            </div>
        </div>
    </div>
</footer>

<a href="https://primexbt.com/?signup=120807" class="go-up btn"><ion-icon name="logo-bitcoin"></ion-icon></a>

    </div>

    <script type="text/javascript" src="page_files/libraries.js"></script>

    <script src="https://unpkg.com/ionicons@5.0.0/dist/ionicons.js"></script>



    <script type="text/javascript" src="page_files/main.js"></script>



<div class="share-selected-text-main-container" style="height: 50px; top: 0px; left: 0px;"><div class="share-selected-text-inner"><a class="share-selected-text-btn share-selected-text-btn-twitter" href="https://twitter.com/intent/tweet?url=https://blog.cindicator.com/bitcoin-is-not-digital-gold/&amp;text=%22%22"><i class="icon-sst-twitter fab fa-twitter" style="pointer-events: none;"></i></a></div></div></body></html>
